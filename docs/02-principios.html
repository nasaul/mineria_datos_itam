<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principios de aprendizaje supervisado – Minería de Datos</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03-regresion_lineal.html" rel="next">
<link href="./01-introduccion.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-2453fe3dad938b07a2e5eff64ea8abce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-c1367505ed6638c8d4e510e1459ae853.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-2453fe3dad938b07a2e5eff64ea8abce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Principios de aprendizaje supervisado – Minería de Datos">
<meta property="og:description" content="">
<meta property="og:image" content="02-principios_files/figure-html/error-decomposition-visualization-output-1.png">
<meta property="og:site_name" content="Minería de Datos">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-principios.html"><span class="chapter-title">Principios de aprendizaje supervisado</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Minería de Datos</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Alternar modo oscuro"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Temario</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-requerimientos-computacion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Requerimientos computacionales</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduccion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-principios.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Principios de aprendizaje supervisado</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Regresion lineal</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-regresion_lineal.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Regresión lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./violaciones_supuestos_regresion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Violaciones de los Supuestos de Regresión Lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./analisis_advertising_dataset.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Análisis de Regresión Lineal con el Dataset Advertising</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ejercicio_wine_quality.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clasificacion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Clasificación</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-arboles.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Árboles de Decisión</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Ejemplos</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduccion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introducción a Python para Minería de Datos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regresion_lineal.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Descenso en gradiente con regresión lineal</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#definición-de-aprendizaje-supervisado" id="toc-definición-de-aprendizaje-supervisado" class="nav-link active" data-scroll-target="#definición-de-aprendizaje-supervisado">Definición de aprendizaje supervisado</a>
  <ul class="collapse">
  <li><a href="#predicción" id="toc-predicción" class="nav-link" data-scroll-target="#predicción">Predicción</a></li>
  <li><a href="#inferencia" id="toc-inferencia" class="nav-link" data-scroll-target="#inferencia">Inferencia</a></li>
  </ul></li>
  <li><a href="#cómo-estimar-f" id="toc-cómo-estimar-f" class="nav-link" data-scroll-target="#cómo-estimar-f">¿Cómo estimar <span class="math inline">\(f\)</span>?</a>
  <ul class="collapse">
  <li><a href="#métodos-paramétricos" id="toc-métodos-paramétricos" class="nav-link" data-scroll-target="#métodos-paramétricos">Métodos paramétricos</a></li>
  <li><a href="#métodos-no-paramétricos" id="toc-métodos-no-paramétricos" class="nav-link" data-scroll-target="#métodos-no-paramétricos">Métodos no paramétricos</a></li>
  </ul></li>
  <li><a href="#bias-variance-tradeoff" id="toc-bias-variance-tradeoff" class="nav-link" data-scroll-target="#bias-variance-tradeoff">Bias-Variance Tradeoff</a>
  <ul class="collapse">
  <li><a href="#descomposición-matemática-del-error-esperado" id="toc-descomposición-matemática-del-error-esperado" class="nav-link" data-scroll-target="#descomposición-matemática-del-error-esperado">Descomposición Matemática del Error Esperado</a></li>
  <li><a href="#interpretación-de-los-componentes" id="toc-interpretación-de-los-componentes" class="nav-link" data-scroll-target="#interpretación-de-los-componentes">Interpretación de los Componentes</a></li>
  <li><a href="#el-tradeoff-fundamental" id="toc-el-tradeoff-fundamental" class="nav-link" data-scroll-target="#el-tradeoff-fundamental">El Tradeoff Fundamental</a></li>
  <li><a href="#conexión-con-overfitting-y-underfitting" id="toc-conexión-con-overfitting-y-underfitting" class="nav-link" data-scroll-target="#conexión-con-overfitting-y-underfitting">Conexión con Overfitting y Underfitting</a></li>
  <li><a href="#relación-con-parámetros-de-los-modelos" id="toc-relación-con-parámetros-de-los-modelos" class="nav-link" data-scroll-target="#relación-con-parámetros-de-los-modelos">Relación con Parámetros de los Modelos</a></li>
  <li><a href="#visualización-del-bias-variance-tradeoff" id="toc-visualización-del-bias-variance-tradeoff" class="nav-link" data-scroll-target="#visualización-del-bias-variance-tradeoff">Visualización del Bias-Variance Tradeoff</a></li>
  <li><a href="#ejemplo-práctico-knn-con-diferentes-valores-de-k" id="toc-ejemplo-práctico-knn-con-diferentes-valores-de-k" class="nav-link" data-scroll-target="#ejemplo-práctico-knn-con-diferentes-valores-de-k">Ejemplo Práctico: KNN con Diferentes Valores de k</a></li>
  <li><a href="#bias-variance-con-regresiones-polinomiales" id="toc-bias-variance-con-regresiones-polinomiales" class="nav-link" data-scroll-target="#bias-variance-con-regresiones-polinomiales">Bias-Variance con Regresiones Polinomiales</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Principios de aprendizaje supervisado</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="definición-de-aprendizaje-supervisado" class="level2">
<h2 class="anchored" data-anchor-id="definición-de-aprendizaje-supervisado">Definición de aprendizaje supervisado</h2>
<p>Supongamos que observamos una variable cuantitativa <span class="math inline">\(Y \in \mathbb{R}\)</span> y tenemos <span class="math inline">\(p\)</span> variables predictoras, <span class="math inline">\(X_1, X_2, ..., X_p\)</span>, las cuales denotaremos como <span class="math inline">\(X = (X_1, X_2, ..., X_p)\)</span>. Supongamos que existe alguna reluación entre ellas y se puede expresar de la siguiente forma:</p>
<p><span class="math display">\[ Y = f(x) + \epsilon\]</span></p>
<ul>
<li>Función <span class="math inline">\(f\)</span>: función desconocida que relaciona a <span class="math inline">\(X\)</span> con <span class="math inline">\(Y\)</span>. Representa la información sistémica que <span class="math inline">\(X\)</span> aporta a <span class="math inline">\(Y\)</span>.</li>
<li>Error <span class="math inline">\(\epsilon\)</span>: representa qué tan equivocados estamos con respecto al verdadero valor de <span class="math inline">\(Y\)</span>.</li>
</ul>
<p>La tarea del aprendizaje supervisado es aprender la función <span class="math inline">\(f\)</span>. Existen dos razones por las cuales estimar <span class="math inline">\(f\)</span>: <strong>predicción</strong> e <strong>inferencia</strong>.</p>
<section id="predicción" class="level3">
<h3 class="anchored" data-anchor-id="predicción">Predicción</h3>
<p>En muchas ocasiones existen un conjunto de variables <span class="math inline">\(X\)</span> que están listas para aprovecharse, sin embargo, puede que no se pueda obtener la variable <span class="math inline">\(Y\)</span> de manera inmediata. En este sentido, podemos predecir la variable <span class="math inline">\(Y\)</span> siguiendo la ecuación:</p>
<p><span class="math display">\[\hat{Y} = \hat{f}(X)\]</span></p>
<p>donde <span class="math inline">\(\hat{f}\)</span> representa nuestro estimador de <span class="math inline">\(f\)</span> y <span class="math inline">\(\hat{Y}\)</span> es nuestra predicción de <span class="math inline">\(Y\)</span>. En este sentido <span class="math inline">\(\hat{f}\)</span> es una <em>caja negra</em> en el sentido en el que no nos preocupa cuál es la función, sino que provee predicciones <strong>precisas</strong> para <span class="math inline">\(Y\)</span>.</p>
<p>La <strong>precisión</strong> de <span class="math inline">\(\hat{Y}\)</span> depende de dos cantidades:</p>
<ul>
<li><strong>Error reducible</strong>: En general, <span class="math inline">\(\hat{f}\)</span> no será un estimador perfecto de <span class="math inline">\(f\)</span> y esto introducirá un error el cuál puede reducirse. Ejemplos: Introducir una estructura lineal cuándo el problema tiene estructura cuadrática, falta de variables explicativas, exceso de variables que no contribuyen a la predicción.</li>
<li><strong>Error ireducible</strong>: La variable <span class="math inline">\(Y\)</span> es una función también de <span class="math inline">\(\epsilon\)</span> y por definición nuestra predicción tendra un error inherente. Ejemplos: Predecir que comerán mañana, determinar si lloverá o no, determinar cuándo ocurrirá un temblor, ¿quién ganará una elección?.</li>
</ul>
<p><span class="math display">\[ \begin{align*}
\mathbb{E}[(Y-\hat{Y})^2] &amp;= \mathbb{E}[(f(X) + \epsilon -\hat{f}(x))^2]\\
&amp;= \underset{Reducible}{\underbrace{\mathbb{E}[(f(X) - \hat{f}(x))^2]}} + \underset{Irreducible}{\underbrace{\text{Var}(\epsilon)}}
\end{align*}\]</span></p>
<p>El objetivo del curso se enfoca en técnicas para estimar <span class="math inline">\(f\)</span> con el objectivo de minimizar el error reducible. Es importante tener en cuenta que el error irreducible siempre nos pondrá una cota en la predicción de <span class="math inline">\(Y\)</span>.</p>
<section id="entendiendo-la-descomposición-del-error" class="level4">
<h4 class="anchored" data-anchor-id="entendiendo-la-descomposición-del-error">Entendiendo la descomposición del error</h4>
<p>Analicemos con más detalle la descomposición del error esperado. Partiendo del error cuadrático medio esperado (Expected Mean Squared Error, MSE):</p>
<p><span class="math display">\[ \begin{align*}
\mathbb{E}[(Y-\hat{Y})^2] &amp;= \mathbb{E}[(f(X) + \epsilon - \hat{f}(X))^2]\\
&amp;= \mathbb{E}[(f(X) - \hat{f}(X) + \epsilon)^2]\\
&amp;= \mathbb{E}[(f(X) - \hat{f}(X))^2] + \mathbb{E}[\epsilon^2] + 2\mathbb{E}[(f(X) - \hat{f}(X))\epsilon]\\
&amp;= \mathbb{E}[(f(X) - \hat{f}(X))^2] + \mathbb{E}[\epsilon^2] + 2\mathbb{E}[(f(X) - \hat{f}(X))]\mathbb{E}[\epsilon]\\
&amp;= \mathbb{E}[(f(X) - \hat{f}(X))^2] + \text{Var}(\epsilon)\\
&amp;= \underset{\text{Error Reducible}}{\underbrace{\mathbb{E}[(f(X) - \hat{f}(X))^2]}} + \underset{\text{Error Irreducible}}{\underbrace{\text{Var}(\epsilon)}}
\end{align*}\]</span></p>
<p>Donde usamos los siguientes hechos: - <span class="math inline">\(\mathbb{E}[\epsilon] = 0\)</span> (el error tiene media cero por definición) - <span class="math inline">\(\epsilon\)</span> es independiente de <span class="math inline">\(X\)</span> (asumimos que el ruido no depende de las características) - <span class="math inline">\(\text{Var}(\epsilon) = \mathbb{E}[\epsilon^2] - (\mathbb{E}[\epsilon])^2 = \mathbb{E}[\epsilon^2]\)</span></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Implicaciones Prácticas
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Error Reducible</strong> <span class="math inline">\(\mathbb{E}[(f(X) - \hat{f}(X))^2]\)</span>: - Depende de qué tan bien nuestra estimación <span class="math inline">\(\hat{f}\)</span> aproxima la verdadera función <span class="math inline">\(f\)</span> - Puede minimizarse usando mejores algoritmos, más datos, mejores características - Ejemplos de fuentes: - Usar regresión lineal cuando la verdadera relación es no lineal - Falta de variables explicativas importantes - Exceso de variables que añaden ruido sin información</p>
<p><strong>Error Irreducible</strong> <span class="math inline">\(\text{Var}(\epsilon)\)</span>: - Representa la variabilidad intrínseca en <span class="math inline">\(Y\)</span> que no puede ser explicada por <span class="math inline">\(X\)</span> - No importa qué tan bien estimemos <span class="math inline">\(f\)</span>, este error siempre permanecerá - Ejemplos de fuentes: - Variables no medidas que afectan <span class="math inline">\(Y\)</span> - Aleatoriedad inherente en el proceso generador de datos - Error de medición en la variable respuesta - Factores estocásticos genuinos (ej: movimiento browniano en precios de acciones)</p>
</div>
</div>
</section>
<section id="ejemplos-concretos-de-error-reducible-e-irreducible" class="level4">
<h4 class="anchored" data-anchor-id="ejemplos-concretos-de-error-reducible-e-irreducible">Ejemplos concretos de error reducible e irreducible</h4>
<p><strong>Ejemplo 1: Predicción de ventas de helados</strong></p>
<p>Supongamos que queremos predecir las ventas diarias de helados (<span class="math inline">\(Y\)</span>) usando la temperatura (<span class="math inline">\(X\)</span>).</p>
<ul>
<li><p><strong>Error reducible</strong>: Si usamos un modelo lineal pero la relación verdadera es cuadrática (a temperaturas muy altas la gente prefiere quedarse en casa), nuestro modelo será sistemáticamente incorrecto. Este error puede reducirse usando un modelo más flexible.</p></li>
<li><p><strong>Error irreducible</strong>: Hay factores aleatorios que afectan las ventas (eventos inesperados, decisiones individuales caprichosas, disponibilidad de competidores ese día específico) que no podemos predecir sin importar qué tan sofisticado sea nuestro modelo.</p></li>
</ul>
<p><strong>Ejemplo 2: Diagnóstico médico</strong></p>
<p>Predecir si un paciente tiene cierta enfermedad (<span class="math inline">\(Y\)</span>) basándose en exámenes de laboratorio (<span class="math inline">\(X\)</span>).</p>
<ul>
<li><p><strong>Error reducible</strong>: Si solo usamos un análisis de sangre cuando deberíamos considerar también presión arterial, historial familiar, y síntomas, estamos omitiendo información útil que podría mejorar nuestras predicciones.</p></li>
<li><p><strong>Error irreducible</strong>: Incluso con todas las pruebas posibles, hay variabilidad biológica natural e interacciones complejas no observables que hacen imposible predecir con 100% de certeza.</p></li>
</ul>
</section>
<section id="visualización-de-la-descomposición-del-error" class="level4">
<h4 class="anchored" data-anchor-id="visualización-de-la-descomposición-del-error">Visualización de la descomposición del error</h4>
<p>Veamos gráficamente cómo se descompone el error en una situación concreta:</p>
<div id="cell-error-decomposition-visualization" class="cell" data-fig-height="5" data-fig-width="14" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="error-decomposition-visualization" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="02-principios_files/figure-html/error-decomposition-visualization-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Descomposición del error: Error reducible vs irreducible</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretación de las Gráficas
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Datos observados (puntos grises)</strong>: <span class="math inline">\(Y = f(X) + \epsilon\)</span>, muestran dispersión debido al error irreducible</li>
<li><strong>Línea azul continua</strong>: La función verdadera <span class="math inline">\(f(X)\)</span> (desconocida en la práctica)</li>
<li><strong>Línea discontinua de color</strong>: Nuestra estimación <span class="math inline">\(\hat{f}(X)\)</span></li>
</ul>
<p><strong>Panel izquierdo (Modelo Subajustado)</strong>: - El modelo lineal no captura la curvatura de la función verdadera - Error reducible ALTO: la diferencia entre <span class="math inline">\(f(X)\)</span> y <span class="math inline">\(\hat{f}(X)\)</span> es grande - Este modelo tiene alto <strong>sesgo</strong> (bias)</p>
<p><strong>Panel central (Modelo Apropiado)</strong>: - El modelo cuadrático captura bien la forma de la función verdadera - Error reducible BAJO: <span class="math inline">\(\hat{f}(X)\)</span> está cerca de <span class="math inline">\(f(X)\)</span> - Balance óptimo entre complejidad y ajuste</p>
<p><strong>Panel derecho (Modelo Sobreajustado)</strong>: - El modelo de grado 10 intenta capturar también el ruido - Error reducible artificialmente bajo en estos datos específicos - Pero tendría alto error en datos nuevos (alta <strong>varianza</strong>)</p>
</div>
</div>
</section>
</section>
<section id="inferencia" class="level3">
<h3 class="anchored" data-anchor-id="inferencia">Inferencia</h3>
<p>Existen problemas en donde nos interesa más entender la relación intrinseca que existe entre <span class="math inline">\(Y\)</span> y <span class="math inline">\(X\)</span>. En esta situación nuestro objetivo no es hacer predicción, entonces <span class="math inline">\(\hat{f}\)</span> ya no puede ser tratada como una caja negra. En este tipo de enfoque se contestan preguntas cómo:</p>
<ul>
<li>¿Cuáles son los predictores que se asocian con la variable <span class="math inline">\(Y\)</span>?: Muchas veces solo un subconjunto de los datos <span class="math inline">\(X\)</span> son los que realmente están relacionados con <span class="math inline">\(Y\)</span>.</li>
<li>¿Cuál es la relación entre <span class="math inline">\(Y\)</span> y <span class="math inline">\(X_i\)</span>?</li>
<li>¿La relación entre <span class="math inline">\(Y\)</span> y <span class="math inline">\(X_i\)</span> es lineal o más compleja?</li>
</ul>
</section>
</section>
<section id="cómo-estimar-f" class="level2">
<h2 class="anchored" data-anchor-id="cómo-estimar-f">¿Cómo estimar <span class="math inline">\(f\)</span>?</h2>
<p>Asumiremos que tenemos <span class="math inline">\(n\)</span> datos diferentes estas observaciones serán llamadas <strong>conjunto de entrenamiento</strong>. <span class="math inline">\(x_{ij}\)</span> representa el valor del predictor <span class="math inline">\(j\)</span> para la observación <span class="math inline">\(i\)</span>, donde <span class="math inline">\(i=1,2,...,n\)</span> y <span class="math inline">\(j=1,2,...,p\)</span>. <span class="math inline">\(y_i\)</span> representa la variable respuesta de la observación <span class="math inline">\(i\)</span>. Entonces nuestro <strong>conjunto de entrenamiento</strong> consiste en:</p>
<p><span class="math display">\[{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}\]</span></p>
<p>donde <span class="math inline">\(x_i=(x_{i1}, x_{i2}, ..., x_{ip})^T\)</span>.</p>
<p>Nuestro objetivo es aplicar un método de aprendizaje en el conjunto de datos para poder estimar una función desconocida de <span class="math inline">\(f\)</span>. Nos encantaría encontrar una función <span class="math inline">\(\hat{f}\)</span> de forma tal que <span class="math inline">\(Y\simeq \hat{f}(X)\)</span> para cualquier observación <span class="math inline">\((X, Y)\)</span>. Muchos de estos enfoque se pueden caracterizar como métodos <em>paramétricos</em> o <em>no paramétricos</em>.</p>
<section id="métodos-paramétricos" class="level3">
<h3 class="anchored" data-anchor-id="métodos-paramétricos">Métodos paramétricos</h3>
<p>Los métodos paramétricos involucran un enfoque de dos pasos:</p>
<ol type="1">
<li>Hacemos un supuesto de la forma función de <span class="math inline">\(f\)</span>. Por ejemplo, la más sencilla es que <span class="math inline">\(f\)</span> es linear en <span class="math inline">\(\beta\)</span>:</li>
</ol>
<p><span class="math display">\[ f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p\]</span></p>
<p>Una vez haciendo haciendo el supuesto de linealidad el problema de estimar <span class="math inline">\(f\)</span> es simplificado ya que en lugar de explorar el espacio funcional uno solo necesita estimar <span class="math inline">\(p+1\)</span> coeficientes <span class="math inline">\(\beta_0, ..., \beta_p\)</span>.</p>
<ol start="2" type="1">
<li>Necesitamos un proceso que utilice los datos de entrenamiento para <em>ajustar</em> u <em>entrenar</em> el modelo. El enfoque más sencillo es el método de mínimos cuadrados ordinarios (OLS):</li>
</ol>
<p><span class="math display">\[\underset{\beta_0, \beta_1, ..., \beta_p}{min} \sum_{i=1}^{N}(y_i - (\beta_0 + \beta_1 x_{i1} +\beta_2 x_{i2} + ... + \beta_p X_p))^2\]</span></p>
<p>El enfoque basado en modelado se refiere a los modelos <em>paramétricos</em>; reduce el problema de estimar <span class="math inline">\(f\)</span> a estimar un conjunto de parámetros. La desventaja potencial es que el modelo podría no ser igual a la verdadera <span class="math inline">\(f\)</span> y tendremos malas estimaciones del valor de <span class="math inline">\(y\)</span>.</p>
</section>
<section id="métodos-no-paramétricos" class="level3">
<h3 class="anchored" data-anchor-id="métodos-no-paramétricos">Métodos no paramétricos</h3>
<p>Los métodos no paramétricos <strong>no asumen una forma funcional específica</strong> para <span class="math inline">\(f\)</span>. En lugar de eso, buscan una estimación de <span class="math inline">\(f\)</span> que se ajuste lo más cerca posible a los datos sin estar restringida a una familia particular de funciones.</p>
<section id="motivación" class="level4">
<h4 class="anchored" data-anchor-id="motivación">Motivación</h4>
<p>La principal ventaja de no asumir una forma funcional es la <strong>flexibilidad</strong>:</p>
<ul>
<li>Si la relación verdadera entre <span class="math inline">\(X\)</span> y <span class="math inline">\(Y\)</span> es altamente no lineal y compleja, los métodos paramétricos pueden estar muy sesgados</li>
<li>Los métodos no paramétricos pueden capturar formas funcionales arbitrarias</li>
<li>No necesitamos conocimiento previo sobre la forma de la relación</li>
</ul>
</section>
<section id="ventajas-de-los-métodos-no-paramétricos" class="level4">
<h4 class="anchored" data-anchor-id="ventajas-de-los-métodos-no-paramétricos">Ventajas de los métodos no paramétricos</h4>
<ol type="1">
<li><strong>Mayor flexibilidad</strong>: Pueden ajustarse a relaciones complejas y no lineales</li>
<li><strong>Menos supuestos</strong>: No requieren especificar la forma funcional de <span class="math inline">\(f\)</span></li>
<li><strong>Mejor ajuste potencial</strong>: Si la verdadera <span class="math inline">\(f\)</span> es compleja, pueden ofrecer mejor precisión predictiva</li>
</ol>
</section>
<section id="desventajas-de-los-métodos-no-paramétricos" class="level4">
<h4 class="anchored" data-anchor-id="desventajas-de-los-métodos-no-paramétricos">Desventajas de los métodos no paramétricos</h4>
<ol type="1">
<li><strong>Requieren más datos</strong>: Al no reducir el problema a un conjunto pequeño de parámetros, necesitan muchas más observaciones para estimar <span class="math inline">\(f\)</span> con precisión</li>
<li><strong>Riesgo de sobreajuste</strong>: Con mucha flexibilidad, pueden ajustarse demasiado al ruido de los datos de entrenamiento</li>
<li><strong>Menor interpretabilidad</strong>: Es más difícil entender cómo cada predictor afecta la respuesta</li>
<li><strong>Costo computacional</strong>: Generalmente requieren más recursos computacionales</li>
</ol>
</section>
<section id="ejemplos-de-métodos-no-paramétricos" class="level4">
<h4 class="anchored" data-anchor-id="ejemplos-de-métodos-no-paramétricos">Ejemplos de métodos no paramétricos</h4>
<p><strong>1. K-Nearest Neighbors (KNN)</strong></p>
<p>Para predecir <span class="math inline">\(Y\)</span> en un punto <span class="math inline">\(x_0\)</span>, KNN: - Identifica las <span class="math inline">\(K\)</span> observaciones en el conjunto de entrenamiento más cercanas a <span class="math inline">\(x_0\)</span> - Promedia los valores de <span class="math inline">\(Y\)</span> de esos <span class="math inline">\(K\)</span> vecinos más cercanos</p>
<p><span class="math display">\[\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in \mathcal{N}_K(x_0)} y_i\]</span></p>
<p>donde <span class="math inline">\(\mathcal{N}_K(x_0)\)</span> es el conjunto de los <span class="math inline">\(K\)</span> vecinos más cercanos a <span class="math inline">\(x_0\)</span>.</p>
<p><strong>2. Splines</strong></p>
<p>Los splines son funciones polinomiales por partes que: - Dividen el rango de <span class="math inline">\(X\)</span> en regiones - Ajustan diferentes polinomios de bajo grado en cada región - Garantizan suavidad en los puntos de unión</p>
<p><strong>3. Árboles de Decisión</strong></p>
<p>Los árboles: - Dividen el espacio de predictores en regiones rectangulares - Asignan una predicción constante a cada región - Son interpretables pero pueden sobreajustar fácilmente</p>
<p><strong>4. Métodos de Kernel</strong></p>
<p>Similar a KNN pero usando pesos que decaen suavemente con la distancia:</p>
<p><span class="math display">\[\hat{f}(x_0) = \frac{\sum_{i=1}^n K_h(x_0, x_i) y_i}{\sum_{i=1}^n K_h(x_0, x_i)}\]</span></p>
<p>donde <span class="math inline">\(K_h\)</span> es una función kernel con bandwidth <span class="math inline">\(h\)</span>.</p>
</section>
<section id="comparación-visual-paramétricos-vs-no-paramétricos" class="level4">
<h4 class="anchored" data-anchor-id="comparación-visual-paramétricos-vs-no-paramétricos">Comparación visual: Paramétricos vs No Paramétricos</h4>
<div id="cell-parametric-vs-nonparametric" class="cell" data-fig-height="10" data-fig-width="14" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="parametric-vs-nonparametric" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="02-principios_files/figure-html/parametric-vs-nonparametric-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación entre métodos paramétricos y no paramétricos</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Observaciones Clave
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Métodos Paramétricos (fila superior)</strong>: - Asumen una forma funcional específica (lineal, cuadrática, etc.) - Son más rígidos pero requieren menos datos - Más fáciles de interpretar - Pueden tener sesgo si la forma asumida es incorrecta</p>
<p><strong>Métodos No Paramétricos (fila inferior)</strong>: - No asumen forma funcional específica - Más flexibles, pueden capturar patrones complejos - Requieren más datos para estimar bien - El parámetro de suavizamiento (como <span class="math inline">\(k\)</span> en KNN) controla el tradeoff entre flexibilidad y suavidad</p>
<p><strong>¿Cuándo usar cada uno?</strong> - <strong>Paramétrico</strong>: Cuando tienes pocos datos, necesitas interpretabilidad, o conoces aproximadamente la forma de la relación - <strong>No paramétrico</strong>: Cuando tienes muchos datos, la relación es compleja, o la predicción es más importante que la interpretación</p>
</div>
</div>
</section>
</section>
</section>
<section id="bias-variance-tradeoff" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-tradeoff">Bias-Variance Tradeoff</h2>
<p>Uno de los conceptos más fundamentales en aprendizaje estadístico es el <strong>tradeoff entre sesgo y varianza</strong> (bias-variance tradeoff). Este concepto nos ayuda a entender por qué los modelos fallan y cómo elegir el nivel apropiado de complejidad del modelo.</p>
<section id="descomposición-matemática-del-error-esperado" class="level3">
<h3 class="anchored" data-anchor-id="descomposición-matemática-del-error-esperado">Descomposición Matemática del Error Esperado</h3>
<p>Supongamos que queremos predecir <span class="math inline">\(Y\)</span> usando un modelo <span class="math inline">\(\hat{f}(X)\)</span>. El <strong>error cuadrático medio esperado</strong> (expected MSE) para un punto <span class="math inline">\(x_0\)</span> se puede descomponer de la siguiente manera:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[(Y - \hat{f}(x_0))^2] &amp;= \mathbb{E}[(f(x_0) + \epsilon - \hat{f}(x_0))^2]\\
&amp;= \mathbb{E}[(f(x_0) - \hat{f}(x_0))^2] + \mathbb{E}[\epsilon^2] + 2\mathbb{E}[(f(x_0) - \hat{f}(x_0))]\mathbb{E}[\epsilon]\\
&amp;= \mathbb{E}[(f(x_0) - \hat{f}(x_0))^2] + \text{Var}(\epsilon)
\end{align*}\]</span></p>
<p>Ahora, descomponemos el primer término sumando y restando <span class="math inline">\(\mathbb{E}[\hat{f}(x_0)]\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[(f(x_0) - \hat{f}(x_0))^2] &amp;= \mathbb{E}[(f(x_0) - \mathbb{E}[\hat{f}(x_0)] + \mathbb{E}[\hat{f}(x_0)] - \hat{f}(x_0))^2]\\
&amp;= \mathbb{E}[(f(x_0) - \mathbb{E}[\hat{f}(x_0)])^2] + \mathbb{E}[(\mathbb{E}[\hat{f}(x_0)] - \hat{f}(x_0))^2]\\
&amp;\quad + 2\mathbb{E}[(f(x_0) - \mathbb{E}[\hat{f}(x_0)])(\mathbb{E}[\hat{f}(x_0)] - \hat{f}(x_0))]\\
&amp;= (f(x_0) - \mathbb{E}[\hat{f}(x_0)])^2 + \mathbb{E}[(\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)])^2]\\
&amp;= \text{Bias}^2[\hat{f}(x_0)] + \text{Var}[\hat{f}(x_0)]
\end{align*}\]</span></p>
<p>Por lo tanto, el <strong>error esperado total</strong> se descompone en tres componentes:</p>
<p><span class="math display">\[\boxed{\mathbb{E}[(Y - \hat{f}(x_0))^2] = \underset{\text{Sesgo}^2}{\underbrace{\text{Bias}^2[\hat{f}(x_0)]}} + \underset{\text{Varianza}}{\underbrace{\text{Var}[\hat{f}(x_0)]}} + \underset{\text{Ruido Irreducible}}{\underbrace{\text{Var}(\epsilon)}}}\]</span></p>
</section>
<section id="interpretación-de-los-componentes" class="level3">
<h3 class="anchored" data-anchor-id="interpretación-de-los-componentes">Interpretación de los Componentes</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Los Tres Componentes del Error
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>1. Sesgo (Bias)</strong> - <span class="math inline">\(\text{Bias}[\hat{f}(x_0)] = \mathbb{E}[\hat{f}(x_0)] - f(x_0)\)</span></p>
<p>El sesgo mide el <strong>error sistemático</strong> de nuestro modelo. Es la diferencia entre la predicción promedio de nuestro modelo (si pudiéramos entrenar infinitos modelos con diferentes conjuntos de entrenamiento) y el valor verdadero.</p>
<ul>
<li><strong>Sesgo alto</strong>: El modelo es demasiado simple, no captura la estructura subyacente</li>
<li><strong>Causa</strong>: Supuestos incorrectos sobre la forma de <span class="math inline">\(f\)</span> (ej: asumir linealidad cuando es cuadrática)</li>
<li><strong>Síntoma</strong>: Underfitting (subajuste) - mal desempeño tanto en entrenamiento como en prueba</li>
<li><strong>Ejemplos</strong>: Regresión lineal para datos claramente no lineales, KNN con <span class="math inline">\(k\)</span> muy grande</li>
</ul>
<p><strong>2. Varianza (Variance)</strong> - <span class="math inline">\(\text{Var}[\hat{f}(x_0)] = \mathbb{E}[(\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)])^2]\)</span></p>
<p>La varianza mide cuánto <strong>varían las predicciones</strong> del modelo si lo entrenamos con diferentes conjuntos de entrenamiento.</p>
<ul>
<li><strong>Varianza alta</strong>: El modelo es muy sensible a fluctuaciones en los datos de entrenamiento</li>
<li><strong>Causa</strong>: Modelo demasiado flexible que captura el ruido como si fuera señal</li>
<li><strong>Síntoma</strong>: Overfitting (sobreajuste) - excelente en entrenamiento, malo en prueba</li>
<li><strong>Ejemplos</strong>: Polinomios de grado muy alto, KNN con <span class="math inline">\(k=1\)</span>, árboles muy profundos</li>
</ul>
<p><strong>3. Ruido Irreducible</strong> - <span class="math inline">\(\text{Var}(\epsilon) = \sigma^2\)</span></p>
<p>Es la variabilidad intrínseca que no puede ser reducida sin importar qué modelo usemos.</p>
<ul>
<li>Representa el error mínimo alcanzable</li>
<li>Pone un límite inferior en la precisión de cualquier modelo</li>
<li>No depende de nuestro modelo o algoritmo</li>
</ul>
</div>
</div>
</section>
<section id="el-tradeoff-fundamental" class="level3">
<h3 class="anchored" data-anchor-id="el-tradeoff-fundamental">El Tradeoff Fundamental</h3>
<p>El término “tradeoff” surge porque hay una <strong>relación inversa</strong> entre sesgo y varianza:</p>
<ul>
<li><strong>Aumentar la complejidad del modelo</strong> → ↓ Sesgo, ↑ Varianza</li>
<li><strong>Disminuir la complejidad del modelo</strong> → ↑ Sesgo, ↓ Varianza</li>
</ul>
<p>Esta relación se puede visualizar como:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 21%">
<col style="width: 10%">
<col style="width: 15%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Complejidad del Modelo</th>
<th>Flexibilidad</th>
<th>Sesgo</th>
<th>Varianza</th>
<th>Ejemplo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Muy Baja</td>
<td>Rígido</td>
<td>↑↑ Alto</td>
<td>↓↓ Baja</td>
<td>Regresión lineal</td>
</tr>
<tr class="even">
<td>Baja</td>
<td>Poco flexible</td>
<td>↑ Moderado</td>
<td>↓ Baja</td>
<td>Polinomio grado 2</td>
</tr>
<tr class="odd">
<td><strong>Óptima</strong></td>
<td><strong>Balanceada</strong></td>
<td><strong>↔︎ Medio</strong></td>
<td><strong>↔︎ Media</strong></td>
<td><strong>Modelo ideal</strong></td>
</tr>
<tr class="even">
<td>Alta</td>
<td>Muy flexible</td>
<td>↓ Bajo</td>
<td>↑ Moderada</td>
<td>Polinomio grado 10</td>
</tr>
<tr class="odd">
<td>Muy Alta</td>
<td>Extremo</td>
<td>↓↓ Muy bajo</td>
<td>↑↑ Alta</td>
<td>KNN con k=1</td>
</tr>
</tbody>
</table>
</section>
<section id="conexión-con-overfitting-y-underfitting" class="level3">
<h3 class="anchored" data-anchor-id="conexión-con-overfitting-y-underfitting">Conexión con Overfitting y Underfitting</h3>
<p>El bias-variance tradeoff explica directamente los fenómenos de overfitting y underfitting:</p>
<p><strong>Underfitting (Subajuste)</strong> - Ocurre cuando el modelo es demasiado simple - Caracterizado por <strong>alto sesgo</strong> - El modelo no captura los patrones reales en los datos - Error alto tanto en entrenamiento como en prueba - Ejemplo: Usar regresión lineal para datos con relación cuadrática clara</p>
<p><strong>Overfitting (Sobreajuste)</strong> - Ocurre cuando el modelo es demasiado complejo - Caracterizado por <strong>alta varianza</strong> - El modelo memoriza el ruido en vez de aprender patrones generales - Error bajo en entrenamiento pero alto en prueba - Ejemplo: Polinomio de grado 20 con solo 30 observaciones</p>
<p><strong>Modelo Óptimo</strong> - Balance entre sesgo y varianza - Minimiza el error esperado total - Captura los patrones verdaderos sin memorizar el ruido - Generaliza bien a datos nuevos</p>
</section>
<section id="relación-con-parámetros-de-los-modelos" class="level3">
<h3 class="anchored" data-anchor-id="relación-con-parámetros-de-los-modelos">Relación con Parámetros de los Modelos</h3>
<p>Diferentes modelos tienen diferentes formas de controlar el tradeoff bias-variance:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 46%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Modelo</th>
<th>Parámetro de Complejidad</th>
<th>↑ Parámetro → Efecto</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>KNN</td>
<td><span class="math inline">\(k\)</span> (número de vecinos)</td>
<td>↑ Sesgo, ↓ Varianza</td>
</tr>
<tr class="even">
<td>Regresión Polinomial</td>
<td>Grado del polinomio</td>
<td>↓ Sesgo, ↑ Varianza</td>
</tr>
<tr class="odd">
<td>Árboles de Decisión</td>
<td>Profundidad máxima</td>
<td>↓ Sesgo, ↑ Varianza</td>
</tr>
<tr class="even">
<td>Regresión Ridge/Lasso</td>
<td><span class="math inline">\(\lambda\)</span> (penalización)</td>
<td>↑ Sesgo, ↓ Varianza</td>
</tr>
<tr class="odd">
<td>Redes Neuronales</td>
<td>Número de capas/neuronas</td>
<td>↓ Sesgo, ↑ Varianza</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Implicaciones Prácticas
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>No existe el modelo perfecto</strong>: Siempre habrá un tradeoff entre sesgo y varianza</p></li>
<li><p><strong>Más datos ayudan</strong>: Con más datos, podemos usar modelos más complejos sin aumentar tanto la varianza</p></li>
<li><p><strong>Validación cruzada es clave</strong>: Permite estimar dónde está el punto óptimo de complejidad</p></li>
<li><p><strong>Regularización controla el tradeoff</strong>: Técnicas como Ridge, Lasso, o early stopping permiten ajustar finamente este balance</p></li>
<li><p><strong>Conocer tu problema importa</strong>:</p>
<ul>
<li>Si tienes pocos datos → Prioriza modelos simples (mayor sesgo, menor varianza)</li>
<li>Si tienes muchos datos → Puedes usar modelos complejos (menor sesgo, mayor varianza)</li>
</ul></li>
</ol>
</div>
</div>
</section>
<section id="visualización-del-bias-variance-tradeoff" class="level3">
<h3 class="anchored" data-anchor-id="visualización-del-bias-variance-tradeoff">Visualización del Bias-Variance Tradeoff</h3>
<p>Veamos gráficamente cómo sesgo y varianza cambian con la complejidad del modelo:</p>
<div id="cell-bias-variance-tradeoff-curve" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="bias-variance-tradeoff-curve" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="02-principios_files/figure-html/bias-variance-tradeoff-curve-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Curva clásica del Bias-Variance Tradeoff mostrando el punto óptimo de complejidad</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretación de las Curvas
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Panel Izquierdo - Descomposición del Error</strong>: - <strong>Sesgo² (azul)</strong>: Disminuye cuando aumenta la complejidad del modelo - <strong>Varianza (roja)</strong>: Aumenta cuando aumenta la complejidad del modelo - <strong>Ruido Irreducible (gris)</strong>: Constante, no depende del modelo - <strong>Error Total (morado)</strong>: Suma de los tres componentes, tiene forma de U - <strong>Punto óptimo (verde)</strong>: Minimiza el error total, balancea sesgo y varianza</p>
<p><strong>Panel Derecho - Entrenamiento vs Prueba</strong>: - <strong>Error de Entrenamiento (azul)</strong>: Siempre disminuye con más complejidad - <strong>Error de Prueba (rojo)</strong>: Disminuye inicialmente, luego aumenta (overfitting) - El <strong>gap</strong> entre ambos aumenta con la complejidad → indica overfitting</p>
</div>
</div>
</section>
<section id="ejemplo-práctico-knn-con-diferentes-valores-de-k" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-práctico-knn-con-diferentes-valores-de-k">Ejemplo Práctico: KNN con Diferentes Valores de k</h3>
<p>Veamos cómo el bias-variance tradeoff se manifiesta en K-Nearest Neighbors:</p>
<div id="cell-bias-variance-knn-example" class="cell" data-fig-height="10" data-fig-width="14" data-execution_count="4">
<div class="cell-output cell-output-display">
<div id="bias-variance-knn-example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="02-principios_files/figure-html/bias-variance-knn-example-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Bias-Variance Tradeoff en KNN: Efecto del parámetro k</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Observaciones Clave del Experimento KNN
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>k=1 (Alta Complejidad)</strong>: - Las líneas grises individuales son muy diferentes entre sí → <strong>Alta varianza</strong> - La predicción promedio (roja) se acerca bastante a la función verdadera (azul) → <strong>Bajo sesgo</strong> - Problema: Cada modelo individual es muy inestable y sensible al ruido - <strong>Resultado</strong>: Overfitting - el modelo memoriza los datos específicos</p>
<p><strong>k=5 (Complejidad Media-Alta)</strong>: - Las líneas grises son más similares entre sí → <strong>Varianza moderada</strong> - La predicción promedio aún sigue bien la función verdadera → <strong>Sesgo bajo a moderado</strong> - <strong>Balance razonable</strong> para este problema</p>
<p><strong>k=20 (Complejidad Media-Baja)</strong>: - Las líneas grises son muy similares → <strong>Baja varianza</strong> - La predicción promedio pierde algo de la estructura verdadera → <strong>Sesgo moderado</strong> - Más estable pero menos preciso en capturar detalles</p>
<p><strong>k=50 (Baja Complejidad)</strong>: - Las líneas grises son casi idénticas → <strong>Muy baja varianza</strong> - La predicción promedio es demasiado suave, pierde oscilaciones → <strong>Alto sesgo</strong> - Problema: Modelo demasiado simple que no captura la estructura real - <strong>Resultado</strong>: Underfitting - el modelo es demasiado rígido</p>
<p><strong>Lección Principal</strong>: A medida que <span class="math inline">\(k\)</span> aumenta (complejidad disminuye): - ✓ Ganamos estabilidad (menor varianza) - ✗ Perdemos capacidad de ajuste (mayor sesgo)</p>
</div>
</div>
</section>
<section id="bias-variance-con-regresiones-polinomiales" class="level3">
<h3 class="anchored" data-anchor-id="bias-variance-con-regresiones-polinomiales">Bias-Variance con Regresiones Polinomiales</h3>
<p>Otro ejemplo clásico es el grado del polinomio:</p>
<div id="cell-bias-variance-polynomial" class="cell" data-fig-height="5" data-fig-width="14" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="bias-variance-polynomial" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="02-principios_files/figure-html/bias-variance-polynomial-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Bias-Variance Tradeoff con Regresiones Polinomiales de diferentes grados</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Resumen: Cómo Identificar y Corregir Problemas
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>¿Cómo saber si tienes alto sesgo (underfitting)?</strong> - ✓ Error de entrenamiento alto - ✓ Error de prueba alto - ✓ Poca diferencia entre error de entrenamiento y prueba - ✓ El modelo es demasiado simple para capturar los patrones</p>
<p><strong>Soluciones para alto sesgo</strong>: 1. Aumentar la complejidad del modelo (más características, mayor grado, menor k en KNN) 2. Reducir la regularización 3. Agregar características polinomiales o de interacción 4. Probar un algoritmo más flexible</p>
<p><strong>¿Cómo saber si tienes alta varianza (overfitting)?</strong> - ✓ Error de entrenamiento bajo - ✓ Error de prueba alto - ✓ Gran diferencia (gap) entre error de entrenamiento y prueba - ✓ El modelo es muy sensible a pequeños cambios en los datos</p>
<p><strong>Soluciones para alta varianza</strong>: 1. Reducir la complejidad del modelo (menos características, menor grado, mayor k en KNN) 2. Aumentar la regularización (Ridge, Lasso, dropout) 3. Obtener más datos de entrenamiento 4. Usar ensemble methods (bagging, random forests) 5. Aplicar validación cruzada para seleccionar hiperparámetros</p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiado");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiado");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-introduccion.html" class="pagination-link" aria-label="Introducción">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Introducción</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03-regresion_lineal.html" class="pagination-link" aria-label="Regresión lineal">
        <span class="nav-page-text"><span class="chapter-title">Regresión lineal</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>