[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Minería de Datos",
    "section": "",
    "text": "Temario\n\nIntroducción al aprendizaje de máquina\nPrincipios de aprendizaje supervisado\nRegresión lineal\nMétodos de remuestreo y validación cruzada\nPrincipios de Regularización\nProblemas de clasificación, métricas y evaluación\nÁrboles, bosques aleatorios y boosting\nRedes neuronales\nMétodos no supervisados\n\n\nEvaluación\n\nTarea (20%)\nExamen parcial (20%)\nProyecto final (30%):\n\nEntrega (75%)\nExposición (25%)\n\n\nExamen final (30%)\n\nExistirá una parte extra a los alumnos que contribuyan al aprendizaje de sus compañeros:\n\nContribuciones al repositorio: añadiendo redacción más entendible, añadiendo ejemplos particulares a sus carreras, etc.\nActividad en el canal de Slack: contestando dudas de sus compañeros, iniciando discusiones para resolver problemas.\n\n\n\nProfesor\nNombre: Saúl Caballero Ramírez\nCorreo: saul.caballero.ramirez@gmail.com\nCorreo alternativo: saul@nixtla.io\nEl canal más rápido y efectivo será el siguiente canal de Slack. La idea de este canal es que puedan comunicarse entre ustedes para ayudarse a aprender y si necesitan de mi ayuda intentaré contestar en un periodo corto de tiempo. Cualquier comportamiento inadecuado dentro de este foro será penalizado por las reglas de convivencia del ITAM.\n\n\nReferencias principales\n\nAn Introduction to Statistical Learning, James et al. (2023)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, y Jonathan Taylor. 2023. An Introduction to Statistical Learning: With Applications in Python. Springer Texts en Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Temario"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html",
    "href": "00-requerimientos-computacion.html",
    "title": "Requerimientos computacionales",
    "section": "",
    "text": "Git\nEste capítulo proporciona una guía completa para instalar y configurar las herramientas computacionales necesarias para el curso de Minería de Datos. Cubriremos la instalación y uso básico de Git, GitHub, y Conda en sistemas Windows y macOS.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#git",
    "href": "00-requerimientos-computacion.html#git",
    "title": "Requerimientos computacionales",
    "section": "",
    "text": "¿Qué es Git?\nGit es un sistema de control de versiones distribuido que permite rastrear cambios en archivos y coordinar el trabajo entre múltiples personas. Es esencial para el desarrollo de software y la gestión de proyectos de análisis de datos.\nCaracterísticas principales: - Control de versiones: mantiene un historial completo de cambios - Trabajo colaborativo: permite que múltiples personas trabajen en el mismo proyecto - Ramificación (branching): facilita el desarrollo de características en paralelo - Respaldo distribuido: cada copia del repositorio es un respaldo completo\n\n\nInstalación de Git\n\nWindows\n\nDescargar Git:\n\nVisita https://git-scm.com/download/win\nDescarga la versión más reciente para Windows\n\nInstalación:\n\nEjecuta el archivo descargado\nAcepta las opciones por defecto (recomendado para principiantes)\nImportante: asegúrate de seleccionar “Git Bash Here” durante la instalación\n\nVerificar instalación:\ngit --version\n\n\n\nmacOS\nOpción 1: Usando Homebrew (recomendado)\n# Instalar Homebrew si no lo tienes\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Instalar Git\nbrew install git\nOpción 2: Descarga directa 1. Visita https://git-scm.com/download/mac 2. Descarga e instala el paquete\nOpción 3: Xcode Command Line Tools\nxcode-select --install\n\n\n\nConfiguración inicial de Git\nDespués de instalar Git, configura tu identidad:\ngit config --global user.name \"Tu Nombre\"\ngit config --global user.email \"tu.email@ejemplo.com\"\n\n\nComandos básicos de Git\n\nInicializar un repositorio\n# Crear un nuevo repositorio\ngit init\n\n# Clonar un repositorio existente\ngit clone https://github.com/usuario/repositorio.git\n\n\nOperaciones básicas\n# Ver el estado del repositorio\ngit status\n\n# Añadir archivos al área de staging\ngit add archivo.py\ngit add .  # Añadir todos los archivos\n\n# Crear un commit\ngit commit -m \"Mensaje descriptivo del cambio\"\n\n# Ver el historial de commits\ngit log --oneline\n\n\nTrabajar con repositorios remotos\n# Ver repositorios remotos configurados\ngit remote -v\n\n# Añadir un repositorio remoto\ngit remote add origin https://github.com/usuario/repositorio.git\n\n# Enviar cambios al repositorio remoto\ngit push origin main\n\n# Obtener cambios del repositorio remoto\ngit pull origin main",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#github",
    "href": "00-requerimientos-computacion.html#github",
    "title": "Requerimientos computacionales",
    "section": "GitHub",
    "text": "GitHub\n\n¿Qué es GitHub?\nGitHub es una plataforma de desarrollo colaborativo basada en Git que permite: - Hospedar repositorios de código - Colaborar en proyectos - Realizar seguimiento de issues y bugs - Automatizar workflows - Crear documentación con GitHub Pages\n\n\nCreación de cuenta en GitHub\n\nVisita https://github.com\nHaz clic en “Sign up”\nCompleta el formulario de registro\nVerifica tu email\nConfigura tu perfil\n\n\n\nConectar Git local con GitHub\n\nAutenticación con Token Personal\n\nCrear un Personal Access Token:\n\nVe a GitHub → Settings → Developer settings → Personal access tokens → Tokens (classic)\nGenerate new token (classic)\nSelecciona los permisos necesarios: repo, workflow\nGuarda el token en un lugar seguro\n\nUsar el token:\n# Al hacer push por primera vez, usa tu username y el token como password\ngit push origin main\n\n\n\nAutenticación con SSH (recomendado para uso avanzado)\n\nGenerar clave SSH:\nssh-keygen -t ed25519 -C \"tu.email@ejemplo.com\"\nAñadir la clave a GitHub:\n# Copiar la clave pública\ncat ~/.ssh/id_ed25519.pub\n\nVe a GitHub → Settings → SSH and GPG keys → New SSH key\nPega la clave pública\n\nProbar la conexión:\nssh -T git@github.com",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#pull-requests",
    "href": "00-requerimientos-computacion.html#pull-requests",
    "title": "Requerimientos computacionales",
    "section": "Pull Requests",
    "text": "Pull Requests\n\n¿Qué son los Pull Requests?\nLos Pull Requests (PRs) son una funcionalidad de GitHub que permite: - Proponer cambios a un repositorio - Revisar código antes de integrarlo - Discutir modificaciones - Mantener un historial de cambios\n\n\nFlujo de trabajo con Pull Requests\n\n1. Crear una nueva rama\n# Crear y cambiar a una nueva rama\ngit checkout -b nueva-caracteristica\n\n# O usando el comando más moderno\ngit switch -c nueva-caracteristica\n\n\n2. Realizar cambios y commits\n# Hacer cambios en tu código\n# ...\n\n# Añadir y commitear cambios\ngit add .\ngit commit -m \"Implementa nueva característica\"\n\n\n3. Subir la rama a GitHub\ngit push origin nueva-caracteristica\n\n\n4. Crear el Pull Request\n\nVe a tu repositorio en GitHub\nHaz clic en “Compare & pull request”\nAñade un título y descripción clara\nSelecciona los revisores si es necesario\nHaz clic en “Create pull request”\n\n\n\n5. Proceso de revisión\n\nLos revisores pueden añadir comentarios\nPuedes hacer cambios adicionales con nuevos commits\nUna vez aprobado, el PR puede ser fusionado\n\n\n\n6. Fusionar y limpiar\n# Después de que se fusione el PR, actualiza tu rama principal\ngit checkout main\ngit pull origin main\n\n# Elimina la rama local\ngit branch -d nueva-caracteristica",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#conda",
    "href": "00-requerimientos-computacion.html#conda",
    "title": "Requerimientos computacionales",
    "section": "Conda",
    "text": "Conda\n\n¿Qué es Conda?\nConda es un gestor de paquetes y entornos de código abierto que: - Instala, ejecuta y actualiza paquetes y sus dependencias - Crea, guarda, carga y cambia entre entornos - Funciona con cualquier lenguaje (Python, R, Ruby, Lua, Scala, Java, etc.) - Es multiplataforma (Windows, macOS, Linux)\n\n\nInstalación de Conda\n\nOpción 1: Miniconda (recomendado)\nWindows: 1. Descargar desde https://docs.conda.io/en/latest/miniconda.html 2. Ejecutar el instalador 3. Seguir las instrucciones por defecto 4. Reiniciar la terminal\nmacOS:\n# Descargar e instalar con curl\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\n# Para Apple Silicon (M1/M2)\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nbash Miniconda3-latest-MacOSX-arm64.sh\n\n\nOpción 2: Anaconda (instalación completa)\n\nDescargar desde https://www.anaconda.com/products/distribution\nSeguir las instrucciones de instalación para tu sistema operativo\n\n\n\n\nVerificar la instalación\nconda --version\nconda info",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#gestión-de-entornos-con-conda",
    "href": "00-requerimientos-computacion.html#gestión-de-entornos-con-conda",
    "title": "Requerimientos computacionales",
    "section": "Gestión de Entornos con Conda",
    "text": "Gestión de Entornos con Conda\n\n¿Por qué usar entornos?\nLos entornos virtuales permiten: - Aislar dependencias entre proyectos - Usar diferentes versiones de Python/paquetes - Evitar conflictos entre bibliotecas - Reproducir entornos de trabajo\n\n\nComandos básicos para entornos\n\nCrear entornos\n# Crear un entorno con Python específico\nconda create -n mi_entorno python=3.10\n\n# Crear un entorno con paquetes específicos\nconda create -n ciencia_datos python=3.10 numpy pandas matplotlib\n\n# Crear entorno desde un archivo\nconda env create -f environment.yml\n\n\nActivar y desactivar entornos\n# Activar un entorno\nconda activate mi_entorno\n\n# Desactivar el entorno actual\nconda deactivate\n\n# Listar entornos disponibles\nconda env list\n\n\nGestionar paquetes en entornos\n# Instalar paquetes\nconda install numpy pandas scikit-learn\n\n# Instalar desde conda-forge (recomendado)\nconda install -c conda-forge seaborn\n\n# Instalar múltiples paquetes\nconda install jupyter matplotlib seaborn\n\n# Listar paquetes instalados\nconda list\n\n# Actualizar paquetes\nconda update numpy\nconda update --all\n\n\nEliminar entornos\n# Eliminar un entorno\nconda env remove -n mi_entorno\n\n\n\nArchivo environment.yml\nPara reproducir entornos, crea un archivo environment.yml:\nname: mineria_datos\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - jupyter\n  - numpy\n  - pandas\n  - matplotlib\n  - seaborn\n  - scikit-learn\n  - pip\n  - pip:\n    - some-pip-package\nCrear el entorno desde el archivo:\nconda env create -f environment.yml\n\n\nMejores prácticas\n\nUn entorno por proyecto: Crea un entorno específico para cada proyecto\nUsa conda-forge: Preferir el canal conda-forge para paquetes actualizados\nExporta tus entornos: Mantén archivos environment.yml para reproducibilidad\nActualiza regularmente: Mantén tus entornos actualizados\nDocumenta dependencias: Incluye versiones específicas cuando sea crítico\n\n\n\nSolución de problemas comunes\n\nProblema: Conda no se reconoce en la terminal\nSolución: Reinicia la terminal o añade conda al PATH:\n# Windows (en Command Prompt como administrador)\nconda init cmd.exe\n\n# macOS/Linux\nconda init bash\n\n\nProblema: Conflictos de paquetes\nSolución: Usa mamba (más rápido) o especifica canales:\n# Instalar mamba\nconda install mamba -n base -c conda-forge\n\n# Usar mamba en lugar de conda\nmamba install numpy pandas\n\n\nProblema: Entorno no se activa\nSolución: Verifica la instalación y configuración:\nconda info --envs\nconda config --show",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#configuración-de-conda-en-diferentes-entornos",
    "href": "00-requerimientos-computacion.html#configuración-de-conda-en-diferentes-entornos",
    "title": "Requerimientos computacionales",
    "section": "Configuración de Conda en diferentes entornos",
    "text": "Configuración de Conda en diferentes entornos\n\nConda en Windows con Git Bash\nGit Bash en Windows puede requerir configuración adicional para funcionar correctamente con Conda:\n\n1. Configuración inicial en Git Bash\nDespués de instalar Miniconda en Windows, es posible que conda no esté disponible en Git Bash por defecto:\n# Verificar si conda está disponible\nconda --version\nSi no funciona, necesitas inicializar conda para Git Bash:\n# Navega al directorio de instalación de Miniconda (ajusta la ruta según tu instalación)\ncd /c/Users/TuUsuario/miniconda3/Scripts\n\n# Inicializar conda para bash\n./conda.exe init bash\n\n\n2. Alternativa: Añadir conda al PATH manualmente\nSi la inicialización no funciona, añade conda manualmente al PATH en Git Bash:\n# Añadir estas líneas a tu archivo ~/.bashrc\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3:$PATH\"' &gt;&gt; ~/.bashrc\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3/Scripts:$PATH\"' &gt;&gt; ~/.bashrc\n\n# Recargar el archivo bashrc\nsource ~/.bashrc\n\n\n3. Verificar la configuración\n# Reiniciar Git Bash y verificar\nconda --version\nconda info\n\n\n4. Trabajar con entornos en Git Bash\n# Crear y activar entorno\nconda create -n mineria_datos python=3.10\nconda activate mineria_datos\n\n# Si aparece un error de activación, usa:\nsource activate mineria_datos\n\n\n\nTroubleshooting específico para Windows\n\nProblema: “conda: command not found” en Git Bash\nSoluciones:\n\nReinstalar Miniconda con “Add to PATH” marcado\nUsar el Anaconda Prompt (más confiable en Windows)\nConfigurar manualmente el PATH:\n\n# En ~/.bashrc, añade:\nalias conda='/c/Users/TuUsuario/miniconda3/Scripts/conda.exe'\nalias python='/c/Users/TuUsuario/miniconda3/python.exe'\n\n\nProblema: Activación de entorno no funciona\n# En lugar de conda activate, usa:\nsource /c/Users/TuUsuario/miniconda3/etc/profile.d/conda.sh\nconda activate mineria_datos",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#visual-studio-code",
    "href": "00-requerimientos-computacion.html#visual-studio-code",
    "title": "Requerimientos computacionales",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\n¿Por qué usar VS Code para ciencia de datos?\nVisual Studio Code es un editor de código ligero pero potente que ofrece: - Soporte excelente para Python y Jupyter notebooks - Integración nativa con conda y entornos virtuales - Extensiones específicas para ciencia de datos - Terminal integrado - Control de versiones Git integrado - IntelliSense y debugging avanzado\n\n\nInstalación de Visual Studio Code\n\nWindows y macOS\n\nDescargar desde https://code.visualstudio.com/\nEjecutar el instalador siguiendo las instrucciones por defecto\nReiniciar el sistema si es necesario\n\n\n\n\nExtensiones esenciales para ciencia de datos\nInstala estas extensiones desde el marketplace de VS Code (Ctrl/Cmd + Shift + X):\n\nExtensiones obligatorias:\n- Python (Microsoft)\n- Jupyter (Microsoft)\n- Python Debugger (Microsoft)\n\n\nExtensiones recomendadas:\n- GitLens — Git supercharged\n- Pylance (análisis avanzado de Python)\n- autoDocstring - Python Docstring Generator\n- Python Indent\n- Bracket Pair Colorizer\n- Data Wrangler (Microsoft)\n\n\n\nConfiguración de Python y Conda en VS Code\n\n1. Seleccionar el intérprete de Python\n\nAbre VS Code\nPresiona Ctrl+Shift+P (Windows) o Cmd+Shift+P (macOS)\nEscribe “Python: Select Interpreter”\nSelecciona el intérprete del entorno mineria_datos\n\nLa ruta debería ser similar a: - Windows: C:\\Users\\TuUsuario\\miniconda3\\envs\\mineria_datos\\python.exe - macOS: /Users/TuUsuario/miniconda3/envs/mineria_datos/bin/python\n\n\n2. Verificar la configuración\nCrea un archivo de prueba test.py:\nimport sys\nprint(f\"Python path: {sys.executable}\")\nprint(f\"Python version: {sys.version}\")\n\n# Verificar paquetes\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nprint(\"¡Todos los paquetes importados correctamente!\")\n\n\n3. Configurar terminal integrado\nEn VS Code, abre el terminal integrado (Ctrl+`` o View → Terminal) y configura conda:\nWindows:\n# Si usas Git Bash en VS Code\nconda activate mineria_datos\nmacOS:\nconda activate mineria_datos\n\n\n\nTrabajar con Jupyter Notebooks en VS Code\n\n1. Crear un nuevo notebook\n\nCtrl+Shift+P → “Jupyter: Create New Jupyter Notebook”\nO crear un archivo con extensión .ipynb\n\n\n\n2. Seleccionar kernel\n\nEn la esquina superior derecha del notebook, haz clic en “Select Kernel”\nSelecciona “Python Environments”\nEscoge el entorno mineria_datos\n\n\n\n3. Verificar configuración del notebook\n# Primera celda del notebook\nimport sys\nprint(f\"Ejecutándose en: {sys.executable}\")\n\n# Importar bibliotecas del curso\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nprint(\"Notebook configurado correctamente para el curso!\")\n\n\n\nConfiguración avanzada de VS Code\n\nConfiguración del workspace\nCrea un archivo .vscode/settings.json en tu directorio del proyecto:\n{\n    \"python.defaultInterpreterPath\": \"./mineria_datos/bin/python\",\n    \"python.terminal.activateEnvironment\": true,\n    \"jupyter.defaultKernel\": \"mineria_datos\",\n    \"files.associations\": {\n        \"*.qmd\": \"markdown\"\n    },\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"editor.formatOnSave\": true\n}\n\n\nAtajos de teclado útiles\n\nCtrl+Shift+P: Command Palette\nCtrl+``: Toggle Terminal\nShift+Enter: Ejecutar celda de notebook\nCtrl+Enter: Ejecutar celda sin avanzar\nF5: Iniciar debugging\nCtrl+Shift+G: Control de versiones Git\n\n\n\n\nIntegración con Git en VS Code\n\n1. Clonar repositorio\n\nCtrl+Shift+P → “Git: Clone”\nPega la URL del repositorio\nSelecciona la carpeta destino\n\n\n\n2. Operaciones Git básicas\n\nSource Control panel (Ctrl+Shift+G): Ver cambios\nStage changes: Hacer clic en el “+” junto a los archivos\nCommit: Escribir mensaje y presionar Ctrl+Enter\nPush/Pull: Usar los botones en la barra de estado\n\n\n\n3. Crear branches\n\nClic en el nombre de la branch en la barra de estado\n“Create new branch”\nEscribir el nombre de la nueva branch",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#configuración-completa-del-entorno-para-el-curso",
    "href": "00-requerimientos-computacion.html#configuración-completa-del-entorno-para-el-curso",
    "title": "Requerimientos computacionales",
    "section": "Configuración completa del entorno para el curso",
    "text": "Configuración completa del entorno para el curso\n\nPasos de configuración paso a paso\n\n1. Instalar las herramientas base\n\nGit (siguiendo las instrucciones de instalación según tu sistema operativo)\nConda (Miniconda recomendado)\nVisual Studio Code (opcional pero recomendado)\n\n\n\n2. Crear y configurar el entorno de conda\nEn cualquier terminal (Command Prompt, Git Bash, Terminal de macOS):\n# Crear el entorno del curso\nconda create -n mineria_datos python=3.10\n\n# Activar el entorno\nconda activate mineria_datos\nSi tienes problemas con conda activate en Windows Git Bash:\n# Alternativa para Windows Git Bash\nsource activate mineria_datos\n\n\n3. Instalar paquetes necesarios\nOpción recomendada (con mamba para mayor velocidad):\n# Instalar mamba primero\nconda install mamba -c conda-forge\n\n# Instalar todos los paquetes del curso\nmamba install jupyter numpy pandas matplotlib seaborn scikit-learn nbclient ipykernel pyyaml plotly -y\nOpción alternativa (solo con conda):\nconda install jupyter numpy pandas matplotlib seaborn scikit-learn nbclient ipykernel pyyaml plotly -y\n\n\n4. Configurar Jupyter para el entorno\n# Registrar el entorno como kernel de Jupyter\npython -m ipykernel install --user --name mineria_datos --display-name \"Python (Minería de Datos)\"\n\n# Verificar que el kernel se registró correctamente\njupyter kernelspec list\n\n\n5. Verificar la instalación\n# Probar que todos los paquetes se importan correctamente\npython -c \"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport plotly\nprint('✅ ¡Todos los paquetes instalados correctamente!')\nprint(f'Python version: {pd.__version__}')\nprint(f'Pandas version: {pd.__version__}')\nprint(f'NumPy version: {np.__version__}')\nprint(f'Scikit-learn version: {sklearn.__version__}')\n\"\n\n\n\nConfiguración específica por entorno\n\nPara usuarios de Windows Git Bash\nSi planeas usar Git Bash como tu terminal principal:\n# Añadir conda al PATH permanentemente\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3/Scripts:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Verificar configuración\nconda info\n\n\nPara usuarios de Visual Studio Code\n\nInstalar VS Code y extensiones:\n\nPython (Microsoft)\nJupyter (Microsoft)\nPython Debugger (Microsoft)\n\nConfigurar el intérprete:\n\nCtrl+Shift+P → “Python: Select Interpreter”\nSeleccionar el intérprete del entorno mineria_datos\n\nCrear archivo de configuración del workspace:\n\nCrea .vscode/settings.json en tu directorio de proyecto:\n{\n    \"python.defaultInterpreterPath\": \"~/miniconda3/envs/mineria_datos/bin/python\",\n    \"python.terminal.activateEnvironment\": true,\n    \"jupyter.defaultKernel\": \"mineria_datos\",\n    \"files.associations\": {\n        \"*.qmd\": \"markdown\"\n    }\n}\n\n\nPara usuarios de Jupyter Lab (opcional)\nSi prefieres usar JupyterLab en lugar de notebooks en VS Code:\n# Instalar JupyterLab\nconda install jupyterlab\n\n# Iniciar JupyterLab\njupyter lab\n\n\n\nClonar el repositorio del curso\nUna vez configurado tu entorno, clona el repositorio del curso:\n# Clonar el repositorio (sustituye por la URL real del curso)\ngit clone [URL_DEL_REPOSITORIO_DEL_CURSO]\n\n# Navegar al directorio\ncd nombre-del-repositorio\n\n# Activar el entorno\nconda activate mineria_datos\n\n# Si hay un archivo environment.yml, úsalo para instalar dependencias adicionales\nconda env update -f environment.yml\n\n\nFlujo de trabajo recomendado\n\nIniciar sesión de trabajo:\n# Activar entorno\nconda activate mineria_datos\n\n# Navegar al directorio del proyecto\ncd ruta/al/proyecto\n\n# Abrir VS Code (si lo usas)\ncode .\nPara cada tarea/proyecto:\n\nCrear una nueva branch en Git\nTrabajar en tus notebooks/scripts\nHacer commits regularmente\nCrear Pull Request cuando esté listo\n\nMantener el entorno actualizado:\n# Actualizar paquetes periódicamente\nconda update --all\n\n\n\nVerificación final\nEjecuta este script para verificar que todo está configurado correctamente:\n# test_setup.py\nimport sys\nimport subprocess\n\ndef test_environment():\n    print(\"🔍 Verificando configuración del entorno...\")\n    \n    # Verificar Python\n    print(f\"✅ Python: {sys.version}\")\n    print(f\"✅ Ejecutable: {sys.executable}\")\n    \n    # Verificar paquetes críticos\n    required_packages = [\n        'pandas', 'numpy', 'matplotlib', 'seaborn', \n        'sklearn', 'jupyter', 'plotly'\n    ]\n    \n    for package in required_packages:\n        try:\n            __import__(package)\n            print(f\"✅ {package}: disponible\")\n        except ImportError:\n            print(f\"❌ {package}: NO disponible\")\n    \n    # Verificar conda\n    try:\n        result = subprocess.run(['conda', '--version'], \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"✅ Conda: {result.stdout.strip()}\")\n        else:\n            print(\"❌ Conda: no disponible\")\n    except FileNotFoundError:\n        print(\"❌ Conda: no encontrado en PATH\")\n    \n    # Verificar git\n    try:\n        result = subprocess.run(['git', '--version'], \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"✅ Git: {result.stdout.strip()}\")\n        else:\n            print(\"❌ Git: no disponible\")\n    except FileNotFoundError:\n        print(\"❌ Git: no encontrado en PATH\")\n    \n    print(\"\\n🎉 ¡Verificación completa!\")\n\nif __name__ == \"__main__\":\n    test_environment()\nGuarda este script como test_setup.py y ejecútalo:\npython test_setup.py\n¡Con esta configuración completa, estarás listo para trabajar eficientemente en todos los proyectos del curso de Minería de Datos!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "Introducción",
    "section": "",
    "text": "¿Qué es aprendizaje de máquina?\nMétodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe también aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisión afecta directa e inmediatamente al entorno.\nLas tareas más apropiadas para este enfoque, en general, son aquellas en donde:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#qué-es-aprendizaje-de-máquina",
    "href": "01-introduccion.html#qué-es-aprendizaje-de-máquina",
    "title": "Introducción",
    "section": "",
    "text": "Existe una cantidad considerable de datos relevantes para aprender a ejecutar la tarea.\nEl costo por errores al ejecutar la tarea es relativamente bajo (al menos comparado con alternativas).\nLa tarea se repite de manera más o menos homogénea una cantidad grande de veces.\n\n\nEjemplos de tareas de aprendizaje:\n\nPredecir si un cliente de tarjeta de crédito va a caer en impago en los próximos doce meses.\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica.\nDividir a los clientes de Netflix según sus gustos.\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\nReconocer un tipos de documentos (identificación, comprobante de domicilio, comprobante de ingresos) para acelerar el proceso de evaluación de crédito.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender más del problema que nos interesa: estas soluciones forman parte de un ciclo de análisis de datos donde podemos aprender de una forma más concentrada cuáles son características y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc). Las razones para no tomar un enfoque de reglas construidas “a mano”:\n\nCuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¿qué búsquedas www se enfocan en dar direcciones como resultados? ¿cómo filtrar comentarios no aceptables en foros?\nFinalmente, notamos que en estos problemas nuestro interés principal no es entender qué variables influyen en otras (en el proceso natural o de negocio). Sin más teoría o diseño de datos, los métodos que utilizaremos explotan patrones en los datos que no necesariamente explican cómo funcionan los sistemas de interés.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "Introducción",
    "section": "Aprendizaje supervisado y no supervisado",
    "text": "Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión\nProblemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-principios.html",
    "href": "02-principios.html",
    "title": "Principios de aprendizaje supervisado",
    "section": "",
    "text": "Definición de aprendizaje supervisado\nSupongamos que observamos una variable cuantitativa \\(Y \\in \\mathbb{R}\\) y tenemos \\(p\\) variables predictoras, \\(X_1, X_2, ..., X_p\\), las cuales denotaremos como \\(X = (X_1, X_2, ..., X_p)\\). Supongamos que existe alguna reluación entre ellas y se puede expresar de la siguiente forma:\n\\[ Y = f(x) + \\epsilon\\]\nLa tarea del aprendizaje supervisado es aprender la función \\(f\\). Existen dos razones por las cuales estimar \\(f\\): predicción e inferencia.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#definición-de-aprendizaje-supervisado",
    "href": "02-principios.html#definición-de-aprendizaje-supervisado",
    "title": "Principios de aprendizaje supervisado",
    "section": "",
    "text": "Función \\(f\\): función desconocida que relaciona a \\(X\\) con \\(Y\\). Representa la información sistémica que \\(X\\) aporta a \\(Y\\).\nError \\(\\epsilon\\): representa qué tan equivocados estamos con respecto al verdadero valor de \\(Y\\).\n\n\n\nPredicción\nEn muchas ocasiones existen un conjunto de variables \\(X\\) que están listas para aprovecharse, sin embargo, puede que no se pueda obtener la variable \\(Y\\) de manera inmediata. En este sentido, podemos predecir la variable \\(Y\\) siguiendo la ecuación:\n\\[\\hat{Y} = \\hat{f}(X)\\]\ndonde \\(\\hat{f}\\) representa nuestro estimador de \\(f\\) y \\(\\hat{Y}\\) es nuestra predicción de \\(Y\\). En este sentido \\(\\hat{f}\\) es una caja negra en el sentido en el que no nos preocupa cuál es la función, sino que provee predicciones precisas para \\(Y\\).\nLa precisión de \\(\\hat{Y}\\) depende de dos cantidades:\n\nError reducible: En general, \\(\\hat{f}\\) no será un estimador perfecto de \\(f\\) y esto introducirá un error el cuál puede reducirse. Ejemplos: Introducir una estructura lineal cuándo el problema tiene estructura cuadrática, falta de variables explicativas, exceso de variables que no contribuyen a la predicción.\nError ireducible: La variable \\(Y\\) es una función también de \\(\\epsilon\\) y por definición nuestra predicción tendra un error inherente. Ejemplos: Predecir que comerán mañana, determinar si lloverá o no, determinar cuándo ocurrirá un temblor, ¿quién ganará una elección?.\n\n\\[ \\begin{align*}\n\\mathbb{E}[(Y-\\hat{Y})^2] &= \\mathbb{E}[(f(X) + \\epsilon -\\hat{f}(x))^2]\\\\\n&= \\underset{Reducible}{\\underbrace{\\mathbb{E}[(f(X) - \\hat{f}(x))^2]}} + \\underset{Irreducible}{\\underbrace{\\text{Var}(\\epsilon)}}\n\\end{align*}\\]\nEl objetivo del curso se enfoca en técnicas para estimar \\(f\\) con el objectivo de minimizar el error reducible. Es importante tener en cuenta que el error irreducible siempre nos pondrá una cota en la predicción de \\(Y\\).\n\n\nInferencia\nExisten problemas en donde nos interesa más entender la relación intrinseca que existe entre \\(Y\\) y \\(X\\). En esta situación nuestro objetivo no es hacer predicción, entonces \\(\\hat{f}\\) ya no puede ser tratada como una caja negra. En este tipo de enfoque se contestan preguntas cómo:\n\n¿Cuáles son los predictores que se asocian con la variable \\(Y\\)?: Muchas veces solo un subconjunto de los datos \\(X\\) son los que realmente están relacionados con \\(Y\\).\n¿Cuál es la relación entre \\(Y\\) y \\(X_i\\)?\n¿La relación entre \\(Y\\) y \\(X_i\\) es lineal o más compleja?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#cómo-estimar-f",
    "href": "02-principios.html#cómo-estimar-f",
    "title": "Principios de aprendizaje supervisado",
    "section": "¿Cómo estimar \\(f\\)?",
    "text": "¿Cómo estimar \\(f\\)?\nAsumiremos que tenemos \\(n\\) datos diferentes estas observaciones serán llamadas conjunto de entrenamiento. \\(x_{ij}\\) representa el valor del predictor \\(j\\) para la observación \\(i\\), donde \\(i=1,2,...,n\\) y \\(j=1,2,...,p\\). \\(y_i\\) representa la variable respuesta de la observación \\(i\\). Entonces nuestro conjunto de entrenamiento consiste en:\n\\[{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}\\]\ndonde \\(x_i=(x_{i1}, x_{i2}, ..., x_{ip})^T\\).\nNuestro objetivo es aplicar un método de aprendizaje en el conjunto de datos para poder estimar una función desconocida de \\(f\\). Nos encantaría encontrar una función \\(\\hat{f}\\) de forma tal que \\(Y\\simeq \\hat{f}(X)\\) para cualquier observación \\((X, Y)\\). Muchos de estos enfoque se pueden caracterizar como métodos paramétricos o no paramétricos.\n\nMétodos paramétricos\nLos métodos paramétricos involucran un enfoque de dos pasos:\n\nHacemos un supuesto de la forma función de \\(f\\). Por ejemplo, la más sencilla es que \\(f\\) es linear en \\(\\beta\\):\n\n\\[ f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]\nUna vez haciendo haciendo el supuesto de linealidad el problema de estimar \\(f\\) es simplificado ya que en lugar de explorar el espacio funcional uno solo necesita estimar \\(p+1\\) coeficientes \\(\\beta_0, ..., \\beta_p\\).\n\nNecesitamos un proceso que utilice los datos de entrenamiento para ajustar u entrenar el modelo. El enfoque más sencillo es el método de mínimos cuadrados ordinarios (OLS):\n\n\\[\\underset{\\beta_0, \\beta_1, ..., \\beta_p}{min} \\sum_{i=1}^{N}(y_i - (\\beta_0 + \\beta_1 x_{i1} +\\beta_2 x_{i2} + ... + \\beta_p X_p))^2\\]\nEl enfoque basado en modelado se refiere a los modelos paramétricos; reduce el problema de estimar \\(f\\) a estimar un conjunto de parámetros. La desventaja potencial es que el modelo podría no ser igual a la verdadera \\(f\\) y tendremos malas estimaciones del valor de \\(y\\).\n\n\nMétodos no paramétricos",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html",
    "href": "03-regresion_lineal.html",
    "title": "Regresión lineal",
    "section": "",
    "text": "Regresión Lineal Simple\nComenzaremos con el caso más sencillo: predecir una variable de resultado Y a partir de una única variable predictora X.\nEl modelo matemático que queremos ajustar es una línea recta:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nDonde:\nNuestro objetivo 🎯 es encontrar los mejores valores posibles para los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) usando los datos que tenemos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresión-lineal-simple",
    "href": "03-regresion_lineal.html#regresión-lineal-simple",
    "title": "Regresión lineal",
    "section": "",
    "text": "\\(Y\\): La variable dependiente (lo que queremos predecir).\n\\(X\\): La variable independiente (nuestro predictor).\n\\(\\beta_0\\): El intercepto (el valor de \\(Y\\) cuando \\(X=0\\)).\n\\(\\beta_1\\): La pendiente (cuánto cambia \\(Y\\) por cada unidad que aumenta \\(X\\)).\n\\(\\epsilon\\): El término de error (la parte de \\(Y\\) que nuestro modelo no puede explicar).\n\n\n\n¿Cómo estimamos los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\)?\n“Mejor” para nosotros significa encontrar la línea que minimice la distancia vertical entre cada punto de dato y la propia línea. Específicamente, minimizamos la Suma de los Errores al Cuadrado (SEC o Sum of Squared Errors, SSE).\nLa función de costo (o pérdida) que queremos minimizar es:\n\\[J(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\\]\nTenemos dos métodos principales para encontrar los \\(\\beta\\) que minimizan esta función:\n\nMétodo 1: Las Ecuaciones Normales (La solución analítica 🧠)\nEste método utiliza cálculo para encontrar el mínimo exacto de la función de costo. Para ello, tomamos las derivadas parciales de \\(J\\) con respecto a \\(\\beta_0\\) y \\(\\beta_1\\), las igualamos a cero y resolvemos para los coeficientes.\n\n\n\n\n\n\nDerivada parcial con respecto a \\(\\beta_0\\):\n\n\n\n\n\n\\[\\frac{\\partial J}{\\partial \\beta_0} = \\sum_{i=1}^{n} -2(y_i - \\beta_0 - \\beta_1 x_i) = 0\\] \\[\\sum y_i - n\\beta_0 - \\beta_1 \\sum x_i = 0\\] \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\n\n\n\n\n\n\n\n\n\nDerivada parcial con respecto a \\(\\beta_1\\)\n\n\n\n\n\n\\[\\frac{\\partial J}{\\partial \\beta_1} = \\sum_{i=1}^{n} -2x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0\\] Sustituyendo \\(\\beta_0\\) de la primera ecuación y resolviendo, llegamos a: \\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\n\n\n\nEstas fórmulas nos dan los valores óptimos y exactos de los coeficientes directamente a partir de los datos.\n\n\nMétodo 2: Descenso en Gradiente (La solución iterativa ⚙️)\nEste es un método computacional que nos “acerca” progresivamente a la solución. Es especialmente útil cuando tenemos una cantidad masiva de datos y calcular la solución analítica es muy costoso.\nLa intuición: Imagina que estás en una montaña (la función de costo) y quieres llegar al valle (el costo mínimo). El Descenso en Gradiente te dice que mires a tu alrededor y des un paso en la dirección más inclinada hacia abajo. Repites esto hasta llegar al fondo.\nEl algoritmo funciona así:\n\nInicializa los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) con valores aleatorios (o en ceros).\nCalcula el gradiente de la función de costo. El gradiente es un vector que apunta en la dirección del máximo ascenso. Nosotros iremos en la dirección opuesta.\n\n\\(\\frac{\\partial J}{\\partial \\beta_0} = -2 \\sum (y_i - (\\beta_0 + \\beta_1 x_i))\\)\n\\(\\frac{\\partial J}{\\partial \\beta_1} = -2 \\sum x_i(y_i - (\\beta_0 + \\beta_1 x_i))\\)\n\nActualiza los coeficientes usando una tasa de aprendizaje (\\(\\alpha\\)), que controla el tamaño del paso que damos.\n\n\\(\\beta_0 := \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0}\\)\n\\(\\beta_1 := \\beta_1 - \\alpha \\frac{\\partial J}{\\partial \\beta_1}\\)\n\nRepite los pasos 2 y 3 durante un número determinado de iteraciones o hasta que el cambio en el costo sea muy pequeño (convergencia).\n\n\n\n\n\n\n\nExplicacion visual",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#cuáles-son-los-supuestos-de-la-regresión",
    "href": "03-regresion_lineal.html#cuáles-son-los-supuestos-de-la-regresión",
    "title": "Regresión lineal",
    "section": "¿Cuáles son los supuestos de la regresión? 🧐",
    "text": "¿Cuáles son los supuestos de la regresión? 🧐\nPara que nuestro modelo sea confiable (es decir, para que los coeficientes y las predicciones tengan sentido), debemos cumplir con ciertos supuestos.\n\nLinealidad: La relación entre \\(\\beta\\) y \\(Y\\) debe ser lineal.\n\n¿Para qué sirve? Si la relación no es lineal, nuestro modelo de línea recta será intrínsecamente incorrecto.\n\nIndependencia de los errores: Los errores (residuos) no deben estar correlacionados entre sí.\n\n¿Para qué sirve? Es crucial para datos de series temporales. Si los errores están correlacionados, la información de un error nos da pistas sobre el siguiente, lo cual viola la idea de que cada observación es independiente.\n\nHomocedasticidad (Varianza constante de los errores): La varianza de los errores debe ser constante para todos los niveles de \\(X\\).\n\n¿Para qué sirve? Si la varianza cambia (heterocedasticidad), nuestras predicciones serán mejores para algunas partes de los datos que para otras, y los intervalos de confianza para los coeficientes serán poco fiables. Visualmente, en un gráfico de residuos vs. valores predichos, no queremos ver una forma de cono o embudo.\n\nNormalidad de los errores: Los errores deben seguir una distribución normal con media cero.\n\n¿Para qué sirve? Este supuesto es fundamental para poder realizar pruebas de hipótesis sobre los coeficientes (como los p-values) y construir intervalos de confianza. Podemos verificarlo con un histograma de los residuos o un gráfico Q-Q.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#cómo-evaluar-la-precisión-del-modelo",
    "href": "03-regresion_lineal.html#cómo-evaluar-la-precisión-del-modelo",
    "title": "Regresión lineal",
    "section": "¿Cómo evaluar la precisión del modelo? 📈",
    "text": "¿Cómo evaluar la precisión del modelo? 📈\nUna vez que hemos ajustado el modelo, ¿cómo sabemos si es bueno?\n\nCoeficiente de Determinación (\\(R^2\\))\nEl \\(R^2\\) mide la proporción de la varianza total en la variable dependiente (\\(Y\\)) que es explicada por nuestro modelo.\n\\[R^2 = 1 - \\frac{\\text{Suma de Errores al Cuadrado (SEC)}}{\\text{Suma Total de Cuadrados (STC)}} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\\]\n\n\\(R^2\\) varía entre 0 y 1 (o 0% y 100%).\nUn \\(R^2\\) de 0.85 significa que el 85% de la variabilidad en \\(Y\\) puede ser explicada por \\(X\\).\nUn \\(R^2\\) más alto generalmente indica un mejor ajuste del modelo.\n\n\n\np-values (Valores p)\nEl p-value nos ayuda a determinar si nuestra variable predictora \\(X\\) es estadísticamente significativa. Responde a la pregunta: ¿Es probable que la relación que observamos entre \\(X\\) y \\(Y\\) haya ocurrido por puro azar?\n\nHipótesis Nula (\\(H_0\\)): No hay relación entre \\(X\\) y \\(Y\\) (es decir, \\(\\beta_1 = 0\\)).\nHipótesis Alternativa (\\(H_a\\)): Sí hay una relación entre \\(X\\) y \\(Y\\) (es decir, \\(\\beta_1 \\neq 0\\)).\n\nUn p-value pequeño (típicamente &lt; 0.05) nos da evidencia para rechazar la hipótesis nula. Esto sugiere que nuestra variable \\(X\\) es un predictor útil para \\(Y\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#métricas-de-error-de-predicción",
    "href": "03-regresion_lineal.html#métricas-de-error-de-predicción",
    "title": "Regresión lineal",
    "section": "Métricas de Error de Predicción",
    "text": "Métricas de Error de Predicción\nAdemás del \\(R^2\\), existen múltiples métricas para evaluar qué tan bien predice nuestro modelo. Cada una tiene sus ventajas y casos de uso específicos:\n\nError Cuadrático Medio (MSE)\nEl MSE mide el promedio de los errores al cuadrado:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\n\nVentajas: Penaliza fuertemente errores grandes, diferenciable (útil para optimización)\nDesventajas: Sensible a valores atípicos, difícil de interpretar (unidades al cuadrado)\nCuándo usar: Cuando errores grandes son especialmente costosos\n\n\n\nRaíz del Error Cuadrático Medio (RMSE)\nEl RMSE es la raíz cuadrada del MSE:\n\\[RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]\n\nVentajas: Mismas unidades que la variable objetivo, interpretable\nDesventajas: Aún sensible a valores atípicos\nInterpretación: “En promedio, nuestras predicciones se desvían X unidades del valor real”\n\n\n\nError Absoluto Medio (MAE)\nEl MAE mide el promedio de los errores absolutos:\n\\[MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\]\n\nVentajas: Robusto a valores atípicos, fácil de interpretar\nDesventajas: No diferenciable en cero, trata todos los errores por igual\nCuándo usar: Cuando hay valores atípicos o todos los errores tienen igual importancia\n\n\n\nError Porcentual Absoluto Medio (MAPE)\nEl MAPE expresa el error como porcentaje del valor real:\n\\[MAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\\]\n\nVentajas: Interpretable (% de error), adimensional, útil para comparar modelos en diferentes escalas\nDesventajas: Indefinido cuando \\(y_i = 0\\), asimétrico (penaliza más las sobreestimaciones)\nInterpretación: “Nuestras predicciones se desvían en promedio X% del valor real”\nCuándo usar: Para comparar precisión entre diferentes productos, regiones, o escalas\n\n\n\nError Porcentual Absoluto Medio Simétrico (SMAPE)\nEl SMAPE es una versión simétrica del MAPE:\n\\[SMAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}\\]\n\nVentajas: Simétrico, acotado entre 0% y 200%\nDesventajas: Puede ser contraintuitivo, no tan estándar como MAPE\nCuándo usar: Cuando queremos evitar el sesgo del MAPE hacia sobreestimaciones\n\n\n\nError Logarítmico Cuadrático Medio (MSLE)\nEl MSLE usa transformación logarítmica:\n\\[MSLE = \\frac{1}{n} \\sum_{i=1}^{n} (\\log(1 + y_i) - \\log(1 + \\hat{y}_i))^2\\]\n\nVentajas: Penaliza más las subestimaciones que las sobreestimaciones\nDesventajas: Solo para valores positivos, menos interpretable\nCuándo usar: Cuando subestimar es más costoso que sobreestimar (ej: demanda de inventario)\n\n\n\n\\(R^2\\) Ajustado\nEl \\(R^2\\) ajustado penaliza por el número de variables en el modelo:\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\nDonde \\(p\\) es el número de predictores.\n\nVentajas: No aumenta automáticamente al añadir variables\nCuándo usar: Para comparar modelos con diferente número de variables\nInterpretación: Similar a \\(R^2\\) pero más conservador\n\n\n¿Cuál métrica elegir?\nLa elección de métrica depende del contexto del problema:\n\n\n\n\n\n\n\n\nMétrica\nMejor para\nEvitar cuando\n\n\n\n\nRMSE\nErrores grandes son costosos\nHay muchos valores atípicos\n\n\nMAE\nErrores tienen igual importancia\nNecesitas diferenciabilidad\n\n\nMAPE\nComparar diferentes escalas\nHay valores cercanos a cero\n\n\nSMAPE\nComparar con simetría\nInterpretación debe ser simple\n\n\nR²\nExplicar variabilidad\nSolo importa precisión de predicción\n\n\n\n\n\n\n\n\n\nRecomendación práctica\n\n\n\nUsa múltiples métricas para evaluar tu modelo. Una combinación típica sería: - RMSE para precisión general - MAPE para interpretabilidad de negocio\n- R² para explicación de variabilidad",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresión-lineal-múltiple",
    "href": "03-regresion_lineal.html#regresión-lineal-múltiple",
    "title": "Regresión lineal",
    "section": "Regresión Lineal Múltiple",
    "text": "Regresión Lineal Múltiple\nAhora, ¿qué pasa si tenemos múltiples predictores (\\(X_1, X_2, ..., X_p\\))? El modelo se expande:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\\]\nLa intuición es la misma, pero en lugar de ajustar una línea, estamos ajustando un hiperplano en un espacio multidimensional.\nPara manejar esto de forma elegante, usamos notación matricial:\n\\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\nDonde: - \\(\\mathbf{y}\\) es el vector de observaciones. - \\(\\mathbf{X}\\) es la matriz de diseño (con una primera columna de unos para el intercepto). - \\(\\boldsymbol{\\beta}\\) es el vector de coeficientes. - \\(\\boldsymbol{\\epsilon}\\) es el vector de errores.\nLa función de costo en forma matricial es: \\[J(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#transformaciones-comunes-en-modelos-lineales",
    "href": "03-regresion_lineal.html#transformaciones-comunes-en-modelos-lineales",
    "title": "Regresión lineal",
    "section": "Transformaciones Comunes en Modelos Lineales",
    "text": "Transformaciones Comunes en Modelos Lineales\nA veces, la relación entre X e Y no es estrictamente lineal. Las transformaciones logarítmicas nos permiten modelar relaciones no lineales y, además, ofrecen interpretaciones muy útiles en términos de cambios porcentuales.\n\nModelo Log-Nivel (Transformación en Y)\nEste modelo se usa cuando el efecto de X sobre Y no es absoluto, sino porcentual. Por ejemplo, cómo un año más de educación afecta el porcentaje de aumento salarial.\n\nEcuación: \\(\\ln(Y) = \\beta_0 + \\beta_1 X + \\epsilon\\)\nInterpretación: Un incremento de una unidad en \\(X\\) está asociado con un cambio de \\((100 \\cdot \\beta_1)\\%\\) en \\(Y\\).\n\n\n\n\n\n\n\nExplicación Matemática de la Aproximación\n\n\n\n\n\nLa clave está en la propiedad del logaritmo y el cálculo. La derivada de \\(\\ln(Y)\\) con respecto a \\(X\\) es \\(\\beta_1\\): \\[\\frac{d(\\ln(Y))}{dX} = \\beta_1\\] Sabemos que \\(d(\\ln(Y)) = \\frac{dY}{Y}\\). Por tanto: \\[\\frac{dY/Y}{dX} = \\beta_1\\] Para cambios pequeños (o discretos, \\(\\Delta\\)), podemos aproximar los diferenciales: \\[\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X}\\] Si consideramos un cambio unitario en X, \\(\\Delta X = 1\\), entonces: \\[\\beta_1 \\approx \\frac{\\Delta Y}{Y}\\] Esto significa que \\(\\beta_1\\) es la aproximación del cambio porcentual en \\(Y\\) ante un cambio de una unidad en \\(X\\).\n\n\n\n\n\nModelo Nivel-Log (Transformación en X)\nEste modelo es útil cuando el efecto de X sobre Y se reduce a medida que X aumenta (rendimientos decrecientes). Por ejemplo, el efecto de añadir presupuesto de marketing sobre las ventas.\n\nEcuación: \\(Y = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\)\nInterpretación: Un incremento del 1% en \\(X\\) está asociado con un cambio de \\((\\beta_1 / 100)\\) unidades en \\(Y\\).\n\n\n\n\n\n\n\nExplicación Matemática de la Aproximación\n\n\n\n\n\nTomamos la derivada de \\(Y\\) con respecto a \\(\\ln(X)\\): \\[\\frac{dY}{d(\\ln(X))} = \\beta_1\\] Usando la regla de la cadena, sabemos que \\(d(\\ln(X)) = \\frac{dX}{X}\\). Sustituyendo: \\[\\frac{dY}{dX/X} = \\beta_1 \\implies dY = \\beta_1 \\frac{dX}{X}\\] Para cambios discretos, aproximamos: \\[\\Delta Y \\approx \\beta_1 \\frac{\\Delta X}{X}\\] Si consideramos un cambio del 1% en X, entonces \\(\\frac{\\Delta X}{X} = 0.01\\). La ecuación se convierte en: \\[\\Delta Y \\approx \\beta_1 (0.01) = \\frac{\\beta_1}{100}\\] Esto significa que un cambio del 1% en \\(X\\) provoca un cambio de \\(\\beta_1/100\\) unidades en \\(Y\\).\n\n\n\n\n\nModelo Log-Log (Transformación en X e Y)\nEste modelo es muy común en economía y modela la elasticidad constante entre dos variables.\n\nEcuación: \\(\\ln(Y) = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\)\nInterpretación: Un incremento del 1% en \\(X\\) está asociado con un cambio del \\(\\beta_1\\%\\) en \\(Y\\).\n\n\n\n\n\n\n\nExplicación Matemática de la Aproximación\n\n\n\n\n\nEste caso combina los dos anteriores. \\(\\beta_1\\) es la derivada de \\(\\ln(Y)\\) con respecto a \\(\\ln(X)\\), que es la definición de elasticidad. \\[\\beta_1 = \\frac{d(\\ln(Y))}{d(\\ln(X))}\\] Usando las propiedades del cálculo que vimos antes: \\[\\beta_1 = \\frac{dY/Y}{dX/X}\\] Aproximando para cambios discretos: \\[\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X / X}\\] Esta es la definición de elasticidad: el cambio porcentual en \\(Y\\) dividido por el cambio porcentual en \\(X\\). Por lo tanto, si \\(X\\) cambia en un 1% (\\(\\Delta X / X = 0.01\\)), el cambio porcentual en \\(Y\\) (\\(\\Delta Y / Y\\)) será aproximadamente \\(\\beta_1 \\times 0.01\\), es decir, un \\(\\beta_1\\%\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresión-regularizada-penalizada",
    "href": "03-regresion_lineal.html#regresión-regularizada-penalizada",
    "title": "Regresión lineal",
    "section": "Regresión Regularizada (Penalizada) 🎯",
    "text": "Regresión Regularizada (Penalizada) 🎯\nHasta ahora hemos visto la regresión lineal clásica, pero ¿qué pasa cuando tenemos muchas variables o cuando nuestro modelo sufre de sobreajuste? Aquí es donde entran las técnicas de regularización.\n\n¿Por qué necesitamos regularización?\nLa regresión lineal ordinaria (OLS) puede presentar varios problemas:\n\nSobreajuste: Cuando tenemos muchas variables relativas al número de observaciones\nMulticolinealidad: Variables predictoras altamente correlacionadas\nInestabilidad: Pequeños cambios en los datos causan grandes cambios en los coeficientes\nInterpretabilidad: Demasiadas variables hacen difícil entender el modelo\n\nLa regularización añade una penalización a la función de costo para controlar la complejidad del modelo.\n\n\n\nRidge Regression (Regresión Ridge) 🏔️\nLa regresión Ridge añade una penalización L2 (suma de cuadrados) a los coeficientes:\n\\[J_{Ridge}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\\]\nDonde: - \\(\\lambda &gt; 0\\) es el parámetro de regularización - \\(\\sum_{j=1}^{p} \\beta_j^2\\) es la penalización L2\n\nCaracterísticas de Ridge:\n✅ Ventajas: - Reduce el sobreajuste - Maneja bien la multicolinealidad - Siempre tiene solución única - Estabiliza los coeficientes\n❌ Desventajas: - NO elimina variables (coeficientes nunca son exactamente cero) - Dificulta la interpretabilidad - Requiere estandarizar las variables\n\n\nSolución Analítica:\n\\[\\hat{\\boldsymbol{\\beta}}_{Ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nEl término \\(\\lambda\\mathbf{I}\\) hace que la matriz sea invertible incluso con multicolinealidad.\n\n\n¿Cómo elegir λ?\n\nλ = 0: Regresión ordinaria (sin penalización)\nλ → ∞: Todos los coeficientes → 0\nλ óptimo: Se encuentra usando validación cruzada\n\n\n\n\n\nLasso Regression (Least Absolute Shrinkage and Selection Operator) ✂️\nLa regresión Lasso usa penalización L1 (suma de valores absolutos):\n\\[J_{Lasso}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\\]\n\nCaracterísticas de Lasso:\n✅ Ventajas: - Selección automática de variables (coeficientes = 0) - Modelos más interpretables y simples - Útil cuando muchas variables son irrelevantes\n❌ Desventajas: - Puede ser inestable con grupos de variables correlacionadas - Selecciona arbitrariamente entre variables correlacionadas - No tiene solución analítica cerrada\n\n\nLa “Magia” de L1: ¿Por qué produce ceros exactos?\nLa penalización L1 crea una región factible con esquinas puntiagudas. La solución óptima tiende a ocurrir en estas esquinas, donde algunos coeficientes son exactamente cero.\n\n\n\n\n\n\nIntuición Geométrica\n\n\n\n\n\nImagina que estás minimizando una función bajo la restricción de que \\(|\\beta_1| + |\\beta_2| \\leq t\\). Esta restricción forma un diamante en 2D. La función objetivo forma elipses. La solución está donde la elipse más pequeña toca el diamante, y esto frecuentemente ocurre en los vértices (donde \\(\\beta_1 = 0\\) o \\(\\beta_2 = 0\\)).\n\n\n\n\n\n\n\nElastic Net: Lo Mejor de Ambos Mundos 🕸️\nElastic Net combina las penalizaciones L1 y L2:\n\\[J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\\]\nO equivalentemente, con un parámetro de mezcla \\(\\alpha\\):\n\\[J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\left[ \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right]\\]\nDonde: - \\(\\alpha \\in [0,1]\\) controla la mezcla entre L1 y L2 - \\(\\alpha = 0\\): Pure Ridge - \\(\\alpha = 1\\): Pure Lasso - \\(\\alpha = 0.5\\): Igual peso a ambas penalizaciones\n\nCaracterísticas de Elastic Net:\n✅ Ventajas: - Selección de variables como Lasso - Estabilidad como Ridge - Maneja bien grupos de variables correlacionadas - Más flexible que Ridge o Lasso por separado\n❌ Desventajas: - Dos hiperparámetros para ajustar (\\(\\lambda\\) y \\(\\alpha\\)) - Más complejo computacionalmente\n\n\n\n\nComparación Visual: Ridge vs Lasso vs Elastic Net\n\n\n\n\n\n\n\n\n\nAspecto\nRidge\nLasso\nElastic Net\n\n\n\n\nPenalización\nL2: \\(\\sum \\beta_j^2\\)\nL1: \\(\\sum |\\beta_j|\\)\nL1 + L2 combinadas\n\n\nSelección de variables\n❌ No\n✅ Sí\n✅ Sí\n\n\nCoeficientes exactamente cero\n❌ No\n✅ Sí\n✅ Sí\n\n\nManejo de multicolinealidad\n✅ Excelente\n⚠️ Problemático\n✅ Muy bueno\n\n\nEstabilidad\n✅ Alta\n⚠️ Media\n✅ Alta\n\n\nInterpretabilidad\n⚠️ Media\n✅ Alta\n✅ Alta\n\n\nCuando usar\nTodas las variables importan\nPocas variables importantes\nSituaciones mixtas\n\n\n\n\n\n¿Cuándo usar cada método?\n\nUsa Ridge cuando:\n\nCrees que todas las variables contribuyen al modelo\nTienes multicolinealidad severa\nQuieres estabilizar coeficientes sin eliminar variables\nEl número de observaciones es pequeño relativo a variables\n\n\n\nUsa Lasso cuando:\n\nCrees que pocas variables son realmente importantes\nQuieres un modelo simple e interpretable\nNecesitas selección automática de variables\nTienes muchas variables irrelevantes\n\n\n\nUsa Elastic Net cuando:\n\nNo estás seguro de cuántas variables son importantes\nTienes grupos de variables correlacionadas\nQuieres balancear selección y estabilidad\nEs tu primera opción cuando no conoces la estructura de los datos\n\n\n\n\n\nValidación de Modelos y Selección de Hiperparámetros\n\n¿Por qué necesitamos dividir nuestros datos?\nCuando construimos modelos de machine learning, enfrentamos un dilema fundamental: ¿cómo sabemos si nuestro modelo funcionará bien con datos nuevos?\n\nEl Problema del Sobreajuste\nImagina que estás preparándote para un examen. Si solo estudias las preguntas exactas que aparecerán en el examen, podrías obtener una calificación perfecta. Pero si las preguntas cambian ligeramente, tu rendimiento se desplomaría. Esto es sobreajuste: el modelo memoriza los datos de entrenamiento pero no generaliza.\n\n\n\nDivisión Típica de Datos: Entrenamiento/Validación/Prueba\nLa estrategia estándar es dividir nuestros datos en tres conjuntos:\n📊 Dataset Completo (100%)\n├── 🏋️ Entrenamiento (60%) - Para ajustar coeficientes\n├── 🎯 Validación (20%)     - Para seleccionar hiperparámetros  \n└── 🧪 Prueba (20%)         - Para evaluación final\n\nConjunto de Entrenamiento (60%)\n\nPropósito: Ajustar los coeficientes \\(\\beta\\) del modelo\nAnalogía: Los ejercicios que haces para aprender\n\n\n\nConjunto de Validación (20%)\n\nPropósito: Comparar diferentes hiperparámetros (como \\(\\lambda\\) en Ridge/Lasso)\nAnalogía: Exámenes de práctica para decidir qué estrategia de estudio funciona mejor\n\n\n\nConjunto de Prueba (20%)\n\nPropósito: Evaluación final y honesta del modelo\nAnalogía: El examen final real\n⚠️ Regla de Oro: ¡Solo se usa UNA vez al final!\n\n\n\n\n¿Qué pasa si tenemos pocos datos?\nCuando nuestro dataset es pequeño (&lt; 1000 observaciones), dividir en tres partes puede ser problemático:\n❌ Problemas con datasets pequeños: - Conjunto de entrenamiento muy pequeño → modelo pobre - Conjunto de validación pequeño → selección inestable de hiperparámetros - Conjunto de prueba pequeño → evaluación poco confiable\nSolución: ¡Validación Cruzada!\n\n\n\nValidación Cruzada (Cross-Validation)\nLa validación cruzada es una técnica que maximiza el uso de nuestros datos limitados. En lugar de usar una sola división, usamos múltiples divisiones.\n\nValidación Cruzada k-fold\nEl método más común es k-fold cross-validation:\n\nDividir el dataset en \\(k\\) “pliegues” (folds) de igual tamaño\nRepetir \\(k\\) veces:\n\nUsar \\(k-1\\) pliegues para entrenamiento\nUsar 1 pliegue para validación\n\nPromediar los resultados de las \\(k\\) evaluaciones\n\n\n\n\n\n\nVisualización de 5-Fold Cross Validation mostrando cómo se dividen los datos en cada iteración\n\n\n\n\n\n\nVentajas de la Validación Cruzada\n✅ Maximiza el uso de datos: Cada observación se usa tanto para entrenamiento como validación\n✅ Estimación más robusta: Promedia múltiples evaluaciones independientes\n✅ Reduce la varianza: Menos dependiente de una división particular\n✅ Detecta inestabilidad: Si los resultados varían mucho entre folds, el modelo es inestable\n\n\n\nValidación Cruzada para Selección de Hiperparámetros\nEn regresión regularizada, usamos CV para encontrar el mejor \\(\\lambda\\):\n\n\n🎯 SELECCIÓN DE HIPERPARÁMETROS CON VALIDACIÓN CRUZADA\n============================================================\nPara cada valor de λ:\n  1. Aplicar 5-fold CV\n  2. Calcular error promedio\n  3. Seleccionar λ con menor error\n\n\n\n\n\nCurva de validación mostrando cómo seleccionar el hiperparámetro óptimo λ usando validación cruzada\n\n\n\n\n\n📈 Resultado: λ óptimo = 0.1274\n📉 Error de CV mínimo = 0.4776\n\n\n\n\nProceso Completo de Validación\nEl flujo completo para modelos regularizados es:\n1. 📊 Dividir datos originales\n   └── 80% para desarrollo (entrenamiento + validación)\n   └── 20% para prueba final (¡NO TOCAR hasta el final!)\n\n2. 🔄 En el conjunto de desarrollo:\n   └── Para cada λ candidato:\n       ├── Aplicar k-fold CV\n       ├── Calcular error promedio\n       └── Guardar resultado\n\n3. 🎯 Seleccionar λ con menor error de CV\n\n4. 🏗️ Entrenar modelo final con λ óptimo en TODO el conjunto de desarrollo\n\n5. 🧪 Evaluación final en conjunto de prueba\n\n\nVariantes de Validación Cruzada\n\nLeave-One-Out CV (LOOCV)\n\nk = n (número de observaciones)\nVentaja: Máximo uso de datos para entrenamiento\nDesventaja: Computacionalmente costoso, alta varianza\n\n\n\nStratified CV\n\nPara problemas de clasificación\nMantiene la proporción de clases en cada fold\n\n\n\nTime Series CV\n\nPara datos temporales\nRespeta el orden temporal (no mezcla futuro con pasado)\n\n\n\n\n\n\n\n⚠️ Errores Comunes\n\n\n\n\nData Leakage: Usar información del conjunto de prueba durante el desarrollo\nMúltiples evaluaciones: Evaluar repetidamente en el conjunto de prueba\nSelección de modelo sesgada: Elegir el modelo basándose en el conjunto de prueba\nCV incorrecto: Aplicar transformaciones antes de la división de CV\n\n\n\n\n\n\n¿Cuándo usar cada enfoque?\n\n\n\n\n\n\n\n\nTamaño del Dataset\nEnfoque Recomendado\nRazón\n\n\n\n\nGrande (&gt;10,000)\nTrain/Validation/Test\nSuficientes datos para división estable\n\n\nMediano (1,000-10,000)\nTrain/Test + CV\nCV para hiperparámetros, test para evaluación final\n\n\nPequeño (&lt;1,000)\nSolo CV (sin test separado)\nMaximizar datos disponibles\n\n\nMuy pequeño (&lt;100)\nLOOCV o Bootstrap\nCada observación es valiosa\n\n\n\n\n\n\n\n\n\nConsejo Práctico\n\n\n\nEmpieza siempre con Elastic Net con \\(\\alpha = 0.5\\). Si el modelo selecciona muchas variables, prueba valores de \\(\\alpha\\) más cercanos a 1 (más Lasso). Si elimina variables importantes, prueba valores cercanos a 0 (más Ridge).\n\n\n\n\n\n\nEjercicio Práctico: Comparando los Tres Métodos\nEn el notebook correspondiente, implementaremos:\n\nGeneración de datos con diferentes estructuras de correlación\nComparación visual de los caminos de regularización\nValidación cruzada para selección de hiperparámetros\nEvaluación del rendimiento en datos de prueba\nInterpretación de los coeficientes seleccionados\n\nPregunta de reflexión: ¿En qué situaciones esperarías que Ridge supere a Lasso, y viceversa?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and\nJonathan Taylor. 2023. An Introduction to\nStatistical Learning: With Applications in Python. Springer\nTexts in Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Referencias"
    ]
  },
  {
    "objectID": "introduccion.html",
    "href": "introduccion.html",
    "title": "Introducción a Python para Minería de Datos",
    "section": "",
    "text": "1. Librerías Necesarias\nEste notebook cubre los fundamentos de Python necesarios para minería de datos, incluyendo:\nPrimero importamos todas las librerías que usaremos en este notebook:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#fundamentos-de-numpy",
    "href": "introduccion.html#fundamentos-de-numpy",
    "title": "Introducción a Python para Minería de Datos",
    "section": "2. Fundamentos de NumPy",
    "text": "2. Fundamentos de NumPy\nNumPy es la librería fundamental para computación científica en Python. Proporciona arrays multidimensionales eficientes.\n\n2.1 Creación de Arrays\n\n# Creamos un array de 100 números igualmente espaciados entre 0 y 10\nX = np.linspace(0, 10, 100)\nprint(\"Primeros 5 elementos:\", X[:5])\nprint(\"Forma del array:\", X.shape)\nprint(\"Últimos 10 elementos:\", X[90:])\n\nPrimeros 5 elementos: [0.        0.1010101 0.2020202 0.3030303 0.4040404]\nForma del array: (100,)\nÚltimos 10 elementos: [ 9.09090909  9.19191919  9.29292929  9.39393939  9.49494949  9.5959596\n  9.6969697   9.7979798   9.8989899  10.        ]\n\n\n\n\n2.2 Operaciones con Arrays y Generación de Datos\n\n# Acceso a elementos individuales\nprint(\"Elemento en posición 1:\", X[1])\n\n# Generación de datos sintéticos para regresión lineal\n# y = 0.5*x + 10 + ruido_gaussiano\ny_linear = 0.5 * X + 10 + np.random.normal(loc=0, scale=1.5, size=100)\nprint(\"Primeros 5 valores de y:\", y_linear[:5])\n\nElemento en posición 1: 0.10101010101010101\nPrimeros 5 valores de y: [10.947363    7.72186528 10.63466357 10.19993526 11.22329185]",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#fundamentos-de-pandas",
    "href": "introduccion.html#fundamentos-de-pandas",
    "title": "Introducción a Python para Minería de Datos",
    "section": "3. Fundamentos de Pandas",
    "text": "3. Fundamentos de Pandas\nPandas es la librería principal para manipulación y análisis de datos estructurados en Python.\n\n3.1 Creación de DataFrames\n\n# Creamos un DataFrame con nuestros datos\ndf = pd.DataFrame({\"x\": X, \"y\": y_linear})\nprint(\"Primeras 5 filas del DataFrame:\")\nprint(df.head())\nprint(\"\\nInformación del DataFrame:\")\nprint(df.info())\n\nPrimeras 5 filas del DataFrame:\n         x          y\n0  0.00000  10.947363\n1  0.10101   7.721865\n2  0.20202  10.634664\n3  0.30303  10.199935\n4  0.40404  11.223292\n\nInformación del DataFrame:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   x       100 non-null    float64\n 1   y       100 non-null    float64\ndtypes: float64(2)\nmemory usage: 1.7 KB\nNone\n\n\n\n\n3.2 Manipulación de DataFrames\n\n# Filtrado de datos\nprint(\"Filas donde y &gt; 12:\")\ndf_filtrado = df.query(\"y &gt; 12\")\nprint(df_filtrado)\n\n# Estadísticas descriptivas\nprint(\"\\nCuantiles de la variable y:\")\nprint(df.y.quantile(q=[0.25, 0.5, 0.75, 0.99]))\n\nFilas donde y &gt; 12:\n            x          y\n10   1.010101  12.755558\n17   1.717172  16.315991\n28   2.828283  12.890289\n31   3.131313  12.095512\n33   3.333333  12.186190\n35   3.535354  16.519562\n38   3.838384  14.936178\n39   3.939394  12.445576\n43   4.343434  13.407873\n44   4.444444  14.740559\n46   4.646465  14.085751\n47   4.747475  13.406937\n48   4.848485  12.711032\n49   4.949495  14.302195\n51   5.151515  13.419018\n53   5.353535  14.202517\n54   5.454545  13.460248\n55   5.555556  13.570844\n56   5.656566  15.336198\n58   5.858586  13.248156\n59   5.959596  12.120376\n60   6.060606  13.536254\n61   6.161616  13.877174\n62   6.262626  12.643431\n64   6.464646  13.144207\n65   6.565657  16.558753\n66   6.666667  14.174915\n67   6.767677  13.188301\n68   6.868687  15.327148\n69   6.969697  14.229089\n70   7.070707  14.671629\n71   7.171717  15.430842\n72   7.272727  14.078094\n74   7.474747  14.913045\n75   7.575758  16.341642\n76   7.676768  13.882098\n77   7.777778  14.801524\n78   7.878788  16.116168\n79   7.979798  14.168691\n80   8.080808  12.395476\n81   8.181818  12.295980\n82   8.282828  15.132112\n83   8.383838  12.813598\n84   8.484848  14.631908\n86   8.686869  14.303784\n87   8.787879  13.279560\n88   8.888889  12.344803\n89   8.989899  12.711722\n91   9.191919  16.917684\n92   9.292929  14.830744\n93   9.393939  15.292800\n95   9.595960  12.746624\n96   9.696970  16.000313\n97   9.797980  15.209167\n98   9.898990  14.825473\n99  10.000000  13.841673\n\nCuantiles de la variable y:\n0.25    11.154739\n0.50    12.420526\n0.75    14.181815\n0.99    16.562342\nName: y, dtype: float64\n\n\n\n# Renombramiento de columnas\nprint(\"Renombrando columnas...\")\ndf_renamed = df.rename(columns={\"x\": \"variable_independiente\", \"y\": \"variable_dependiente\"})\nprint(df_renamed.head())\n\n# Volvemos a los nombres originales para el resto del notebook\ndf = df.rename(columns={\"x\": \"x\", \"y\": \"y\"})\n\nRenombrando columnas...\n   variable_independiente  variable_dependiente\n0                 0.00000             10.947363\n1                 0.10101              7.721865\n2                 0.20202             10.634664\n3                 0.30303             10.199935\n4                 0.40404             11.223292",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#iteración-en-python",
    "href": "introduccion.html#iteración-en-python",
    "title": "Introducción a Python para Minería de Datos",
    "section": "4. Iteración en Python",
    "text": "4. Iteración en Python\n\n4.1 Iteración sobre Diccionarios\n\n# Ejemplo de diccionario con diferentes tipos de datos\ndiccionario_ejemplo = {\n    \"entero\": 42,\n    \"array_numpy\": np.array([1, 2, 3, 4]),\n    \"cadena\": \"minería de datos\",\n    \"flotante\": 3.14159\n}\n\n# Iteración sobre diccionarios\nprint(\"Iterando sobre el diccionario:\")\nfor clave, valor in diccionario_ejemplo.items():\n    print(f\"Clave: {clave} | Valor: {valor} | Tipo: {type(valor).__name__}\")\n\nIterando sobre el diccionario:\nClave: entero | Valor: 42 | Tipo: int\nClave: array_numpy | Valor: [1 2 3 4] | Tipo: ndarray\nClave: cadena | Valor: minería de datos | Tipo: str\nClave: flotante | Valor: 3.14159 | Tipo: float\n\n\n\n\n4.2 Iteración sobre Listas\n\n# Ejemplo con lista de números\nnumeros = [1, 4, 9, 16, 25]\nprint(\"Iterando sobre lista de números:\")\nfor i, numero in enumerate(numeros):\n    print(f\"Posición {i}: {numero}, raíz cuadrada: {np.sqrt(numero):.2f}\")\n    \n# Ejemplo con diferentes valores de k para KNN\nvalores_k = [1, 5, 10, 20, 50]\nprint(\"\\nIterando sobre valores de k:\")\nfor k in valores_k:\n    print(f\"Valor de k: {k}\")\n\nIterando sobre lista de números:\nPosición 0: 1, raíz cuadrada: 1.00\nPosición 1: 4, raíz cuadrada: 2.00\nPosición 2: 9, raíz cuadrada: 3.00\nPosición 3: 16, raíz cuadrada: 4.00\nPosición 4: 25, raíz cuadrada: 5.00\n\nIterando sobre valores de k:\nValor de k: 1\nValor de k: 5\nValor de k: 10\nValor de k: 20\nValor de k: 50",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#visualización-con-matplotlib-y-seaborn",
    "href": "introduccion.html#visualización-con-matplotlib-y-seaborn",
    "title": "Introducción a Python para Minería de Datos",
    "section": "5. Visualización con Matplotlib y Seaborn",
    "text": "5. Visualización con Matplotlib y Seaborn\n\n5.1 Gráfica de Dispersión Básica\n\n# Gráfica de dispersión básica\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x=\"x\", y=\"y\")\nplt.title(\"Relación entre x y y\")\nplt.xlabel(\"Variable independiente (x)\")\nplt.ylabel(\"Variable dependiente (y)\")\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#funciones-con-type-hints",
    "href": "introduccion.html#funciones-con-type-hints",
    "title": "Introducción a Python para Minería de Datos",
    "section": "6. Funciones con Type Hints",
    "text": "6. Funciones con Type Hints\n\n6.1 Función para Generar Datos\n\ndef generar_datos_lineales(\n    n: int = 100,\n    slope: float = 0.5,\n    intercept: float = 1.0,\n    noise_scale: float = 1.5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Genera datos sintéticos para regresión lineal.\n    \n    Parámetros:\n    -----------\n    n : int\n        Número de puntos de datos a generar\n    slope : float\n        Pendiente de la relación lineal\n    intercept : float\n        Intercepto de la relación lineal\n    noise_scale : float\n        Desviación estándar del ruido gaussiano\n    \n    Retorna:\n    --------\n    pd.DataFrame\n        DataFrame con columnas 'x' y 'y'\n    \"\"\"\n    X = np.linspace(0, 10, n)\n    y = slope * X + intercept + np.random.normal(scale=noise_scale, size=n)\n    return pd.DataFrame({\"x\": X, \"y\": y})\n\n# Ejemplo de uso\ndf_nuevo = generar_datos_lineales(n=50, slope=2.0, intercept=5.0)\nprint(\"Datos generados:\")\nprint(df_nuevo.head())\n\nDatos generados:\n          x         y\n0  0.000000  6.962633\n1  0.204082  5.261446\n2  0.408163  6.349024\n3  0.612245  6.767730\n4  0.816327  5.768821\n\n\n\n\n6.2 Función para Visualización\n\ndef crear_grafica_dispersion(\n    data_frame: pd.DataFrame,\n    x_col: str = \"x\",\n    y_col: str = \"y\",\n    titulo: str = \"Gráfica de Dispersión\",\n    nombre_eje_x: str = \"X\",\n    nombre_eje_y: str = \"Y\"\n) -&gt; None:\n    \"\"\"\n    Crea una gráfica de dispersión con formato personalizado.\n    \n    Parámetros:\n    -----------\n    data_frame : pd.DataFrame\n        DataFrame que contiene los datos\n    x_col : str\n        Nombre de la columna para el eje x\n    y_col : str\n        Nombre de la columna para el eje y\n    titulo : str\n        Título de la gráfica\n    nombre_eje_x : str\n        Etiqueta del eje x\n    nombre_eje_y : str\n        Etiqueta del eje y\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.scatterplot(data=data_frame, x=x_col, y=y_col)\n    plt.title(titulo)\n    plt.xlabel(nombre_eje_x)\n    plt.ylabel(nombre_eje_y)\n    plt.show()\n\n# Ejemplo de uso\ncrear_grafica_dispersion(\n    df_nuevo, \n    titulo=\"Datos Sintéticos Generados\",\n    nombre_eje_x=\"Variable Independiente\",\n    nombre_eje_y=\"Variable Dependiente\"\n)",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#regresión-lineal",
    "href": "introduccion.html#regresión-lineal",
    "title": "Introducción a Python para Minería de Datos",
    "section": "7. Regresión Lineal",
    "text": "7. Regresión Lineal\n\n7.1 Implementación y Ajuste del Modelo\n\n# Generamos datos para trabajar\ndf = generar_datos_lineales(n=1000, slope=1.0, intercept=2.0, noise_scale=1.5)\n\n# 1. Crear el modelo de regresión lineal\nmodelo_lr = LinearRegression()\n\n# 2. Ajustar el modelo (entrenamiento)\n# Nota: sklearn necesita X como matriz (2D) y y como vector (1D)\nX_features = df[[\"x\"]]  # Matriz 2D\ny_target = df[\"y\"]     # Vector 1D\n\nmodelo_lr.fit(X_features, y_target)\n\nprint(f\"Coeficiente (pendiente): {modelo_lr.coef_[0]:.3f}\")\nprint(f\"Intercepto: {modelo_lr.intercept_:.3f}\")\n\nCoeficiente (pendiente): 1.006\nIntercepto: 1.935\n\n\n\n\n7.2 Predicciones y Evaluación\n\n# 3. Hacer predicciones\ny_pred_lr = modelo_lr.predict(X_features)\n\n# 4. Evaluar el modelo\nmse_lr = mean_squared_error(y_target, y_pred_lr)\nprint(f\"Error Cuadrático Medio (MSE): {mse_lr:.3f}\")\n\n# Agregar predicciones al DataFrame para visualización\ndf[\"y_pred_lr\"] = y_pred_lr\n\nError Cuadrático Medio (MSE): 2.369\n\n\n\n\n7.3 Visualización del Modelo\n\n# Visualización de datos originales y línea de regresión\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"x\", y=\"y\", alpha=0.6, label=\"Datos Originales\")\nsns.lineplot(data=df, x=\"x\", y=\"y_pred_lr\", color=\"red\", linewidth=2, label=\"Regresión Lineal\")\nplt.title(\"Regresión Lineal: Datos vs Predicciones\")\nplt.xlabel(\"Variable Independiente (x)\")\nplt.ylabel(\"Variable Dependiente (y)\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#k-nearest-neighbors-knn",
    "href": "introduccion.html#k-nearest-neighbors-knn",
    "title": "Introducción a Python para Minería de Datos",
    "section": "8. K-Nearest Neighbors (KNN)",
    "text": "8. K-Nearest Neighbors (KNN)\n\n8.1 Implementación Básica de KNN\n\n# 1. Crear modelo KNN con k=10\nmodelo_knn = KNeighborsRegressor(n_neighbors=10)\n\n# 2. Ajustar el modelo\nmodelo_knn.fit(X_features, y_target)\n\n# 3. Hacer predicciones\ny_pred_knn = modelo_knn.predict(X_features)\n\n# 4. Evaluar el modelo\nmse_knn = mean_squared_error(y_target, y_pred_knn)\nprint(f\"MSE KNN (k=10): {mse_knn:.3f}\")\nprint(f\"MSE Regresión Lineal: {mse_lr:.3f}\")\n\n# Agregar predicciones al DataFrame\ndf[\"y_pred_knn10\"] = y_pred_knn\n\nMSE KNN (k=10): 2.055\nMSE Regresión Lineal: 2.369\n\n\n\n\n8.2 Función para Evaluar Diferentes Valores de K\n\ndef evaluar_knn_diferentes_k(\n    X: pd.DataFrame, \n    y: pd.Series, \n    valores_k: list\n) -&gt; dict:\n    \"\"\"\n    Evalúa el rendimiento de KNN para diferentes valores de k.\n    \n    Parámetros:\n    -----------\n    X : pd.DataFrame\n        Variables independientes\n    y : pd.Series\n        Variable dependiente\n    valores_k : list\n        Lista de valores de k a evaluar\n    \n    Retorna:\n    --------\n    dict\n        Diccionario con k como clave y MSE como valor\n    \"\"\"\n    resultados = {}\n    \n    for k in valores_k:\n        # Crear y ajustar modelo\n        knn = KNeighborsRegressor(n_neighbors=k)\n        knn.fit(X, y)\n        \n        # Predicciones y evaluación\n        y_pred = knn.predict(X)\n        mse = mean_squared_error(y, y_pred)\n        \n        resultados[k] = mse\n        print(f\"k={k}: MSE={mse:.3f}\")\n    \n    return resultados\n\n# Evaluar diferentes valores de k\nvalores_k = [1, 5, 10, 20, 50, 100]\nprint(\"Evaluando diferentes valores de k:\")\nresultados_k = evaluar_knn_diferentes_k(X_features, y_target, valores_k)\n\nEvaluando diferentes valores de k:\nk=1: MSE=0.000\nk=5: MSE=1.837\nk=10: MSE=2.055\nk=20: MSE=2.206\nk=50: MSE=2.335\nk=100: MSE=2.367\n\n\n\n\n8.3 Visualización de Diferentes Valores de K\n\n# Crear DataFrame para comparar diferentes valores de k\ndf_comparacion = df[[\"x\", \"y\"]].copy()\n\n# Agregar predicciones para diferentes valores de k\nfor k in [1, 10, 100]:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_features, y_target)\n    df_comparacion[f\"y_pred_k{k}\"] = knn.predict(X_features)\n\n# Visualización comparativa\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, k in enumerate([1, 10, 100]):\n    ax = axes[i]\n    ax.scatter(df_comparacion[\"x\"], df_comparacion[\"y\"], alpha=0.6, label=\"Datos\")\n    ax.plot(df_comparacion[\"x\"], df_comparacion[f\"y_pred_k{k}\"], 'r-', linewidth=2, label=f\"KNN k={k}\")\n    ax.set_title(f\"KNN con k={k}\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#selección-del-valor-óptimo-de-k",
    "href": "introduccion.html#selección-del-valor-óptimo-de-k",
    "title": "Introducción a Python para Minería de Datos",
    "section": "9. Selección del Valor Óptimo de K",
    "text": "9. Selección del Valor Óptimo de K\n\n9.1 Evaluación Sistemática de K\n\n# Generar un dataset más grande para la evaluación\ndf_grande = generar_datos_lineales(n=1000, slope=2.0, intercept=3.0, noise_scale=2.0)\nX_grande = df_grande[[\"x\"]]\ny_grande = df_grande[\"y\"]\n\n# Rango de valores k a evaluar\nmax_k = min(200, len(df_grande) // 5)  # k máximo razonable\nvalores_k_rango = np.arange(1, max_k, 10)\n\nprint(f\"Evaluando k desde 1 hasta {max_k-1} (cada 10 valores)\")\nprint(f\"Total de valores a evaluar: {len(valores_k_rango)}\")\n\nEvaluando k desde 1 hasta 199 (cada 10 valores)\nTotal de valores a evaluar: 20\n\n\n\n# Evaluación de todos los valores de k\nerrores_k = []\nfor k in valores_k_rango:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_grande, y_grande)\n    y_pred = knn.predict(X_grande)\n    mse = mean_squared_error(y_grande, y_pred)\n    errores_k.append(mse)\n\n# Encontrar el k óptimo\nk_optimo = valores_k_rango[np.argmin(errores_k)]\nerror_minimo = min(errores_k)\n\nprint(f\"Valor óptimo de k: {k_optimo}\")\nprint(f\"MSE mínimo: {error_minimo:.3f}\")\n\nValor óptimo de k: 1\nMSE mínimo: 0.000\n\n\n\n\n9.2 Visualización de la Curva de Error vs K\n\n# Gráfica de MSE vs k\nplt.figure(figsize=(10, 6))\nplt.plot(valores_k_rango, errores_k, 'b-', linewidth=2, marker='o', markersize=4)\nplt.axvline(x=k_optimo, color='r', linestyle='--', \n            label=f'k óptimo = {k_optimo}\\nMSE = {error_minimo:.3f}')\nplt.xlabel('Valor de k')\nplt.ylabel('Error Cuadrático Medio (MSE)')\nplt.title('Selección del Valor Óptimo de k en KNN')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#estimación-del-error-real",
    "href": "introduccion.html#estimación-del-error-real",
    "title": "Introducción a Python para Minería de Datos",
    "section": "10. Estimación del Error Real",
    "text": "10. Estimación del Error Real\n\n10.1 División Train-Test\n\n# Generar dataset para validación\nnp.random.seed(42)  # Para reproducibilidad\ndf_validacion = generar_datos_lineales(n=2000, slope=1.5, intercept=2.5, noise_scale=2.0)\n\n# División train-test\nX_val = df_validacion[[\"x\"]]\ny_val = df_validacion[\"y\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_val, y_val, test_size=0.3, random_state=42\n)\n\nprint(f\"Datos de entrenamiento: {X_train.shape[0]}\")\nprint(f\"Datos de prueba: {X_test.shape[0]}\")\n\nDatos de entrenamiento: 1400\nDatos de prueba: 600\n\n\n\n\n10.2 Función para Evaluación Train-Test\n\ndef evaluar_modelo_train_test(\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n    k: int\n) -&gt; tuple:\n    \"\"\"\n    Evalúa KNN usando división train-test.\n    \n    Retorna:\n    --------\n    tuple\n        (error_entrenamiento, error_prueba)\n    \"\"\"\n    # Entrenar el modelo\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    # Predicciones\n    y_train_pred = knn.predict(X_train)\n    y_test_pred = knn.predict(X_test)\n    \n    # Errores\n    error_train = mean_squared_error(y_train, y_train_pred)\n    error_test = mean_squared_error(y_test, y_test_pred)\n    \n    return error_train, error_test\n\n# Evaluar diferentes valores de k\nvalores_k_test = [1, 5, 10, 20, 50, 100]\nerrores_train = []\nerrores_test = []\n\nprint(\"Evaluación con división train-test:\")\nprint(\"-\" * 40)\nfor k in valores_k_test:\n    error_train, error_test = evaluar_modelo_train_test(X_train, X_test, y_train, y_test, k)\n    errores_train.append(error_train)\n    errores_test.append(error_test)\n    print(f\"k={k:2d} | Train MSE: {error_train:.3f} | Test MSE: {error_test:.3f}\")\n\nEvaluación con división train-test:\n----------------------------------------\nk= 1 | Train MSE: 0.000 | Test MSE: 7.600\nk= 5 | Train MSE: 3.092 | Test MSE: 5.007\nk=10 | Train MSE: 3.436 | Test MSE: 4.425\nk=20 | Train MSE: 3.641 | Test MSE: 4.214\nk=50 | Train MSE: 3.820 | Test MSE: 4.016\nk=100 | Train MSE: 3.874 | Test MSE: 3.975\n\n\n\n\n10.3 Visualización de Sesgo vs Varianza\n\n# Gráfica de error de entrenamiento vs error de prueba\nplt.figure(figsize=(10, 6))\nplt.plot(valores_k_test, errores_train, 'b-o', linewidth=2, label='Error de Entrenamiento')\nplt.plot(valores_k_test, errores_test, 'r-o', linewidth=2, label='Error de Prueba')\nplt.xlabel('Valor de k')\nplt.ylabel('Error Cuadrático Medio (MSE)')\nplt.title('Curva de Validación: Error de Entrenamiento vs Error de Prueba')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')  # Escala logarítmica para mejor visualización\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.4 Comparación Final: Regresión Lineal vs KNN Óptimo\n\n# Encontrar el k óptimo basado en error de prueba\nk_optimo_test = valores_k_test[np.argmin(errores_test)]\nprint(f\"Valor óptimo de k (basado en error de prueba): {k_optimo_test}\")\n\n# Evaluar regresión lineal\nlr_final = LinearRegression()\nlr_final.fit(X_train, y_train)\ny_test_pred_lr = lr_final.predict(X_test)\nerror_test_lr = mean_squared_error(y_test, y_test_pred_lr)\n\n# Evaluar KNN óptimo\nknn_final = KNeighborsRegressor(n_neighbors=k_optimo_test)\nknn_final.fit(X_train, y_train)\ny_test_pred_knn = knn_final.predict(X_test)\nerror_test_knn = mean_squared_error(y_test, y_test_pred_knn)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"COMPARACIÓN FINAL - ERROR DE PRUEBA (ESTIMACIÓN REAL)\")\nprint(\"=\"*50)\nprint(f\"Regresión Lineal:        {error_test_lr:.3f}\")\nprint(f\"KNN (k={k_optimo_test}):             {error_test_knn:.3f}\")\nprint(\"=\"*50)\n\nif error_test_knn &lt; error_test_lr:\n    mejora = ((error_test_lr - error_test_knn) / error_test_lr) * 100\n    print(f\"KNN es mejor por {mejora:.1f}%\")\nelse:\n    mejora = ((error_test_knn - error_test_lr) / error_test_knn) * 100\n    print(f\"Regresión Lineal es mejor por {mejora:.1f}%\")\n\nValor óptimo de k (basado en error de prueba): 100\n\n==================================================\nCOMPARACIÓN FINAL - ERROR DE PRUEBA (ESTIMACIÓN REAL)\n==================================================\nRegresión Lineal:        3.887\nKNN (k=100):             3.975\n==================================================\nRegresión Lineal es mejor por 2.2%",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#resumen",
    "href": "introduccion.html#resumen",
    "title": "Introducción a Python para Minería de Datos",
    "section": "Resumen",
    "text": "Resumen\nEn este notebook hemos cubierto:\n\nNumPy: Creación y manipulación de arrays para datos numéricos\nPandas: Manipulación de datos estructurados con DataFrames\nIteración: Técnicas para iterar sobre diccionarios y listas\nVisualización: Creación de gráficas informativas con matplotlib/seaborn\nType Hints: Definición de funciones bien documentadas y tipadas\nRegresión Lineal: Implementación, ajuste y evaluación\nKNN: Implementación y comparación de diferentes valores de k\nSelección de Hiperparámetros: Técnicas para encontrar el k óptimo\nValidación: Estimación del error real usando división train-test\n\n\nConceptos Clave Aprendidos:\n\nSesgo vs Varianza: KNN con k pequeño tiene alta varianza, k grande tiene alto sesgo\nValidación: El error de entrenamiento subestima el error real\nSelección de Modelos: Comparar diferentes algoritmos usando datos de prueba\nType Hints: Mejoran la legibilidad y mantenibilidad del código",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "regresion_lineal_alumnos.html",
    "href": "regresion_lineal_alumnos.html",
    "title": "Descenso en gradiente con regresión lineal",
    "section": "",
    "text": "Implementación multivariada\ndef generar_datos_lineales_multivariados(n_muestras: int, n_caracteristicas: int, ruido: float = 0.5) -&gt; tuple[pd.DataFrame, np.ndarray, float]:\n    \"\"\"\n    Genera un conjunto de datos sintético para regresión lineal multivariada.\n\n    Args:\n        n_muestras (int): El número de puntos de datos a generar (filas).\n        n_caracteristicas (int): El número de variables independientes (características).\n        ruido (float): La desviación estándar del ruido gaussiano a añadir.\n                       Controla la dispersión de los puntos.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray, float]:\n            - Un DataFrame de pandas con las características (x1, x2, ...) y la variable objetivo (y).\n            - El array de coeficientes (pesos) reales que se usaron para generar los datos.\n            - El intercepto (sesgo) real que se usó.\n    \"\"\"\n    # 1. Generar las características (X) con valores aleatorios entre 0 y 10\n    X = 10 * np.random.rand(n_muestras, n_caracteristicas)\n\n    # 2. Generar coeficientes e intercepto reales aleatorios\n    # Estos son los \"verdaderos\" parámetros que un modelo intentaría encontrar.\n    coeficientes_reales = np.random.randn(n_caracteristicas) * 2\n    intercepto_real = np.random.randn() * 5\n\n    # 3. Generar el ruido gaussiano\n    # El ruido simula la variabilidad aleatoria en los datos del mundo real.\n    ruido_gaussiano = np.random.randn(n_muestras) * ruido\n\n    # 4. Calcular la variable objetivo (y) usando la ecuación lineal\n    # y = (X • coeficientes) + intercepto + ruido\n    y = np.dot(X, coeficientes_reales) + intercepto_real + ruido_gaussiano\n\n    # 5. Formatear la salida en un DataFrame de pandas\n    nombres_columnas = [f'x{i+1}' for i in range(n_caracteristicas)]\n    datos = pd.DataFrame(X, columns=nombres_columnas)\n    datos['y'] = y\n\n    return datos, coeficientes_reales, intercepto_real\nnum_muestras = 200\nnum_caracteristicas = 10 \nnivel_ruido = 1.5\n\n# Generar los datos\ndatos_generados, coeficientes, intercepto = generar_datos_lineales_multivariados(\n    n_muestras=num_muestras,\n    n_caracteristicas=num_caracteristicas,\n    ruido=nivel_ruido\n)\ndef descenso_gradiente_multivariado(datos: pd.DataFrame, learning_rate: float, iteraciones: int) -&gt; tuple[np.ndarray, float, list]:\n    \"\"\"\n    Realiza el descenso de gradiente para una regresión lineal multivariada.\n\n    Args:\n        datos (pd.DataFrame): DataFrame que contiene las características y la variable objetivo 'y'.\n        learning_rate (float): La tasa de aprendizaje.\n        iteraciones (int): El número de iteraciones para ejecutar el algoritmo.\n\n    Returns:\n        tuple[np.ndarray, float, list]:\n            - El array de coeficientes (pesos) optimizados.\n            - El intercepto (sesgo) optimizado.\n            - Una lista con el historial del Error Cuadrático Medio (MSE) en cada iteración.\n    \"\"\"\n    # 1. Preparar los datos\n    X = ...  # Matriz de características\n    y = datos['y'].values              # Vector de la variable objetivo\n    n_muestras, n_caracteristicas = ...\n\n    # 2. Inicializar parámetros\n    coeficientes = np.zeros(n_caracteristicas)\n    intercepto = 0.0\n    historial_error = []\n\n    # 3. Iterar para optimizar los parámetros\n    for i in range(iteraciones):\n        # Calcular las predicciones (producto punto de X y coeficientes)\n        # Ecuación: y_pred = (X • coeficientes) + intercepto\n        y_pred = ...\n\n        # Calcular el Error Cuadrático Medio (MSE) y guardarlo\n        error = ...\n        historial_error.append(error)\n\n        # Calcular los gradientes (derivadas parciales)\n        # El gradiente es la dirección de máximo ascenso del error.\n        # Lo calculamos de forma vectorizada para eficiencia.\n        D_coeficientes = ...\n        D_intercepto = ...\n\n        # 4. Actualizar los parámetros (moverse en dirección opuesta al gradiente)\n        coeficientes = ...\n        intercepto = ...\n\n    return coeficientes, intercepto, historial_error\nnum_muestras = 200\nnum_caracteristicas = 3\nnivel_ruido = 1.5\ndatos_generados, coef_reales, int_real = generar_datos_lineales_multivariados(\n    n_muestras=num_muestras,\n    n_caracteristicas=num_caracteristicas,\n    ruido=nivel_ruido\n)\n\nX_original = datos_generados.drop('y', axis=1)\ny = datos_generados['y']\n\nscaler = StandardScaler()\nX_escalado = scaler.fit_transform(X_original)\n\ndatos_escalados = pd.DataFrame(X_escalado, columns=X_original.columns)\ndatos_escalados['y'] = y.values\n\nlearning_rate = 0.01\niteraciones = 1000\ncoef_aprendidos, int_aprendido, error_hist = descenso_gradiente_multivariado(\n    datos=datos_escalados,\n    learning_rate=learning_rate,\n    iteraciones=iteraciones\n)\n\nmodelo_sklearn = LinearRegression()\nmodelo_sklearn.fit(X_escalado, y)\ncoef_sklearn = modelo_sklearn.coef_\nint_sklearn = modelo_sklearn.intercept_\nprint(\"--- Comparación de Parámetros (con datos escalados) ---\")\nprint(f\"Intercepto GD: {int_aprendido:.4f}  |  Intercepto Sklearn: {int_sklearn:.4f}\")\nfor i in range(num_caracteristicas):\n    print(f\"Coef. x{i+1} GD: {coef_aprendidos[i]:.4f} |  Coef. x{i+1} Sklearn: {coef_sklearn[i]:.4f}\")",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descenso en gradiente con regresión lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html",
    "href": "violaciones_supuestos_regresion.html",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "",
    "text": "1. Violación del Supuesto de Linealidad\nEste notebook demuestra gráficamente cómo pueden violarse los cuatro supuestos principales de la regresión lineal:\nEl supuesto de linealidad requiere que la relación entre X e Y sea lineal. Cuando esto se viola, un modelo lineal será inadecuado.\ndef crear_datos_no_lineales(n=200, tipo='cuadratico', noise=1.0, seed=42):\n    \"\"\"Genera datos con relaciones no lineales\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(-3, 3, n)\n    \n    if tipo == 'cuadratico':\n        y = 2 * x**2 + 1 + np.random.normal(0, noise, n)\n        titulo = \"Relación Cuadrática\"\n    elif tipo == 'exponencial':\n        y = np.exp(x/2) + np.random.normal(0, noise, n)\n        titulo = \"Relación Exponencial\"\n    elif tipo == 'sinusoidal':\n        y = 5 * np.sin(2*x) + x + np.random.normal(0, noise, n)\n        titulo = \"Relación Sinusoidal\"\n    elif tipo == 'logaritmico':\n        x = np.linspace(0.1, 10, n)\n        y = 3 * np.log(x) + 2 + np.random.normal(0, noise, n)\n        titulo = \"Relación Logarítmica\"\n    \n    return x, y, titulo\n\n# Crear subplots para diferentes tipos de no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Violaciones del Supuesto de Linealidad', fontsize=16, fontweight='bold')\n\ntipos = ['cuadratico', 'exponencial', 'sinusoidal', 'logaritmico']\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    \n    # Gráfico\n    axes[row, col].scatter(x, y, alpha=0.6, label='Datos reales')\n    axes[row, col].plot(x, y_pred, 'r-', linewidth=2, label='Regresión lineal')\n    axes[row, col].set_title(titulo)\n    axes[row, col].set_xlabel('X')\n    axes[row, col].set_ylabel('Y')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n    \n    # Calcular R²\n    r2 = lr.score(X_reshaped, y)\n    axes[row, col].text(0.05, 0.95, f'R² = {r2:.3f}', \n                        transform=axes[row, col].transAxes, \n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-linealidad",
    "href": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-linealidad",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "",
    "text": "Diagnóstico: Gráficos de Residuos vs Valores Predichos\nUna forma de detectar no linealidad es examinar los residuos. Si hay patrones en los residuos, indica problemas con el modelo.\n\n# Crear gráficos de residuos para detectar no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Gráficos de Residuos: Detectando No Linealidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    residuos = y - y_pred\n    \n    # Gráfico de residuos\n    axes[row, col].scatter(y_pred, residuos, alpha=0.6)\n    axes[row, col].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[row, col].set_title(f'Residuos - {titulo}')\n    axes[row, col].set_xlabel('Valores Predichos')\n    axes[row, col].set_ylabel('Residuos')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 INTERPRETACIÓN:\")\nprint(\"✅ Residuos aleatorios alrededor de 0 = Linealidad OK\")\nprint(\"❌ Patrones en residuos = Violación de linealidad\")\nprint(\"   - Curva: Relación cuadrática o polinomial\")\nprint(\"   - Tendencia: Relación exponencial o logarítmica\")\nprint(\"   - Oscilación: Efectos cíclicos o sinusoidales\")\n\n\n\n\n\n\n\n\n📊 INTERPRETACIÓN:\n✅ Residuos aleatorios alrededor de 0 = Linealidad OK\n❌ Patrones en residuos = Violación de linealidad\n   - Curva: Relación cuadrática o polinomial\n   - Tendencia: Relación exponencial o logarítmica\n   - Oscilación: Efectos cíclicos o sinusoidales",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#solución",
    "href": "violaciones_supuestos_regresion.html#solución",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "Solución",
    "text": "Solución\n\n# Crear subplots para diferentes tipos de no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Violaciones del Supuesto de Linealidad', fontsize=16, fontweight='bold')\n\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    if tipo == \"cuadratico\":\n        X_reshaped = X_reshaped**2\n    if tipo == \"exponencial\":\n        X_reshaped = np.exp(X_reshaped)\n    if tipo == \"sinusoidal\":\n        X_reshaped = np.concatenate((np.sin(2*X_reshaped), X_reshaped), axis = 1)\n    if tipo == \"logaritmico\":\n        X_reshaped = np.log(X_reshaped)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    \n    # Gráfico\n    axes[row, col].scatter(x, y, alpha=0.6, label='Datos reales')\n    axes[row, col].plot(x, y_pred, 'r-', linewidth=2, label='Regresión lineal')\n    axes[row, col].set_title(titulo)\n    axes[row, col].set_xlabel('X')\n    axes[row, col].set_ylabel('Y')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n    \n    # Calcular R²\n    r2 = lr.score(X_reshaped, y)\n    axes[row, col].text(0.05, 0.95, f'R² = {r2:.3f}', \n                        transform=axes[row, col].transAxes, \n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Crear gráficos de residuos para detectar no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Gráficos de Residuos: Detectando No Linealidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    if tipo == \"cuadratico\":\n        X_reshaped = X_reshaped**2\n    if tipo == \"exponencial\":\n        X_reshaped = np.exp(X_reshaped)\n    if tipo == \"sinusoidal\":\n        X_reshaped = np.concatenate((np.sin(2*X_reshaped), X_reshaped), axis = 1)\n    if tipo == \"logaritmico\":\n        X_reshaped = np.log(X_reshaped)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    residuos = y - y_pred\n    \n    # Gráfico de residuos\n    axes[row, col].scatter(y_pred, residuos, alpha=0.6)\n    axes[row, col].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[row, col].set_title(f'Residuos - {titulo}')\n    axes[row, col].set_xlabel('Valores Predichos')\n    axes[row, col].set_ylabel('Residuos')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-independencia",
    "href": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-independencia",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "2. Violación del Supuesto de Independencia",
    "text": "2. Violación del Supuesto de Independencia\nLos errores deben ser independientes entre sí. Esto es especialmente importante en datos de series temporales.\n\ndef crear_datos_correlacionados(n=200, correlacion=0.7, seed=42):\n    \"\"\"Genera datos con errores correlacionados (autocorrelación)\"\"\"\n    np.random.seed(seed)\n    \n    # Datos independientes (correctos)\n    x = np.linspace(0, 10, n)\n    errores_independientes = np.random.normal(0, 2, n)\n    y_independiente = 2 + 1.5 * x + errores_independientes\n    \n    # Datos con autocorrelación (violación)\n    errores_correlacionados = np.zeros(n)\n    errores_correlacionados[0] = np.random.normal(0, 2)\n    \n    for i in range(1, n):\n        errores_correlacionados[i] = (correlacion * errores_correlacionados[i-1] + \n                                     np.sqrt(1 - correlacion**2) * np.random.normal(0, 2))\n    \n    y_correlacionado = 2 + 1.5 * x + errores_correlacionados\n    \n    return x, y_independiente, y_correlacionado, errores_independientes, errores_correlacionados\n\n# Generar datos\nx, y_indep, y_corr, err_indep, err_corr = crear_datos_correlacionados()\n\n# Ajustar modelos\nlr_indep = LinearRegression().fit(x.reshape(-1, 1), y_indep)\nlr_corr = LinearRegression().fit(x.reshape(-1, 1), y_corr)\n\ny_pred_indep = lr_indep.predict(x.reshape(-1, 1))\ny_pred_corr = lr_corr.predict(x.reshape(-1, 1))\n\nresiduos_indep = y_indep - y_pred_indep\nresiduos_corr = y_corr - y_pred_corr\n\n# Crear gráficos\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Violación del Supuesto de Independencia', fontsize=16, fontweight='bold')\n\n# Fila 1: Datos independientes (correctos)\naxes[0, 0].scatter(x, y_indep, alpha=0.6, color='blue')\naxes[0, 0].plot(x, y_pred_indep, 'r-', linewidth=2)\naxes[0, 0].set_title('Datos con Errores Independientes ✅')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('Y')\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(residuos_indep, 'o-', alpha=0.7, color='blue')\naxes[0, 1].axhline(y=0, color='red', linestyle='--')\naxes[0, 1].set_title('Residuos vs Tiempo')\naxes[0, 1].set_xlabel('Observación')\naxes[0, 1].set_ylabel('Residuos')\naxes[0, 1].grid(True, alpha=0.3)\n\naxes[0, 2].scatter(residuos_indep[:-1], residuos_indep[1:], alpha=0.6, color='blue')\naxes[0, 2].set_title('Residuos(t) vs Residuos(t-1)')\naxes[0, 2].set_xlabel('Residuo en t-1')\naxes[0, 2].set_ylabel('Residuo en t')\naxes[0, 2].grid(True, alpha=0.3)\n\n# Calcular correlación\ncorr_indep = np.corrcoef(residuos_indep[:-1], residuos_indep[1:])[0, 1]\naxes[0, 2].text(0.05, 0.95, f'r = {corr_indep:.3f}', \n                transform=axes[0, 2].transAxes,\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n# Fila 2: Datos correlacionados (violación)\naxes[1, 0].scatter(x, y_corr, alpha=0.6, color='orange')\naxes[1, 0].plot(x, y_pred_corr, 'r-', linewidth=2)\naxes[1, 0].set_title('Datos con Errores Correlacionados ❌')\naxes[1, 0].set_xlabel('X')\naxes[1, 0].set_ylabel('Y')\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(residuos_corr, 'o-', alpha=0.7, color='orange')\naxes[1, 1].axhline(y=0, color='red', linestyle='--')\naxes[1, 1].set_title('Residuos vs Tiempo')\naxes[1, 1].set_xlabel('Observación')\naxes[1, 1].set_ylabel('Residuos')\naxes[1, 1].grid(True, alpha=0.3)\n\naxes[1, 2].scatter(residuos_corr[:-1], residuos_corr[1:], alpha=0.6, color='orange')\naxes[1, 2].set_title('Residuos(t) vs Residuos(t-1)')\naxes[1, 2].set_xlabel('Residuo en t-1')\naxes[1, 2].set_ylabel('Residuo en t')\naxes[1, 2].grid(True, alpha=0.3)\n\n# Calcular correlación\ncorr_corr = np.corrcoef(residuos_corr[:-1], residuos_corr[1:])[0, 1]\naxes[1, 2].text(0.05, 0.95, f'r = {corr_corr:.3f}', \n                transform=axes[1, 2].transAxes,\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 INTERPRETACIÓN:\")\nprint(f\"✅ Errores independientes: correlación = {corr_indep:.3f} (cerca de 0)\")\nprint(f\"❌ Errores correlacionados: correlación = {corr_corr:.3f} (lejos de 0)\")\nprint(\"\\n🔍 DIAGNÓSTICO:\")\nprint(\"- Gráfico temporal: patrones o tendencias en residuos\")\nprint(\"- Autocorrelación: correlación significativa entre residuos consecutivos\")\nprint(\"- Prueba Durbin-Watson: estadístico cerca de 2 = independencia\")\n\n\n\n\n\n\n\n\n📊 INTERPRETACIÓN:\n✅ Errores independientes: correlación = -0.058 (cerca de 0)\n❌ Errores correlacionados: correlación = 0.670 (lejos de 0)\n\n🔍 DIAGNÓSTICO:\n- Gráfico temporal: patrones o tendencias en residuos\n- Autocorrelación: correlación significativa entre residuos consecutivos\n- Prueba Durbin-Watson: estadístico cerca de 2 = independencia",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-homocedasticidad",
    "href": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-homocedasticidad",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "3. Violación del Supuesto de Homocedasticidad",
    "text": "3. Violación del Supuesto de Homocedasticidad\nLa varianza de los errores debe ser constante. Cuando varía, tenemos heterocedasticidad.\n\ndef crear_datos_heteroscedasticos(n=200, tipo='creciente', seed=42):\n    \"\"\"Genera datos con diferentes tipos de heterocedasticidad\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(1, 10, n)\n    \n    if tipo == 'homoscedastico':\n        # Varianza constante (correcto)\n        error = np.random.normal(0, 2, n)\n        titulo = \"Homocedasticidad (Correcto) ✅\"\n    elif tipo == 'creciente':\n        # Varianza aumenta con X\n        error = np.random.normal(0, 0.5 * x, n)\n        titulo = \"Heterocedasticidad Creciente ❌\"\n    elif tipo == 'decreciente':\n        # Varianza disminuye con X\n        error = np.random.normal(0, 5 / x, n)\n        titulo = \"Heterocedasticidad Decreciente ❌\"\n    elif tipo == 'embudo':\n        # Forma de embudo\n        error = np.random.normal(0, 0.1 + 0.8 * np.abs(x - 5.5), n)\n        titulo = \"Heterocedasticidad en Embudo ❌\"\n    \n    y = 2 + 1.5 * x + error\n    return x, y, titulo\n\n# Crear diferentes tipos de heterocedasticidad\ntipos_het = ['homoscedastico', 'creciente', 'decreciente', 'embudo']\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\nfig.suptitle('Violación del Supuesto de Homocedasticidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos_het):\n    x, y, titulo = crear_datos_heteroscedasticos(tipo=tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    # Gráfico de datos\n    color = 'green' if tipo == 'homoscedastico' else 'red'\n    axes[0, i].scatter(x, y, alpha=0.6, color=color)\n    axes[0, i].plot(x, y_pred, 'black', linewidth=2)\n    axes[0, i].set_title(titulo)\n    axes[0, i].set_xlabel('X')\n    axes[0, i].set_ylabel('Y')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Gráfico de residuos\n    axes[1, i].scatter(y_pred, residuos, alpha=0.6, color=color)\n    axes[1, i].axhline(y=0, color='black', linestyle='--')\n    axes[1, i].set_title(f'Residuos vs Predichos')\n    axes[1, i].set_xlabel('Valores Predichos')\n    axes[1, i].set_ylabel('Residuos')\n    axes[1, i].grid(True, alpha=0.3)\n    \n    # Añadir líneas de tendencia en residuos para visualizar heterocedasticidad\n    if tipo != 'homoscedastico':\n        z = np.polyfit(y_pred, np.abs(residuos), 1)\n        p = np.poly1d(z)\n        axes[1, i].plot(y_pred, p(y_pred), \"r--\", alpha=0.8)\n        axes[1, i].plot(y_pred, -p(y_pred), \"r--\", alpha=0.8)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 INTERPRETACIÓN:\")\nprint(\"✅ Homocedasticidad: Residuos con dispersión constante\")\nprint(\"❌ Heterocedasticidad: Residuos con dispersión variable\")\nprint(\"\\n🔍 PATRONES COMUNES:\")\nprint(\"- Embudo creciente: Varianza aumenta con valores predichos\")\nprint(\"- Embudo decreciente: Varianza disminuye con valores predichos\")\nprint(\"- Forma de diamante: Varianza máxima en valores medios\")\n\n\n\n\n\n\n\n\n📊 INTERPRETACIÓN:\n✅ Homocedasticidad: Residuos con dispersión constante\n❌ Heterocedasticidad: Residuos con dispersión variable\n\n🔍 PATRONES COMUNES:\n- Embudo creciente: Varianza aumenta con valores predichos\n- Embudo decreciente: Varianza disminuye con valores predichos\n- Forma de diamante: Varianza máxima en valores medios\n\n\n\nPrueba Estadística: Test de Breusch-Pagan\nUna prueba formal para detectar heterocedasticidad.\n\nfrom scipy.stats import chi2\n\ndef breusch_pagan_test(residuos, x):\n    \"\"\"Implementa el test de Breusch-Pagan para heterocedasticidad\"\"\"\n    n = len(residuos)\n    \n    # Regresión de residuos al cuadrado sobre X\n    residuos_cuadrados = residuos**2\n    lr_bp = LinearRegression()\n    lr_bp.fit(x.reshape(-1, 1), residuos_cuadrados)\n    \n    # R² de la regresión auxiliar\n    r2_bp = lr_bp.score(x.reshape(-1, 1), residuos_cuadrados)\n    \n    # Estadístico de prueba\n    lm_statistic = n * r2_bp\n    \n    # P-value (distribución chi-cuadrado con 1 grado de libertad)\n    p_value = 1 - chi2.cdf(lm_statistic, df=1)\n    \n    return lm_statistic, p_value\n\nprint(\"🧪 TEST DE BREUSCH-PAGAN PARA HETEROCEDASTICIDAD\\n\")\nprint(\"H₀: Homocedasticidad (varianza constante)\")\nprint(\"H₁: Heterocedasticidad (varianza no constante)\\n\")\n\nfor tipo in tipos_het:\n    x, y, titulo = crear_datos_heteroscedasticos(tipo=tipo)\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    residuos = y - lr.predict(x.reshape(-1, 1))\n    \n    lm_stat, p_val = breusch_pagan_test(residuos, x)\n    \n    resultado = \"RECHAZA H₀\" if p_val &lt; 0.05 else \"NO RECHAZA H₀\"\n    emoji = \"❌\" if p_val &lt; 0.05 else \"✅\"\n    \n    print(f\"{emoji} {tipo.upper():15} | LM = {lm_stat:6.2f} | p-value = {p_val:.4f} | {resultado}\")\n\nprint(\"\\n💡 Interpretación: p-value &lt; 0.05 indica heterocedasticidad\")\n\n🧪 TEST DE BREUSCH-PAGAN PARA HETEROCEDASTICIDAD\n\nH₀: Homocedasticidad (varianza constante)\nH₁: Heterocedasticidad (varianza no constante)\n\n✅ HOMOSCEDASTICO  | LM =   0.00 | p-value = 0.9588 | NO RECHAZA H₀\n❌ CRECIENTE       | LM =  27.79 | p-value = 0.0000 | RECHAZA H₀\n❌ DECRECIENTE     | LM =  31.91 | p-value = 0.0000 | RECHAZA H₀\n✅ EMBUDO          | LM =   0.14 | p-value = 0.7122 | NO RECHAZA H₀\n\n💡 Interpretación: p-value &lt; 0.05 indica heterocedasticidad",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-normalidad",
    "href": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-normalidad",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "4. Violación del Supuesto de Normalidad",
    "text": "4. Violación del Supuesto de Normalidad\nLos errores deben seguir una distribución normal. Esto es crucial para las pruebas de hipótesis y intervalos de confianza.\n\ndef crear_datos_no_normales(n=200, tipo='normal', seed=42):\n    \"\"\"Genera datos con diferentes distribuciones de error\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(0, 10, n)\n    \n    if tipo == 'normal':\n        error = np.random.normal(0, 2, n)\n        titulo = \"Errores Normales ✅\"\n    elif tipo == 'asimetrico':\n        # Distribución asimétrica (exponencial)\n        error = np.random.exponential(2, n) - 2\n        titulo = \"Errores Asimétricos ❌\"\n    elif tipo == 'colas_pesadas':\n        # Distribución t con colas pesadas\n        error = stats.t.rvs(df=3, scale=2, size=n)\n        titulo = \"Errores con Colas Pesadas ❌\"\n    elif tipo == 'bimodal':\n        # Distribución bimodal\n        mask = np.random.binomial(1, 0.5, n).astype(bool)\n        error = np.where(mask, \n                        np.random.normal(-2, 1, n),\n                        np.random.normal(2, 1, n))\n        titulo = \"Errores Bimodales ❌\"\n    \n    y = 2 + 1.5 * x + error\n    return x, y, error, titulo\n\n# Crear diferentes tipos de no normalidad\ntipos_norm = ['normal', 'asimetrico', 'colas_pesadas', 'bimodal']\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\nfig.suptitle('Violación del Supuesto de Normalidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos_norm):\n    x, y, error_real, titulo = crear_datos_no_normales(tipo=tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    color = 'green' if tipo == 'normal' else 'red'\n    \n    # Fila 1: Datos originales\n    axes[0, i].scatter(x, y, alpha=0.6, color=color)\n    axes[0, i].plot(x, y_pred, 'black', linewidth=2)\n    axes[0, i].set_title(titulo)\n    axes[0, i].set_xlabel('X')\n    axes[0, i].set_ylabel('Y')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Fila 2: Histograma de residuos\n    axes[1, i].hist(residuos, bins=25, density=True, alpha=0.7, color=color, edgecolor='black')\n    \n    # Superponer distribución normal teórica\n    x_norm = np.linspace(residuos.min(), residuos.max(), 100)\n    y_norm = stats.norm.pdf(x_norm, residuos.mean(), residuos.std())\n    axes[1, i].plot(x_norm, y_norm, 'blue', linewidth=2, label='Normal teórica')\n    \n    axes[1, i].set_title(f'Histograma de Residuos')\n    axes[1, i].set_xlabel('Residuos')\n    axes[1, i].set_ylabel('Densidad')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n    \n    # Fila 3: Q-Q plot\n    stats.probplot(residuos, dist=\"norm\", plot=axes[2, i])\n    axes[2, i].set_title(f'Q-Q Plot')\n    axes[2, i].grid(True, alpha=0.3)\n    \n    # Calcular test de normalidad\n    shapiro_stat, shapiro_p = stats.shapiro(residuos)\n    axes[2, i].text(0.05, 0.95, f'Shapiro p={shapiro_p:.3f}', \n                    transform=axes[2, i].transAxes,\n                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 INTERPRETACIÓN:\")\nprint(\"✅ Q-Q plot lineal + Shapiro p &gt; 0.05 = Normalidad\")\nprint(\"❌ Q-Q plot no lineal + Shapiro p &lt; 0.05 = No normalidad\")\nprint(\"\\n🔍 PATRONES EN Q-Q PLOTS:\")\nprint(\"- Curva S: Asimetría\")\nprint(\"- Colas alejadas de la línea: Colas pesadas/ligeras\")\nprint(\"- Múltiples segmentos: Multimodalidad\")\n\n\n\n\n\n\n\n\n📊 INTERPRETACIÓN:\n✅ Q-Q plot lineal + Shapiro p &gt; 0.05 = Normalidad\n❌ Q-Q plot no lineal + Shapiro p &lt; 0.05 = No normalidad\n\n🔍 PATRONES EN Q-Q PLOTS:\n- Curva S: Asimetría\n- Colas alejadas de la línea: Colas pesadas/ligeras\n- Múltiples segmentos: Multimodalidad\n\n\n\nResumen de Pruebas Estadísticas para Normalidad\n\nfrom scipy.stats import jarque_bera, anderson\n\nprint(\"🧪 PRUEBAS DE NORMALIDAD EN RESIDUOS\\n\")\nprint(\"H₀: Los residuos siguen distribución normal\")\nprint(\"H₁: Los residuos NO siguen distribución normal\\n\")\nprint(f\"{'Tipo':15} | {'Shapiro':&gt;10} | {'Jarque-Bera':&gt;12} | {'Anderson':&gt;10} | {'Conclusión'}\")\nprint(\"-\" * 75)\n\nfor tipo in tipos_norm:\n    x, y, _, titulo = crear_datos_no_normales(tipo=tipo)\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    residuos = y - lr.predict(x.reshape(-1, 1))\n    \n    # Shapiro-Wilk test\n    shapiro_stat, shapiro_p = stats.shapiro(residuos)\n    \n    # Jarque-Bera test\n    jb_stat, jb_p = jarque_bera(residuos)\n    \n    # Anderson-Darling test\n    ad_stat, ad_critical, ad_significance = anderson(residuos, dist='norm')\n    ad_result = \"Rechaza\" if ad_stat &gt; ad_critical[2] else \"No rechaza\"  # 5% nivel\n    \n    # Conclusión general\n    tests_reject = sum([shapiro_p &lt; 0.05, jb_p &lt; 0.05, ad_stat &gt; ad_critical[2]])\n    \n    if tests_reject &gt;= 2:\n        conclusion = \"❌ No Normal\"\n    elif tests_reject == 1:\n        conclusion = \"⚠️  Dudoso\"\n    else:\n        conclusion = \"✅ Normal\"\n    \n    print(f\"{tipo:15} | {shapiro_p:10.4f} | {jb_p:12.4f} | {ad_result:&gt;10} | {conclusion}\")\n\nprint(\"\\n💡 Interpretación: p-value &lt; 0.05 indica violación de normalidad\")\nprint(\"💡 Anderson-Darling: Estadístico &gt; valor crítico indica no normalidad\")\n\n🧪 PRUEBAS DE NORMALIDAD EN RESIDUOS\n\nH₀: Los residuos siguen distribución normal\nH₁: Los residuos NO siguen distribución normal\n\nTipo            |    Shapiro |  Jarque-Bera |   Anderson | Conclusión\n---------------------------------------------------------------------------\nnormal          |     0.7354 |       0.7293 | No rechaza | ✅ Normal\nasimetrico      |     0.0000 |       0.0000 |    Rechaza | ❌ No Normal\ncolas_pesadas   |     0.0003 |       0.0000 |    Rechaza | ❌ No Normal\nbimodal         |     0.0000 |       0.0010 |    Rechaza | ❌ No Normal\n\n💡 Interpretación: p-value &lt; 0.05 indica violación de normalidad\n💡 Anderson-Darling: Estadístico &gt; valor crítico indica no normalidad",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#consecuencias-de-violar-los-supuestos",
    "href": "violaciones_supuestos_regresion.html#consecuencias-de-violar-los-supuestos",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "5. Consecuencias de Violar los Supuestos",
    "text": "5. Consecuencias de Violar los Supuestos\nVamos a demostrar qué pasa cuando violamos cada supuesto en términos de: - Precisión de las predicciones - Validez de los intervalos de confianza - Confiabilidad de las pruebas de hipótesis\n\ndef simular_consecuencias(n_simulaciones=1000, n_datos=100):\n    \"\"\"Simula las consecuencias de violar supuestos\"\"\"\n    \n    resultados = {\n        'correcto': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'no_lineal': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'heteroscedastico': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'no_normal': {'mse': [], 'coef_estimados': [], 'p_values': []}\n    }\n    \n    coef_verdadero = 1.5  # Coeficiente real\n    \n    for _ in range(n_simulaciones):\n        x = np.linspace(0, 10, n_datos)\n        X = x.reshape(-1, 1)\n        \n        # Caso correcto\n        y_correcto = 2 + coef_verdadero * x + np.random.normal(0, 1, n_datos)\n        lr_correcto = LinearRegression().fit(X, y_correcto)\n        \n        # Caso no lineal (usando modelo lineal en datos cuadráticos)\n        y_no_lineal = 2 + 0.5 * x**2 + np.random.normal(0, 1, n_datos)\n        lr_no_lineal = LinearRegression().fit(X, y_no_lineal)\n        \n        # Caso heteroscedástico\n        y_hetero = 2 + coef_verdadero * x + np.random.normal(0, 0.1 + 0.2 * x, n_datos)\n        lr_hetero = LinearRegression().fit(X, y_hetero)\n        \n        # Caso no normal (distribución t)\n        y_no_normal = 2 + coef_verdadero * x + stats.t.rvs(df=3, scale=1, size=n_datos)\n        lr_no_normal = LinearRegression().fit(X, y_no_normal)\n        \n        # Calcular métricas para datos de prueba\n        x_test = np.linspace(0, 10, 50).reshape(-1, 1)\n        y_test_verdadero = 2 + coef_verdadero * x_test.flatten()\n        \n        # MSE en datos de prueba\n        resultados['correcto']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_correcto.predict(x_test))\n        )\n        resultados['no_lineal']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_no_lineal.predict(x_test))\n        )\n        resultados['heteroscedastico']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_hetero.predict(x_test))\n        )\n        resultados['no_normal']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_no_normal.predict(x_test))\n        )\n        \n        # Coeficientes estimados\n        resultados['correcto']['coef_estimados'].append(lr_correcto.coef_[0])\n        resultados['no_lineal']['coef_estimados'].append(lr_no_lineal.coef_[0])\n        resultados['heteroscedastico']['coef_estimados'].append(lr_hetero.coef_[0])\n        resultados['no_normal']['coef_estimados'].append(lr_no_normal.coef_[0])\n    \n    return resultados\n\nprint(\"🔄 Ejecutando simulación (esto puede tomar unos segundos...)\")\nresultados_sim = simular_consecuencias(n_simulaciones=500)\n\n# Crear gráficos de resultados\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Consecuencias de Violar Supuestos de Regresión', fontsize=16, fontweight='bold')\n\ntipos_casos = ['correcto', 'no_lineal', 'heteroscedastico', 'no_normal']\ncolores = ['green', 'red', 'orange', 'purple']\nnombres = ['Correcto ✅', 'No Lineal ❌', 'Heteroscedástico ❌', 'No Normal ❌']\n\n# MSE Distribución\nmse_data = [resultados_sim[caso]['mse'] for caso in tipos_casos]\naxes[0, 0].boxplot(mse_data, labels=nombres)\naxes[0, 0].set_title('Distribución del Error de Predicción (MSE)')\naxes[0, 0].set_ylabel('MSE')\naxes[0, 0].tick_params(axis='x', rotation=45)\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribución de coeficientes estimados\ncoef_data = [resultados_sim[caso]['coef_estimados'] for caso in tipos_casos]\naxes[0, 1].boxplot(coef_data, labels=nombres)\naxes[0, 1].axhline(y=1.5, color='black', linestyle='--', label='Valor verdadero')\naxes[0, 1].set_title('Distribución de Coeficientes Estimados')\naxes[0, 1].set_ylabel('Coeficiente β₁')\naxes[0, 1].tick_params(axis='x', rotation=45)\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Histograma comparativo de MSE\nfor i, (caso, color, nombre) in enumerate(zip(tipos_casos, colores, nombres)):\n    axes[1, 0].hist(resultados_sim[caso]['mse'], bins=30, alpha=0.6, \n                    color=color, label=nombre, density=True)\naxes[1, 0].set_title('Comparación de Distribuciones de MSE')\naxes[1, 0].set_xlabel('MSE')\naxes[1, 0].set_ylabel('Densidad')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Estadísticas resumidas\naxes[1, 1].axis('off')\ntabla_texto = \"RESUMEN ESTADÍSTICO:\\n\\n\"\ntabla_texto += f\"{'Caso':15} | {'MSE Promedio':&gt;12} | {'Sesgo β₁':&gt;10}\\n\"\ntabla_texto += \"-\" * 45 + \"\\n\"\n\nfor caso, nombre in zip(tipos_casos, nombres):\n    mse_prom = np.mean(resultados_sim[caso]['mse'])\n    sesgo = np.mean(resultados_sim[caso]['coef_estimados']) - 1.5\n    tabla_texto += f\"{nombre:15} | {mse_prom:12.4f} | {sesgo:10.4f}\\n\"\n\naxes[1, 1].text(0.1, 0.9, tabla_texto, transform=axes[1, 1].transAxes, \n                fontfamily='monospace', fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 CONCLUSIONES:\")\nprint(\"✅ Supuestos correctos: Estimaciones insesgadas y predicciones precisas\")\nprint(\"❌ No linealidad: Mayor error de predicción y estimaciones sesgadas\")\nprint(\"❌ Heterocedasticidad: Intervalos de confianza incorrectos\")\nprint(\"❌ No normalidad: Pruebas de hipótesis no confiables\")\n\n🔄 Ejecutando simulación (esto puede tomar unos segundos...)\n\n\n\n\n\n\n\n\n\n\n📊 CONCLUSIONES:\n✅ Supuestos correctos: Estimaciones insesgadas y predicciones precisas\n❌ No linealidad: Mayor error de predicción y estimaciones sesgadas\n❌ Heterocedasticidad: Intervalos de confianza incorrectos\n❌ No normalidad: Pruebas de hipótesis no confiables",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#ejercicios-prácticos",
    "href": "violaciones_supuestos_regresion.html#ejercicios-prácticos",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "6. Ejercicios Prácticos",
    "text": "6. Ejercicios Prácticos\n\nEjercicio 1: Identificación de Violaciones\nExamina los siguientes conjuntos de datos y determina qué supuestos se violan:\n\n# Generar datos de ejercicio\nnp.random.seed(123)\n\ndef generar_datos_ejercicio(tipo, n=150):\n    x = np.linspace(0, 10, n)\n    \n    if tipo == 'A':\n        # Múltiples violaciones\n        y = 2 + 0.5 * x**1.5 + np.random.normal(0, 0.1 + 0.3 * x, n)\n    elif tipo == 'B':\n        # Solo heterocedasticidad\n        y = 1 + 2 * x + np.random.normal(0, 0.5 * np.sqrt(x + 1), n)\n    elif tipo == 'C':\n        # Solo no normalidad\n        y = 3 + 1.5 * x + stats.chi2.rvs(df=2, size=n) - 2\n    elif tipo == 'D':\n        # Datos correctos\n        y = 1 + 2 * x + np.random.normal(0, 1.5, n)\n    \n    return x, y\n\n# Crear ejercicios\nfig, axes = plt.subplots(4, 3, figsize=(18, 20))\nfig.suptitle('EJERCICIO: Identifica las Violaciones de Supuestos', fontsize=16, fontweight='bold')\n\ntipos_ejercicio = ['A', 'B', 'C', 'D']\n\nfor i, tipo in enumerate(tipos_ejercicio):\n    x, y = generar_datos_ejercicio(tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    # Columna 1: Datos y modelo\n    axes[i, 0].scatter(x, y, alpha=0.6)\n    axes[i, 0].plot(x, y_pred, 'r-', linewidth=2)\n    axes[i, 0].set_title(f'Conjunto {tipo}: Datos y Modelo')\n    axes[i, 0].set_xlabel('X')\n    axes[i, 0].set_ylabel('Y')\n    axes[i, 0].grid(True, alpha=0.3)\n    \n    # Columna 2: Residuos vs Predichos\n    axes[i, 1].scatter(y_pred, residuos, alpha=0.6)\n    axes[i, 1].axhline(y=0, color='red', linestyle='--')\n    axes[i, 1].set_title(f'Conjunto {tipo}: Residuos vs Predichos')\n    axes[i, 1].set_xlabel('Valores Predichos')\n    axes[i, 1].set_ylabel('Residuos')\n    axes[i, 1].grid(True, alpha=0.3)\n    \n    # Columna 3: Q-Q Plot\n    stats.probplot(residuos, dist=\"norm\", plot=axes[i, 2])\n    axes[i, 2].set_title(f'Conjunto {tipo}: Q-Q Plot')\n    axes[i, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"🤔 PREGUNTA: ¿Qué supuestos se violan en cada conjunto?\")\nprint(\"\\n📝 Analiza cada fila:\")\nprint(\"- Conjunto A: ¿Qué patrones observas?\")\nprint(\"- Conjunto B: ¿Los residuos tienen varianza constante?\")\nprint(\"- Conjunto C: ¿Los residuos siguen distribución normal?\")\nprint(\"- Conjunto D: ¿Este conjunto cumple todos los supuestos?\")\nprint(\"\\n💡 Pista: Examina los patrones en residuos y la forma del Q-Q plot\")\n\n\n\n\n\n\n\n\n🤔 PREGUNTA: ¿Qué supuestos se violan en cada conjunto?\n\n📝 Analiza cada fila:\n- Conjunto A: ¿Qué patrones observas?\n- Conjunto B: ¿Los residuos tienen varianza constante?\n- Conjunto C: ¿Los residuos siguen distribución normal?\n- Conjunto D: ¿Este conjunto cumple todos los supuestos?\n\n💡 Pista: Examina los patrones en residuos y la forma del Q-Q plot",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#resumen-y-recomendaciones",
    "href": "violaciones_supuestos_regresion.html#resumen-y-recomendaciones",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "7. Resumen y Recomendaciones",
    "text": "7. Resumen y Recomendaciones\n\n¿Cómo diagnosticar violaciones?\n\n\n\n\n\n\n\n\nSupuesto\nHerramientas de Diagnóstico\nQué Buscar\n\n\n\n\nLinealidad\nResiduos vs PredichosGráficos parciales\nPatrones curvosTendencias sistemáticas\n\n\nIndependencia\nResiduos vs TiempoAutocorrelación\nPatrones temporalesCorrelación serial\n\n\nHomocedasticidad\nResiduos vs PredichosTest Breusch-Pagan\nForma de embudop-value &lt; 0.05\n\n\nNormalidad\nQ-Q PlotTest Shapiro-Wilk\nDesviación de línea rectap-value &lt; 0.05\n\n\n\n\n\n¿Qué hacer cuando se violan?\n\n\n\n\n\n\n\nViolación\nSoluciones Posibles\n\n\n\n\nNo Linealidad\n• Transformaciones (log, cuadrática)• Modelos no lineales• Splines, polinomios\n\n\nDependencia\n• Modelos de series temporales• Errores estándar robustos• GLS con estructura de correlación\n\n\nHeterocedasticidad\n• Transformaciones (log Y)• Errores estándar robustos• Mínimos cuadrados ponderados\n\n\nNo Normalidad\n• Transformaciones• Métodos no paramétricos• Modelos robustos\n\n\n\n\n\n🎯 Puntos Clave para Recordar\n\nLos gráficos de residuos son tu mejor amigo para diagnosticar problemas\nLas violaciones no siempre son fatales - depende de tu objetivo\nPara predicción: La no normalidad es menos crítica\nPara inferencia: Todos los supuestos son importantes\nSiempre visualiza antes de modelar y después de ajustar\n\n¡Felicidades! 🎉 Ahora tienes las herramientas para identificar y entender las violaciones de los supuestos de regresión lineal.",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  }
]