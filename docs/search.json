[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Miner√≠a de Datos",
    "section": "",
    "text": "Temario\n\nIntroducci√≥n al aprendizaje de m√°quina\nPrincipios de aprendizaje supervisado\nRegresi√≥n lineal\nM√©todos de remuestreo y validaci√≥n cruzada\nPrincipios de Regularizaci√≥n\nProblemas de clasificaci√≥n, m√©tricas y evaluaci√≥n\n√Årboles, bosques aleatorios y boosting\nRedes neuronales\nM√©todos no supervisados\n\n\nEvaluaci√≥n\n\nTarea (20%)\nExamen parcial (20%)\nProyecto final (30%):\n\nEntrega (75%)\nExposici√≥n (25%)\n\n\nExamen final (30%)\n\nExistir√° una parte extra a los alumnos que contribuyan al aprendizaje de sus compa√±eros:\n\nContribuciones al repositorio: a√±adiendo redacci√≥n m√°s entendible, a√±adiendo ejemplos particulares a sus carreras, etc.\nActividad en el canal de Slack: contestando dudas de sus compa√±eros, iniciando discusiones para resolver problemas.\n\n\n\nProfesor\nNombre: Sa√∫l Caballero Ram√≠rez\nCorreo: saul.caballero.ramirez@gmail.com\nCorreo alternativo: saul@nixtla.io\nEl canal m√°s r√°pido y efectivo ser√° el siguiente canal de Slack. La idea de este canal es que puedan comunicarse entre ustedes para ayudarse a aprender y si necesitan de mi ayuda intentar√© contestar en un periodo corto de tiempo. Cualquier comportamiento inadecuado dentro de este foro ser√° penalizado por las reglas de convivencia del ITAM.\n\n\nReferencias principales\n\nAn Introduction to Statistical Learning, James et¬†al. (2023)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, y Jonathan Taylor. 2023. An Introduction to Statistical Learning: With Applications in Python. Springer Texts en Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Temario"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "Introducci√≥n",
    "section": "",
    "text": "¬øQu√© es aprendizaje de m√°quina?\nM√©todos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempe√±o en alguna tarea o toma de decisi√≥n.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe tambi√©n aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisi√≥n afecta directa e inmediatamente al entorno.\nLas tareas m√°s apropiadas para este enfoque, en general, son aquellas en donde:",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#qu√©-es-aprendizaje-de-m√°quina",
    "href": "01-introduccion.html#qu√©-es-aprendizaje-de-m√°quina",
    "title": "Introducci√≥n",
    "section": "",
    "text": "Existe una cantidad considerable de datos relevantes para aprender a ejecutar la tarea.\nEl costo por errores al ejecutar la tarea es relativamente bajo (al menos comparado con alternativas).\nLa tarea se repite de manera m√°s o menos homog√©nea una cantidad grande de veces.\n\n\nEjemplos de tareas de aprendizaje:\n\nPredecir si un cliente de tarjeta de cr√©dito va a caer en impago en los pr√≥ximos doce meses.\nEstimar el ingreso mensual de un hogar a partir de las caracter√≠sticas de la vivienda, posesiones y equipamiento y localizaci√≥n geogr√°fica.\nDividir a los clientes de Netflix seg√∫n sus gustos.\nRecomendar art√≠culos a clientes de un programa de lealtad o servicio online.\nReconocer un tipos de documentos (identificaci√≥n, comprobante de domicilio, comprobante de ingresos) para acelerar el proceso de evaluaci√≥n de cr√©dito.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisi√©ramos obtener una respuesta barata, r√°pida, automatizada, y con suficiente precisi√≥n. Por ejemplo, reconocer caracteres en una placa de coche de una fotograf√≠a se puede hacer por personas, pero eso es lento y costoso. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisi√©ramos superar el desempe√±o actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisi√≥n de dar o no un pr√©stamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender m√°s del problema que nos interesa: estas soluciones forman parte de un ciclo de an√°lisis de datos donde podemos aprender de una forma m√°s concentrada cu√°les son caracter√≠sticas y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen est√°n vac√≠os, entonces es un cero, si el cr√©dito total es mayor al 50% del ingreso anual, declinar el pr√©stamo, etc). Las razones para no tomar un enfoque de reglas construidas ‚Äúa mano‚Äù:\n\nCuando conjuntos de reglas creadas a mano se desempe√±an mal (por ejemplo, para otorgar cr√©ditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser dif√≠ciles de mantener (por ejemplo, un corrector ortogr√°fico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¬øqu√© b√∫squedas www se enfocan en dar direcciones como resultados? ¬øc√≥mo filtrar comentarios no aceptables en foros?\nFinalmente, notamos que en estos problemas nuestro inter√©s principal no es entender qu√© variables influyen en otras (en el proceso natural o de negocio). Sin m√°s teor√≠a o dise√±o de datos, los m√©todos que utilizaremos explotan patrones en los datos que no necesariamente explican c√≥mo funcionan los sistemas de inter√©s.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "Introducci√≥n",
    "section": "Aprendizaje supervisado y no supervisado",
    "text": "Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fen√≥meno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de cr√©dito, utilizamos datos hist√≥ricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresi√≥n: cuando la salida es una variable num√©rica. El ejemplo de estimaci√≥n de ingreso es un problema de regresi√≥n\nProblemas de clasificaci√≥n: cuando la salida es una variable categ√≥rica. El ejemplo de detecci√≥n de d√≠gitos escritos a manos es un problema de clasificaci√≥n.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos m√°s vagos, y por lo mismo pueden ser m√°s dif√≠ciles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "02-principios.html",
    "href": "02-principios.html",
    "title": "Principios de aprendizaje supervisado",
    "section": "",
    "text": "Definici√≥n de aprendizaje supervisado\nSupongamos que observamos una variable cuantitativa \\(Y \\in \\mathbb{R}\\) y tenemos \\(p\\) variables predictoras, \\(X_1, X_2, ..., X_p\\), las cuales denotaremos como \\(X = (X_1, X_2, ..., X_p)\\). Supongamos que existe alguna reluaci√≥n entre ellas y se puede expresar de la siguiente forma:\n\\[ Y = f(x) + \\epsilon\\]\nLa tarea del aprendizaje supervisado es aprender la funci√≥n \\(f\\). Existen dos razones por las cuales estimar \\(f\\): predicci√≥n e inferencia.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#definici√≥n-de-aprendizaje-supervisado",
    "href": "02-principios.html#definici√≥n-de-aprendizaje-supervisado",
    "title": "Principios de aprendizaje supervisado",
    "section": "",
    "text": "Funci√≥n \\(f\\): funci√≥n desconocida que relaciona a \\(X\\) con \\(Y\\). Representa la informaci√≥n sist√©mica que \\(X\\) aporta a \\(Y\\).\nError \\(\\epsilon\\): representa qu√© tan equivocados estamos con respecto al verdadero valor de \\(Y\\).\n\n\n\nPredicci√≥n\nEn muchas ocasiones existen un conjunto de variables \\(X\\) que est√°n listas para aprovecharse, sin embargo, puede que no se pueda obtener la variable \\(Y\\) de manera inmediata. En este sentido, podemos predecir la variable \\(Y\\) siguiendo la ecuaci√≥n:\n\\[\\hat{Y} = \\hat{f}(X)\\]\ndonde \\(\\hat{f}\\) representa nuestro estimador de \\(f\\) y \\(\\hat{Y}\\) es nuestra predicci√≥n de \\(Y\\). En este sentido \\(\\hat{f}\\) es una caja negra en el sentido en el que no nos preocupa cu√°l es la funci√≥n, sino que provee predicciones precisas para \\(Y\\).\nLa precisi√≥n de \\(\\hat{Y}\\) depende de dos cantidades:\n\nError reducible: En general, \\(\\hat{f}\\) no ser√° un estimador perfecto de \\(f\\) y esto introducir√° un error el cu√°l puede reducirse. Ejemplos: Introducir una estructura lineal cu√°ndo el problema tiene estructura cuadr√°tica, falta de variables explicativas, exceso de variables que no contribuyen a la predicci√≥n.\nError ireducible: La variable \\(Y\\) es una funci√≥n tambi√©n de \\(\\epsilon\\) y por definici√≥n nuestra predicci√≥n tendra un error inherente. Ejemplos: Predecir que comer√°n ma√±ana, determinar si llover√° o no, determinar cu√°ndo ocurrir√° un temblor, ¬øqui√©n ganar√° una elecci√≥n?.\n\n\\[ \\begin{align*}\n\\mathbb{E}[(Y-\\hat{Y})^2] &= \\mathbb{E}[(f(X) + \\epsilon -\\hat{f}(x))^2]\\\\\n&= \\underset{Reducible}{\\underbrace{\\mathbb{E}[(f(X) - \\hat{f}(x))^2]}} + \\underset{Irreducible}{\\underbrace{\\text{Var}(\\epsilon)}}\n\\end{align*}\\]\nEl objetivo del curso se enfoca en t√©cnicas para estimar \\(f\\) con el objectivo de minimizar el error reducible. Es importante tener en cuenta que el error irreducible siempre nos pondr√° una cota en la predicci√≥n de \\(Y\\).\n\n\nInferencia\nExisten problemas en donde nos interesa m√°s entender la relaci√≥n intrinseca que existe entre \\(Y\\) y \\(X\\). En esta situaci√≥n nuestro objetivo no es hacer predicci√≥n, entonces \\(\\hat{f}\\) ya no puede ser tratada como una caja negra. En este tipo de enfoque se contestan preguntas c√≥mo:\n\n¬øCu√°les son los predictores que se asocian con la variable \\(Y\\)?: Muchas veces solo un subconjunto de los datos \\(X\\) son los que realmente est√°n relacionados con \\(Y\\).\n¬øCu√°l es la relaci√≥n entre \\(Y\\) y \\(X_i\\)?\n¬øLa relaci√≥n entre \\(Y\\) y \\(X_i\\) es lineal o m√°s compleja?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#c√≥mo-estimar-f",
    "href": "02-principios.html#c√≥mo-estimar-f",
    "title": "Principios de aprendizaje supervisado",
    "section": "¬øC√≥mo estimar \\(f\\)?",
    "text": "¬øC√≥mo estimar \\(f\\)?\nAsumiremos que tenemos \\(n\\) datos diferentes estas observaciones ser√°n llamadas conjunto de entrenamiento. \\(x_{ij}\\) representa el valor del predictor \\(j\\) para la observaci√≥n \\(i\\), donde \\(i=1,2,...,n\\) y \\(j=1,2,...,p\\). \\(y_i\\) representa la variable respuesta de la observaci√≥n \\(i\\). Entonces nuestro conjunto de entrenamiento consiste en:\n\\[{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}\\]\ndonde \\(x_i=(x_{i1}, x_{i2}, ..., x_{ip})^T\\).\nNuestro objetivo es aplicar un m√©todo de aprendizaje en el conjunto de datos para poder estimar una funci√≥n desconocida de \\(f\\). Nos encantar√≠a encontrar una funci√≥n \\(\\hat{f}\\) de forma tal que \\(Y\\simeq \\hat{f}(X)\\) para cualquier observaci√≥n \\((X, Y)\\). Muchos de estos enfoque se pueden caracterizar como m√©todos param√©tricos o no param√©tricos.\n\nM√©todos param√©tricos\nLos m√©todos param√©tricos involucran un enfoque de dos pasos:\n\nHacemos un supuesto de la forma funci√≥n de \\(f\\). Por ejemplo, la m√°s sencilla es que \\(f\\) es linear en \\(\\beta\\):\n\n\\[ f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]\nUna vez haciendo haciendo el supuesto de linealidad el problema de estimar \\(f\\) es simplificado ya que en lugar de explorar el espacio funcional uno solo necesita estimar \\(p+1\\) coeficientes \\(\\beta_0, ..., \\beta_p\\).\n\nNecesitamos un proceso que utilice los datos de entrenamiento para ajustar u entrenar el modelo. El enfoque m√°s sencillo es el m√©todo de m√≠nimos cuadrados ordinarios (OLS):\n\n\\[\\underset{\\beta_0, \\beta_1, ..., \\beta_p}{min} \\sum_{i=1}^{N}(y_i - (\\beta_0 + \\beta_1 x_{i1} +\\beta_2 x_{i2} + ... + \\beta_p X_p))^2\\]\nEl enfoque basado en modelado se refiere a los modelos param√©tricos; reduce el problema de estimar \\(f\\) a estimar un conjunto de par√°metros. La desventaja potencial es que el modelo podr√≠a no ser igual a la verdadera \\(f\\) y tendremos malas estimaciones del valor de \\(y\\).\n\n\nM√©todos no param√©tricos",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html",
    "href": "03-regresion_lineal.html",
    "title": "Regresi√≥n lineal",
    "section": "",
    "text": "Regresi√≥n Lineal Simple\nComenzaremos con el caso m√°s sencillo: predecir una variable de resultado Y a partir de una √∫nica variable predictora X.\nEl modelo matem√°tico que queremos ajustar es una l√≠nea recta:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nDonde:\nNuestro objetivo üéØ es encontrar los mejores valores posibles para los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) usando los datos que tenemos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresi√≥n-lineal-simple",
    "href": "03-regresion_lineal.html#regresi√≥n-lineal-simple",
    "title": "Regresi√≥n lineal",
    "section": "",
    "text": "\\(Y\\): La variable dependiente (lo que queremos predecir).\n\\(X\\): La variable independiente (nuestro predictor).\n\\(\\beta_0\\): El intercepto (el valor de \\(Y\\) cuando \\(X=0\\)).\n\\(\\beta_1\\): La pendiente (cu√°nto cambia \\(Y\\) por cada unidad que aumenta \\(X\\)).\n\\(\\epsilon\\): El t√©rmino de error (la parte de \\(Y\\) que nuestro modelo no puede explicar).\n\n\n\n¬øC√≥mo estimamos los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\)?\n‚ÄúMejor‚Äù para nosotros significa encontrar la l√≠nea que minimice la distancia vertical entre cada punto de dato y la propia l√≠nea. Espec√≠ficamente, minimizamos la Suma de los Errores al Cuadrado (SEC o Sum of Squared Errors, SSE).\nLa funci√≥n de costo (o p√©rdida) que queremos minimizar es:\n\\[J(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\\]\nTenemos dos m√©todos principales para encontrar los \\(\\beta\\) que minimizan esta funci√≥n:\n\nM√©todo 1: Las Ecuaciones Normales (La soluci√≥n anal√≠tica üß†)\nEste m√©todo utiliza c√°lculo para encontrar el m√≠nimo exacto de la funci√≥n de costo. Para ello, tomamos las derivadas parciales de \\(J\\) con respecto a \\(\\beta_0\\) y \\(\\beta_1\\), las igualamos a cero y resolvemos para los coeficientes.\n\n\n\n\n\n\nDerivada parcial con respecto a \\(\\beta_0\\):\n\n\n\n\n\n\\[\\frac{\\partial J}{\\partial \\beta_0} = \\sum_{i=1}^{n} -2(y_i - \\beta_0 - \\beta_1 x_i) = 0\\] \\[\\sum y_i - n\\beta_0 - \\beta_1 \\sum x_i = 0\\] \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\n\n\n\n\n\n\n\n\n\nDerivada parcial con respecto a \\(\\beta_1\\)\n\n\n\n\n\n\\[\\frac{\\partial J}{\\partial \\beta_1} = \\sum_{i=1}^{n} -2x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0\\] Sustituyendo \\(\\beta_0\\) de la primera ecuaci√≥n y resolviendo, llegamos a: \\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\n\n\n\nEstas f√≥rmulas nos dan los valores √≥ptimos y exactos de los coeficientes directamente a partir de los datos.\n\n\nM√©todo 2: Descenso en Gradiente (La soluci√≥n iterativa ‚öôÔ∏è)\nEste es un m√©todo computacional que nos ‚Äúacerca‚Äù progresivamente a la soluci√≥n. Es especialmente √∫til cuando tenemos una cantidad masiva de datos y calcular la soluci√≥n anal√≠tica es muy costoso.\nLa intuici√≥n: Imagina que est√°s en una monta√±a (la funci√≥n de costo) y quieres llegar al valle (el costo m√≠nimo). El Descenso en Gradiente te dice que mires a tu alrededor y des un paso en la direcci√≥n m√°s inclinada hacia abajo. Repites esto hasta llegar al fondo.\nEl algoritmo funciona as√≠:\n\nInicializa los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) con valores aleatorios (o en ceros).\nCalcula el gradiente de la funci√≥n de costo. El gradiente es un vector que apunta en la direcci√≥n del m√°ximo ascenso. Nosotros iremos en la direcci√≥n opuesta.\n\n\\(\\frac{\\partial J}{\\partial \\beta_0} = -2 \\sum (y_i - (\\beta_0 + \\beta_1 x_i))\\)\n\\(\\frac{\\partial J}{\\partial \\beta_1} = -2 \\sum x_i(y_i - (\\beta_0 + \\beta_1 x_i))\\)\n\nActualiza los coeficientes usando una tasa de aprendizaje (\\(\\alpha\\)), que controla el tama√±o del paso que damos.\n\n\\(\\beta_0 := \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0}\\)\n\\(\\beta_1 := \\beta_1 - \\alpha \\frac{\\partial J}{\\partial \\beta_1}\\)\n\nRepite los pasos 2 y 3 durante un n√∫mero determinado de iteraciones o hasta que el cambio en el costo sea muy peque√±o (convergencia).\n\n\n\n\n\n\n\nExplicacion visual",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#cu√°les-son-los-supuestos-de-la-regresi√≥n",
    "href": "03-regresion_lineal.html#cu√°les-son-los-supuestos-de-la-regresi√≥n",
    "title": "Regresi√≥n lineal",
    "section": "¬øCu√°les son los supuestos de la regresi√≥n? üßê",
    "text": "¬øCu√°les son los supuestos de la regresi√≥n? üßê\nPara que nuestro modelo sea confiable (es decir, para que los coeficientes y las predicciones tengan sentido), debemos cumplir con ciertos supuestos.\n\nLinealidad: La relaci√≥n entre \\(\\beta\\) y \\(Y\\) debe ser lineal.\n\n¬øPara qu√© sirve? Si la relaci√≥n no es lineal, nuestro modelo de l√≠nea recta ser√° intr√≠nsecamente incorrecto.\n\nIndependencia de los errores: Los errores (residuos) no deben estar correlacionados entre s√≠.\n\n¬øPara qu√© sirve? Es crucial para datos de series temporales. Si los errores est√°n correlacionados, la informaci√≥n de un error nos da pistas sobre el siguiente, lo cual viola la idea de que cada observaci√≥n es independiente.\n\nHomocedasticidad (Varianza constante de los errores): La varianza de los errores debe ser constante para todos los niveles de \\(X\\).\n\n¬øPara qu√© sirve? Si la varianza cambia (heterocedasticidad), nuestras predicciones ser√°n mejores para algunas partes de los datos que para otras, y los intervalos de confianza para los coeficientes ser√°n poco fiables. Visualmente, en un gr√°fico de residuos vs.¬†valores predichos, no queremos ver una forma de cono o embudo.\n\nNormalidad de los errores: Los errores deben seguir una distribuci√≥n normal con media cero.\n\n¬øPara qu√© sirve? Este supuesto es fundamental para poder realizar pruebas de hip√≥tesis sobre los coeficientes (como los p-values) y construir intervalos de confianza. Podemos verificarlo con un histograma de los residuos o un gr√°fico Q-Q.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#c√≥mo-evaluar-la-precisi√≥n-del-modelo",
    "href": "03-regresion_lineal.html#c√≥mo-evaluar-la-precisi√≥n-del-modelo",
    "title": "Regresi√≥n lineal",
    "section": "¬øC√≥mo evaluar la precisi√≥n del modelo? üìà",
    "text": "¬øC√≥mo evaluar la precisi√≥n del modelo? üìà\nUna vez que hemos ajustado el modelo, ¬øc√≥mo sabemos si es bueno?\n\nCoeficiente de Determinaci√≥n (\\(R^2\\))\nEl \\(R^2\\) mide la proporci√≥n de la varianza total en la variable dependiente (\\(Y\\)) que es explicada por nuestro modelo.\n\\[R^2 = 1 - \\frac{\\text{Suma de Errores al Cuadrado (SEC)}}{\\text{Suma Total de Cuadrados (STC)}} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\\]\n\n\\(R^2\\) var√≠a entre 0 y 1 (o 0% y 100%).\nUn \\(R^2\\) de 0.85 significa que el 85% de la variabilidad en \\(Y\\) puede ser explicada por \\(X\\).\nUn \\(R^2\\) m√°s alto generalmente indica un mejor ajuste del modelo.\n\n\n\np-values (Valores p)\nEl p-value nos ayuda a determinar si nuestra variable predictora \\(X\\) es estad√≠sticamente significativa. Responde a la pregunta: ¬øEs probable que la relaci√≥n que observamos entre \\(X\\) y \\(Y\\) haya ocurrido por puro azar?\n\nHip√≥tesis Nula (\\(H_0\\)): No hay relaci√≥n entre \\(X\\) y \\(Y\\) (es decir, \\(\\beta_1 = 0\\)).\nHip√≥tesis Alternativa (\\(H_a\\)): S√≠ hay una relaci√≥n entre \\(X\\) y \\(Y\\) (es decir, \\(\\beta_1 \\neq 0\\)).\n\nUn p-value peque√±o (t√≠picamente &lt; 0.05) nos da evidencia para rechazar la hip√≥tesis nula. Esto sugiere que nuestra variable \\(X\\) es un predictor √∫til para \\(Y\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#m√©tricas-de-error-de-predicci√≥n",
    "href": "03-regresion_lineal.html#m√©tricas-de-error-de-predicci√≥n",
    "title": "Regresi√≥n lineal",
    "section": "M√©tricas de Error de Predicci√≥n",
    "text": "M√©tricas de Error de Predicci√≥n\nAdem√°s del \\(R^2\\), existen m√∫ltiples m√©tricas para evaluar qu√© tan bien predice nuestro modelo. Cada una tiene sus ventajas y casos de uso espec√≠ficos:\n\nError Cuadr√°tico Medio (MSE)\nEl MSE mide el promedio de los errores al cuadrado:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\n\nVentajas: Penaliza fuertemente errores grandes, diferenciable (√∫til para optimizaci√≥n)\nDesventajas: Sensible a valores at√≠picos, dif√≠cil de interpretar (unidades al cuadrado)\nCu√°ndo usar: Cuando errores grandes son especialmente costosos\n\n\n\nRa√≠z del Error Cuadr√°tico Medio (RMSE)\nEl RMSE es la ra√≠z cuadrada del MSE:\n\\[RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]\n\nVentajas: Mismas unidades que la variable objetivo, interpretable\nDesventajas: A√∫n sensible a valores at√≠picos\nInterpretaci√≥n: ‚ÄúEn promedio, nuestras predicciones se desv√≠an X unidades del valor real‚Äù\n\n\n\nError Absoluto Medio (MAE)\nEl MAE mide el promedio de los errores absolutos:\n\\[MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\]\n\nVentajas: Robusto a valores at√≠picos, f√°cil de interpretar\nDesventajas: No diferenciable en cero, trata todos los errores por igual\nCu√°ndo usar: Cuando hay valores at√≠picos o todos los errores tienen igual importancia\n\n\n\nError Porcentual Absoluto Medio (MAPE)\nEl MAPE expresa el error como porcentaje del valor real:\n\\[MAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\\]\n\nVentajas: Interpretable (% de error), adimensional, √∫til para comparar modelos en diferentes escalas\nDesventajas: Indefinido cuando \\(y_i = 0\\), asim√©trico (penaliza m√°s las sobreestimaciones)\nInterpretaci√≥n: ‚ÄúNuestras predicciones se desv√≠an en promedio X% del valor real‚Äù\nCu√°ndo usar: Para comparar precisi√≥n entre diferentes productos, regiones, o escalas\n\n\n\nError Porcentual Absoluto Medio Sim√©trico (SMAPE)\nEl SMAPE es una versi√≥n sim√©trica del MAPE:\n\\[SMAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}\\]\n\nVentajas: Sim√©trico, acotado entre 0% y 200%\nDesventajas: Puede ser contraintuitivo, no tan est√°ndar como MAPE\nCu√°ndo usar: Cuando queremos evitar el sesgo del MAPE hacia sobreestimaciones\n\n\n\nError Logar√≠tmico Cuadr√°tico Medio (MSLE)\nEl MSLE usa transformaci√≥n logar√≠tmica:\n\\[MSLE = \\frac{1}{n} \\sum_{i=1}^{n} (\\log(1 + y_i) - \\log(1 + \\hat{y}_i))^2\\]\n\nVentajas: Penaliza m√°s las subestimaciones que las sobreestimaciones\nDesventajas: Solo para valores positivos, menos interpretable\nCu√°ndo usar: Cuando subestimar es m√°s costoso que sobreestimar (ej: demanda de inventario)\n\n\n\n\\(R^2\\) Ajustado\nEl \\(R^2\\) ajustado penaliza por el n√∫mero de variables en el modelo:\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\nDonde \\(p\\) es el n√∫mero de predictores.\n\nVentajas: No aumenta autom√°ticamente al a√±adir variables\nCu√°ndo usar: Para comparar modelos con diferente n√∫mero de variables\nInterpretaci√≥n: Similar a \\(R^2\\) pero m√°s conservador\n\n\n¬øCu√°l m√©trica elegir?\nLa elecci√≥n de m√©trica depende del contexto del problema:\n\n\n\n\n\n\n\n\nM√©trica\nMejor para\nEvitar cuando\n\n\n\n\nRMSE\nErrores grandes son costosos\nHay muchos valores at√≠picos\n\n\nMAE\nErrores tienen igual importancia\nNecesitas diferenciabilidad\n\n\nMAPE\nComparar diferentes escalas\nHay valores cercanos a cero\n\n\nSMAPE\nComparar con simetr√≠a\nInterpretaci√≥n debe ser simple\n\n\nR¬≤\nExplicar variabilidad\nSolo importa precisi√≥n de predicci√≥n\n\n\n\n\n\n\n\n\n\nRecomendaci√≥n pr√°ctica\n\n\n\nUsa m√∫ltiples m√©tricas para evaluar tu modelo. Una combinaci√≥n t√≠pica ser√≠a: - RMSE para precisi√≥n general - MAPE para interpretabilidad de negocio\n- R¬≤ para explicaci√≥n de variabilidad",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresi√≥n-lineal-m√∫ltiple",
    "href": "03-regresion_lineal.html#regresi√≥n-lineal-m√∫ltiple",
    "title": "Regresi√≥n lineal",
    "section": "Regresi√≥n Lineal M√∫ltiple",
    "text": "Regresi√≥n Lineal M√∫ltiple\nAhora, ¬øqu√© pasa si tenemos m√∫ltiples predictores (\\(X_1, X_2, ..., X_p\\))? El modelo se expande:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\\]\nLa intuici√≥n es la misma, pero en lugar de ajustar una l√≠nea, estamos ajustando un hiperplano en un espacio multidimensional.\nPara manejar esto de forma elegante, usamos notaci√≥n matricial:\n\\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\nDonde: - \\(\\mathbf{y}\\) es el vector de observaciones. - \\(\\mathbf{X}\\) es la matriz de dise√±o (con una primera columna de unos para el intercepto). - \\(\\boldsymbol{\\beta}\\) es el vector de coeficientes. - \\(\\boldsymbol{\\epsilon}\\) es el vector de errores.\nLa funci√≥n de costo en forma matricial es: \\[J(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#transformaciones-comunes-en-modelos-lineales",
    "href": "03-regresion_lineal.html#transformaciones-comunes-en-modelos-lineales",
    "title": "Regresi√≥n lineal",
    "section": "Transformaciones Comunes en Modelos Lineales",
    "text": "Transformaciones Comunes en Modelos Lineales\nA veces, la relaci√≥n entre X e Y no es estrictamente lineal. Las transformaciones logar√≠tmicas nos permiten modelar relaciones no lineales y, adem√°s, ofrecen interpretaciones muy √∫tiles en t√©rminos de cambios porcentuales.\n\nModelo Log-Nivel (Transformaci√≥n en Y)\nEste modelo se usa cuando el efecto de X sobre Y no es absoluto, sino porcentual. Por ejemplo, c√≥mo un a√±o m√°s de educaci√≥n afecta el porcentaje de aumento salarial.\n\nEcuaci√≥n: \\(\\ln(Y) = \\beta_0 + \\beta_1 X + \\epsilon\\)\nInterpretaci√≥n: Un incremento de una unidad en \\(X\\) est√° asociado con un cambio de \\((100 \\cdot \\beta_1)\\%\\) en \\(Y\\).\n\n\n\n\n\n\n\nExplicaci√≥n Matem√°tica de la Aproximaci√≥n\n\n\n\n\n\nLa clave est√° en la propiedad del logaritmo y el c√°lculo. La derivada de \\(\\ln(Y)\\) con respecto a \\(X\\) es \\(\\beta_1\\): \\[\\frac{d(\\ln(Y))}{dX} = \\beta_1\\] Sabemos que \\(d(\\ln(Y)) = \\frac{dY}{Y}\\). Por tanto: \\[\\frac{dY/Y}{dX} = \\beta_1\\] Para cambios peque√±os (o discretos, \\(\\Delta\\)), podemos aproximar los diferenciales: \\[\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X}\\] Si consideramos un cambio unitario en X, \\(\\Delta X = 1\\), entonces: \\[\\beta_1 \\approx \\frac{\\Delta Y}{Y}\\] Esto significa que \\(\\beta_1\\) es la aproximaci√≥n del cambio porcentual en \\(Y\\) ante un cambio de una unidad en \\(X\\).\n\n\n\n\n\nModelo Nivel-Log (Transformaci√≥n en X)\nEste modelo es √∫til cuando el efecto de X sobre Y se reduce a medida que X aumenta (rendimientos decrecientes). Por ejemplo, el efecto de a√±adir presupuesto de marketing sobre las ventas.\n\nEcuaci√≥n: \\(Y = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\)\nInterpretaci√≥n: Un incremento del 1% en \\(X\\) est√° asociado con un cambio de \\((\\beta_1 / 100)\\) unidades en \\(Y\\).\n\n\n\n\n\n\n\nExplicaci√≥n Matem√°tica de la Aproximaci√≥n\n\n\n\n\n\nTomamos la derivada de \\(Y\\) con respecto a \\(\\ln(X)\\): \\[\\frac{dY}{d(\\ln(X))} = \\beta_1\\] Usando la regla de la cadena, sabemos que \\(d(\\ln(X)) = \\frac{dX}{X}\\). Sustituyendo: \\[\\frac{dY}{dX/X} = \\beta_1 \\implies dY = \\beta_1 \\frac{dX}{X}\\] Para cambios discretos, aproximamos: \\[\\Delta Y \\approx \\beta_1 \\frac{\\Delta X}{X}\\] Si consideramos un cambio del 1% en X, entonces \\(\\frac{\\Delta X}{X} = 0.01\\). La ecuaci√≥n se convierte en: \\[\\Delta Y \\approx \\beta_1 (0.01) = \\frac{\\beta_1}{100}\\] Esto significa que un cambio del 1% en \\(X\\) provoca un cambio de \\(\\beta_1/100\\) unidades en \\(Y\\).\n\n\n\n\n\nModelo Log-Log (Transformaci√≥n en X e Y)\nEste modelo es muy com√∫n en econom√≠a y modela la elasticidad constante entre dos variables.\n\nEcuaci√≥n: \\(\\ln(Y) = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\)\nInterpretaci√≥n: Un incremento del 1% en \\(X\\) est√° asociado con un cambio del \\(\\beta_1\\%\\) en \\(Y\\).\n\n\n\n\n\n\n\nExplicaci√≥n Matem√°tica de la Aproximaci√≥n\n\n\n\n\n\nEste caso combina los dos anteriores. \\(\\beta_1\\) es la derivada de \\(\\ln(Y)\\) con respecto a \\(\\ln(X)\\), que es la definici√≥n de elasticidad. \\[\\beta_1 = \\frac{d(\\ln(Y))}{d(\\ln(X))}\\] Usando las propiedades del c√°lculo que vimos antes: \\[\\beta_1 = \\frac{dY/Y}{dX/X}\\] Aproximando para cambios discretos: \\[\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X / X}\\] Esta es la definici√≥n de elasticidad: el cambio porcentual en \\(Y\\) dividido por el cambio porcentual en \\(X\\). Por lo tanto, si \\(X\\) cambia en un 1% (\\(\\Delta X / X = 0.01\\)), el cambio porcentual en \\(Y\\) (\\(\\Delta Y / Y\\)) ser√° aproximadamente \\(\\beta_1 \\times 0.01\\), es decir, un \\(\\beta_1\\%\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresi√≥n-regularizada-penalizada",
    "href": "03-regresion_lineal.html#regresi√≥n-regularizada-penalizada",
    "title": "Regresi√≥n lineal",
    "section": "Regresi√≥n Regularizada (Penalizada) üéØ",
    "text": "Regresi√≥n Regularizada (Penalizada) üéØ\nHasta ahora hemos visto la regresi√≥n lineal cl√°sica, pero ¬øqu√© pasa cuando tenemos muchas variables o cuando nuestro modelo sufre de sobreajuste? Aqu√≠ es donde entran las t√©cnicas de regularizaci√≥n.\n\n¬øPor qu√© necesitamos regularizaci√≥n?\nLa regresi√≥n lineal ordinaria (OLS) puede presentar varios problemas:\n\nSobreajuste: Cuando tenemos muchas variables relativas al n√∫mero de observaciones\nMulticolinealidad: Variables predictoras altamente correlacionadas\nInestabilidad: Peque√±os cambios en los datos causan grandes cambios en los coeficientes\nInterpretabilidad: Demasiadas variables hacen dif√≠cil entender el modelo\n\nLa regularizaci√≥n a√±ade una penalizaci√≥n a la funci√≥n de costo para controlar la complejidad del modelo.\n\n\n\nRidge Regression (Regresi√≥n Ridge) üèîÔ∏è\nLa regresi√≥n Ridge a√±ade una penalizaci√≥n L2 (suma de cuadrados) a los coeficientes:\n\\[J_{Ridge}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\\]\nDonde: - \\(\\lambda &gt; 0\\) es el par√°metro de regularizaci√≥n - \\(\\sum_{j=1}^{p} \\beta_j^2\\) es la penalizaci√≥n L2\n\nCaracter√≠sticas de Ridge:\n‚úÖ Ventajas: - Reduce el sobreajuste - Maneja bien la multicolinealidad - Siempre tiene soluci√≥n √∫nica - Estabiliza los coeficientes\n‚ùå Desventajas: - NO elimina variables (coeficientes nunca son exactamente cero) - Dificulta la interpretabilidad - Requiere estandarizar las variables\n\n\nSoluci√≥n Anal√≠tica:\n\\[\\hat{\\boldsymbol{\\beta}}_{Ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nEl t√©rmino \\(\\lambda\\mathbf{I}\\) hace que la matriz sea invertible incluso con multicolinealidad.\n\n\n¬øC√≥mo elegir Œª?\n\nŒª = 0: Regresi√≥n ordinaria (sin penalizaci√≥n)\nŒª ‚Üí ‚àû: Todos los coeficientes ‚Üí 0\nŒª √≥ptimo: Se encuentra usando validaci√≥n cruzada\n\n\n\n\n\nLasso Regression (Least Absolute Shrinkage and Selection Operator) ‚úÇÔ∏è\nLa regresi√≥n Lasso usa penalizaci√≥n L1 (suma de valores absolutos):\n\\[J_{Lasso}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\\]\n\nCaracter√≠sticas de Lasso:\n‚úÖ Ventajas: - Selecci√≥n autom√°tica de variables (coeficientes = 0) - Modelos m√°s interpretables y simples - √ötil cuando muchas variables son irrelevantes\n‚ùå Desventajas: - Puede ser inestable con grupos de variables correlacionadas - Selecciona arbitrariamente entre variables correlacionadas - No tiene soluci√≥n anal√≠tica cerrada\n\n\nLa ‚ÄúMagia‚Äù de L1: ¬øPor qu√© produce ceros exactos?\nLa penalizaci√≥n L1 crea una regi√≥n factible con esquinas puntiagudas. La soluci√≥n √≥ptima tiende a ocurrir en estas esquinas, donde algunos coeficientes son exactamente cero.\n\n\n\n\n\n\nIntuici√≥n Geom√©trica\n\n\n\n\n\nImagina que est√°s minimizando una funci√≥n bajo la restricci√≥n de que \\(|\\beta_1| + |\\beta_2| \\leq t\\). Esta restricci√≥n forma un diamante en 2D. La funci√≥n objetivo forma elipses. La soluci√≥n est√° donde la elipse m√°s peque√±a toca el diamante, y esto frecuentemente ocurre en los v√©rtices (donde \\(\\beta_1 = 0\\) o \\(\\beta_2 = 0\\)).\n\n\n\n\n\n\n\nElastic Net: Lo Mejor de Ambos Mundos üï∏Ô∏è\nElastic Net combina las penalizaciones L1 y L2:\n\\[J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\\]\nO equivalentemente, con un par√°metro de mezcla \\(\\alpha\\):\n\\[J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\left[ \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right]\\]\nDonde: - \\(\\alpha \\in [0,1]\\) controla la mezcla entre L1 y L2 - \\(\\alpha = 0\\): Pure Ridge - \\(\\alpha = 1\\): Pure Lasso - \\(\\alpha = 0.5\\): Igual peso a ambas penalizaciones\n\nCaracter√≠sticas de Elastic Net:\n‚úÖ Ventajas: - Selecci√≥n de variables como Lasso - Estabilidad como Ridge - Maneja bien grupos de variables correlacionadas - M√°s flexible que Ridge o Lasso por separado\n‚ùå Desventajas: - Dos hiperpar√°metros para ajustar (\\(\\lambda\\) y \\(\\alpha\\)) - M√°s complejo computacionalmente\n\n\n\n\nComparaci√≥n Visual: Ridge vs Lasso vs Elastic Net\n\n\n\n\n\n\n\n\n\nAspecto\nRidge\nLasso\nElastic Net\n\n\n\n\nPenalizaci√≥n\nL2: \\(\\sum \\beta_j^2\\)\nL1: \\(\\sum |\\beta_j|\\)\nL1 + L2 combinadas\n\n\nSelecci√≥n de variables\n‚ùå No\n‚úÖ S√≠\n‚úÖ S√≠\n\n\nCoeficientes exactamente cero\n‚ùå No\n‚úÖ S√≠\n‚úÖ S√≠\n\n\nManejo de multicolinealidad\n‚úÖ Excelente\n‚ö†Ô∏è Problem√°tico\n‚úÖ Muy bueno\n\n\nEstabilidad\n‚úÖ Alta\n‚ö†Ô∏è Media\n‚úÖ Alta\n\n\nInterpretabilidad\n‚ö†Ô∏è Media\n‚úÖ Alta\n‚úÖ Alta\n\n\nCuando usar\nTodas las variables importan\nPocas variables importantes\nSituaciones mixtas\n\n\n\n\n\n¬øCu√°ndo usar cada m√©todo?\n\nUsa Ridge cuando:\n\nCrees que todas las variables contribuyen al modelo\nTienes multicolinealidad severa\nQuieres estabilizar coeficientes sin eliminar variables\nEl n√∫mero de observaciones es peque√±o relativo a variables\n\n\n\nUsa Lasso cuando:\n\nCrees que pocas variables son realmente importantes\nQuieres un modelo simple e interpretable\nNecesitas selecci√≥n autom√°tica de variables\nTienes muchas variables irrelevantes\n\n\n\nUsa Elastic Net cuando:\n\nNo est√°s seguro de cu√°ntas variables son importantes\nTienes grupos de variables correlacionadas\nQuieres balancear selecci√≥n y estabilidad\nEs tu primera opci√≥n cuando no conoces la estructura de los datos\n\n\n\n\n\nValidaci√≥n de Modelos y Selecci√≥n de Hiperpar√°metros\n\n¬øPor qu√© necesitamos dividir nuestros datos?\nCuando construimos modelos de machine learning, enfrentamos un dilema fundamental: ¬øc√≥mo sabemos si nuestro modelo funcionar√° bien con datos nuevos?\n\nEl Problema del Sobreajuste\nImagina que est√°s prepar√°ndote para un examen. Si solo estudias las preguntas exactas que aparecer√°n en el examen, podr√≠as obtener una calificaci√≥n perfecta. Pero si las preguntas cambian ligeramente, tu rendimiento se desplomar√≠a. Esto es sobreajuste: el modelo memoriza los datos de entrenamiento pero no generaliza.\n\n\n\nDivisi√≥n T√≠pica de Datos: Entrenamiento/Validaci√≥n/Prueba\nLa estrategia est√°ndar es dividir nuestros datos en tres conjuntos:\nüìä Dataset Completo (100%)\n‚îú‚îÄ‚îÄ üèãÔ∏è Entrenamiento (60%) - Para ajustar coeficientes\n‚îú‚îÄ‚îÄ üéØ Validaci√≥n (20%)     - Para seleccionar hiperpar√°metros  \n‚îî‚îÄ‚îÄ üß™ Prueba (20%)         - Para evaluaci√≥n final\n\nConjunto de Entrenamiento (60%)\n\nProp√≥sito: Ajustar los coeficientes \\(\\beta\\) del modelo\nAnalog√≠a: Los ejercicios que haces para aprender\n\n\n\nConjunto de Validaci√≥n (20%)\n\nProp√≥sito: Comparar diferentes hiperpar√°metros (como \\(\\lambda\\) en Ridge/Lasso)\nAnalog√≠a: Ex√°menes de pr√°ctica para decidir qu√© estrategia de estudio funciona mejor\n\n\n\nConjunto de Prueba (20%)\n\nProp√≥sito: Evaluaci√≥n final y honesta del modelo\nAnalog√≠a: El examen final real\n‚ö†Ô∏è Regla de Oro: ¬°Solo se usa UNA vez al final!\n\n\n\n\n¬øQu√© pasa si tenemos pocos datos?\nCuando nuestro dataset es peque√±o (&lt; 1000 observaciones), dividir en tres partes puede ser problem√°tico:\n‚ùå Problemas con datasets peque√±os: - Conjunto de entrenamiento muy peque√±o ‚Üí modelo pobre - Conjunto de validaci√≥n peque√±o ‚Üí selecci√≥n inestable de hiperpar√°metros - Conjunto de prueba peque√±o ‚Üí evaluaci√≥n poco confiable\nSoluci√≥n: ¬°Validaci√≥n Cruzada!\n\n\n\nValidaci√≥n Cruzada (Cross-Validation)\nLa validaci√≥n cruzada es una t√©cnica que maximiza el uso de nuestros datos limitados. En lugar de usar una sola divisi√≥n, usamos m√∫ltiples divisiones.\n\nValidaci√≥n Cruzada k-fold\nEl m√©todo m√°s com√∫n es k-fold cross-validation:\n\nDividir el dataset en \\(k\\) ‚Äúpliegues‚Äù (folds) de igual tama√±o\nRepetir \\(k\\) veces:\n\nUsar \\(k-1\\) pliegues para entrenamiento\nUsar 1 pliegue para validaci√≥n\n\nPromediar los resultados de las \\(k\\) evaluaciones\n\n\n\n\n\n\nVisualizaci√≥n de 5-Fold Cross Validation mostrando c√≥mo se dividen los datos en cada iteraci√≥n\n\n\n\n\n\n\nVentajas de la Validaci√≥n Cruzada\n‚úÖ Maximiza el uso de datos: Cada observaci√≥n se usa tanto para entrenamiento como validaci√≥n\n‚úÖ Estimaci√≥n m√°s robusta: Promedia m√∫ltiples evaluaciones independientes\n‚úÖ Reduce la varianza: Menos dependiente de una divisi√≥n particular\n‚úÖ Detecta inestabilidad: Si los resultados var√≠an mucho entre folds, el modelo es inestable\n\n\n\nValidaci√≥n Cruzada para Selecci√≥n de Hiperpar√°metros\nEn regresi√≥n regularizada, usamos CV para encontrar el mejor \\(\\lambda\\):\n\n\nüéØ SELECCI√ìN DE HIPERPAR√ÅMETROS CON VALIDACI√ìN CRUZADA\n============================================================\nPara cada valor de Œª:\n  1. Aplicar 5-fold CV\n  2. Calcular error promedio\n  3. Seleccionar Œª con menor error\n\n\n\n\n\nCurva de validaci√≥n mostrando c√≥mo seleccionar el hiperpar√°metro √≥ptimo Œª usando validaci√≥n cruzada\n\n\n\n\n\nüìà Resultado: Œª √≥ptimo = 0.1274\nüìâ Error de CV m√≠nimo = 0.4776\n\n\n\n\nProceso Completo de Validaci√≥n\nEl flujo completo para modelos regularizados es:\n1. üìä Dividir datos originales\n   ‚îî‚îÄ‚îÄ 80% para desarrollo (entrenamiento + validaci√≥n)\n   ‚îî‚îÄ‚îÄ 20% para prueba final (¬°NO TOCAR hasta el final!)\n\n2. üîÑ En el conjunto de desarrollo:\n   ‚îî‚îÄ‚îÄ Para cada Œª candidato:\n       ‚îú‚îÄ‚îÄ Aplicar k-fold CV\n       ‚îú‚îÄ‚îÄ Calcular error promedio\n       ‚îî‚îÄ‚îÄ Guardar resultado\n\n3. üéØ Seleccionar Œª con menor error de CV\n\n4. üèóÔ∏è Entrenar modelo final con Œª √≥ptimo en TODO el conjunto de desarrollo\n\n5. üß™ Evaluaci√≥n final en conjunto de prueba\n\n\nVariantes de Validaci√≥n Cruzada\n\nLeave-One-Out CV (LOOCV)\n\nk = n (n√∫mero de observaciones)\nVentaja: M√°ximo uso de datos para entrenamiento\nDesventaja: Computacionalmente costoso, alta varianza\n\n\n\nStratified CV\n\nPara problemas de clasificaci√≥n\nMantiene la proporci√≥n de clases en cada fold\n\n\n\nTime Series CV\n\nPara datos temporales\nRespeta el orden temporal (no mezcla futuro con pasado)\n\n\n\n\n\n\n\n‚ö†Ô∏è Errores Comunes\n\n\n\n\nData Leakage: Usar informaci√≥n del conjunto de prueba durante el desarrollo\nM√∫ltiples evaluaciones: Evaluar repetidamente en el conjunto de prueba\nSelecci√≥n de modelo sesgada: Elegir el modelo bas√°ndose en el conjunto de prueba\nCV incorrecto: Aplicar transformaciones antes de la divisi√≥n de CV\n\n\n\n\n\n\n¬øCu√°ndo usar cada enfoque?\n\n\n\n\n\n\n\n\nTama√±o del Dataset\nEnfoque Recomendado\nRaz√≥n\n\n\n\n\nGrande (&gt;10,000)\nTrain/Validation/Test\nSuficientes datos para divisi√≥n estable\n\n\nMediano (1,000-10,000)\nTrain/Test + CV\nCV para hiperpar√°metros, test para evaluaci√≥n final\n\n\nPeque√±o (&lt;1,000)\nSolo CV (sin test separado)\nMaximizar datos disponibles\n\n\nMuy peque√±o (&lt;100)\nLOOCV o Bootstrap\nCada observaci√≥n es valiosa\n\n\n\n\n\n\n\n\n\nConsejo Pr√°ctico\n\n\n\nEmpieza siempre con Elastic Net con \\(\\alpha = 0.5\\). Si el modelo selecciona muchas variables, prueba valores de \\(\\alpha\\) m√°s cercanos a 1 (m√°s Lasso). Si elimina variables importantes, prueba valores cercanos a 0 (m√°s Ridge).\n\n\n\n\n\n\nEjercicio Pr√°ctico: Comparando los Tres M√©todos\nEn el notebook correspondiente, implementaremos:\n\nGeneraci√≥n de datos con diferentes estructuras de correlaci√≥n\nComparaci√≥n visual de los caminos de regularizaci√≥n\nValidaci√≥n cruzada para selecci√≥n de hiperpar√°metros\nEvaluaci√≥n del rendimiento en datos de prueba\nInterpretaci√≥n de los coeficientes seleccionados\n\nPregunta de reflexi√≥n: ¬øEn qu√© situaciones esperar√≠as que Ridge supere a Lasso, y viceversa?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and\nJonathan Taylor. 2023. An Introduction to\nStatistical Learning: With Applications in Python. Springer\nTexts in Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Referencias"
    ]
  },
  {
    "objectID": "introduccion.html",
    "href": "introduccion.html",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "",
    "text": "1. Librer√≠as Necesarias\nEste notebook cubre los fundamentos de Python necesarios para miner√≠a de datos, incluyendo:\nPrimero importamos todas las librer√≠as que usaremos en este notebook:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#fundamentos-de-numpy",
    "href": "introduccion.html#fundamentos-de-numpy",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "2. Fundamentos de NumPy",
    "text": "2. Fundamentos de NumPy\nNumPy es la librer√≠a fundamental para computaci√≥n cient√≠fica en Python. Proporciona arrays multidimensionales eficientes.\n\n2.1 Creaci√≥n de Arrays\n\n# Creamos un array de 100 n√∫meros igualmente espaciados entre 0 y 10\nX = np.linspace(0, 10, 100)\nprint(\"Primeros 5 elementos:\", X[:5])\nprint(\"Forma del array:\", X.shape)\nprint(\"√öltimos 10 elementos:\", X[90:])\n\nPrimeros 5 elementos: [0.        0.1010101 0.2020202 0.3030303 0.4040404]\nForma del array: (100,)\n√öltimos 10 elementos: [ 9.09090909  9.19191919  9.29292929  9.39393939  9.49494949  9.5959596\n  9.6969697   9.7979798   9.8989899  10.        ]\n\n\n\n\n2.2 Operaciones con Arrays y Generaci√≥n de Datos\n\n# Acceso a elementos individuales\nprint(\"Elemento en posici√≥n 1:\", X[1])\n\n# Generaci√≥n de datos sint√©ticos para regresi√≥n lineal\n# y = 0.5*x + 10 + ruido_gaussiano\ny_linear = 0.5 * X + 10 + np.random.normal(loc=0, scale=1.5, size=100)\nprint(\"Primeros 5 valores de y:\", y_linear[:5])\n\nElemento en posici√≥n 1: 0.10101010101010101\nPrimeros 5 valores de y: [10.947363    7.72186528 10.63466357 10.19993526 11.22329185]",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#fundamentos-de-pandas",
    "href": "introduccion.html#fundamentos-de-pandas",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "3. Fundamentos de Pandas",
    "text": "3. Fundamentos de Pandas\nPandas es la librer√≠a principal para manipulaci√≥n y an√°lisis de datos estructurados en Python.\n\n3.1 Creaci√≥n de DataFrames\n\n# Creamos un DataFrame con nuestros datos\ndf = pd.DataFrame({\"x\": X, \"y\": y_linear})\nprint(\"Primeras 5 filas del DataFrame:\")\nprint(df.head())\nprint(\"\\nInformaci√≥n del DataFrame:\")\nprint(df.info())\n\nPrimeras 5 filas del DataFrame:\n         x          y\n0  0.00000  10.947363\n1  0.10101   7.721865\n2  0.20202  10.634664\n3  0.30303  10.199935\n4  0.40404  11.223292\n\nInformaci√≥n del DataFrame:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   x       100 non-null    float64\n 1   y       100 non-null    float64\ndtypes: float64(2)\nmemory usage: 1.7 KB\nNone\n\n\n\n\n3.2 Manipulaci√≥n de DataFrames\n\n# Filtrado de datos\nprint(\"Filas donde y &gt; 12:\")\ndf_filtrado = df.query(\"y &gt; 12\")\nprint(df_filtrado)\n\n# Estad√≠sticas descriptivas\nprint(\"\\nCuantiles de la variable y:\")\nprint(df.y.quantile(q=[0.25, 0.5, 0.75, 0.99]))\n\nFilas donde y &gt; 12:\n            x          y\n10   1.010101  12.755558\n17   1.717172  16.315991\n28   2.828283  12.890289\n31   3.131313  12.095512\n33   3.333333  12.186190\n35   3.535354  16.519562\n38   3.838384  14.936178\n39   3.939394  12.445576\n43   4.343434  13.407873\n44   4.444444  14.740559\n46   4.646465  14.085751\n47   4.747475  13.406937\n48   4.848485  12.711032\n49   4.949495  14.302195\n51   5.151515  13.419018\n53   5.353535  14.202517\n54   5.454545  13.460248\n55   5.555556  13.570844\n56   5.656566  15.336198\n58   5.858586  13.248156\n59   5.959596  12.120376\n60   6.060606  13.536254\n61   6.161616  13.877174\n62   6.262626  12.643431\n64   6.464646  13.144207\n65   6.565657  16.558753\n66   6.666667  14.174915\n67   6.767677  13.188301\n68   6.868687  15.327148\n69   6.969697  14.229089\n70   7.070707  14.671629\n71   7.171717  15.430842\n72   7.272727  14.078094\n74   7.474747  14.913045\n75   7.575758  16.341642\n76   7.676768  13.882098\n77   7.777778  14.801524\n78   7.878788  16.116168\n79   7.979798  14.168691\n80   8.080808  12.395476\n81   8.181818  12.295980\n82   8.282828  15.132112\n83   8.383838  12.813598\n84   8.484848  14.631908\n86   8.686869  14.303784\n87   8.787879  13.279560\n88   8.888889  12.344803\n89   8.989899  12.711722\n91   9.191919  16.917684\n92   9.292929  14.830744\n93   9.393939  15.292800\n95   9.595960  12.746624\n96   9.696970  16.000313\n97   9.797980  15.209167\n98   9.898990  14.825473\n99  10.000000  13.841673\n\nCuantiles de la variable y:\n0.25    11.154739\n0.50    12.420526\n0.75    14.181815\n0.99    16.562342\nName: y, dtype: float64\n\n\n\n# Renombramiento de columnas\nprint(\"Renombrando columnas...\")\ndf_renamed = df.rename(columns={\"x\": \"variable_independiente\", \"y\": \"variable_dependiente\"})\nprint(df_renamed.head())\n\n# Volvemos a los nombres originales para el resto del notebook\ndf = df.rename(columns={\"x\": \"x\", \"y\": \"y\"})\n\nRenombrando columnas...\n   variable_independiente  variable_dependiente\n0                 0.00000             10.947363\n1                 0.10101              7.721865\n2                 0.20202             10.634664\n3                 0.30303             10.199935\n4                 0.40404             11.223292",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#iteraci√≥n-en-python",
    "href": "introduccion.html#iteraci√≥n-en-python",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "4. Iteraci√≥n en Python",
    "text": "4. Iteraci√≥n en Python\n\n4.1 Iteraci√≥n sobre Diccionarios\n\n# Ejemplo de diccionario con diferentes tipos de datos\ndiccionario_ejemplo = {\n    \"entero\": 42,\n    \"array_numpy\": np.array([1, 2, 3, 4]),\n    \"cadena\": \"miner√≠a de datos\",\n    \"flotante\": 3.14159\n}\n\n# Iteraci√≥n sobre diccionarios\nprint(\"Iterando sobre el diccionario:\")\nfor clave, valor in diccionario_ejemplo.items():\n    print(f\"Clave: {clave} | Valor: {valor} | Tipo: {type(valor).__name__}\")\n\nIterando sobre el diccionario:\nClave: entero | Valor: 42 | Tipo: int\nClave: array_numpy | Valor: [1 2 3 4] | Tipo: ndarray\nClave: cadena | Valor: miner√≠a de datos | Tipo: str\nClave: flotante | Valor: 3.14159 | Tipo: float\n\n\n\n\n4.2 Iteraci√≥n sobre Listas\n\n# Ejemplo con lista de n√∫meros\nnumeros = [1, 4, 9, 16, 25]\nprint(\"Iterando sobre lista de n√∫meros:\")\nfor i, numero in enumerate(numeros):\n    print(f\"Posici√≥n {i}: {numero}, ra√≠z cuadrada: {np.sqrt(numero):.2f}\")\n    \n# Ejemplo con diferentes valores de k para KNN\nvalores_k = [1, 5, 10, 20, 50]\nprint(\"\\nIterando sobre valores de k:\")\nfor k in valores_k:\n    print(f\"Valor de k: {k}\")\n\nIterando sobre lista de n√∫meros:\nPosici√≥n 0: 1, ra√≠z cuadrada: 1.00\nPosici√≥n 1: 4, ra√≠z cuadrada: 2.00\nPosici√≥n 2: 9, ra√≠z cuadrada: 3.00\nPosici√≥n 3: 16, ra√≠z cuadrada: 4.00\nPosici√≥n 4: 25, ra√≠z cuadrada: 5.00\n\nIterando sobre valores de k:\nValor de k: 1\nValor de k: 5\nValor de k: 10\nValor de k: 20\nValor de k: 50",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#visualizaci√≥n-con-matplotlib-y-seaborn",
    "href": "introduccion.html#visualizaci√≥n-con-matplotlib-y-seaborn",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "5. Visualizaci√≥n con Matplotlib y Seaborn",
    "text": "5. Visualizaci√≥n con Matplotlib y Seaborn\n\n5.1 Gr√°fica de Dispersi√≥n B√°sica\n\n# Gr√°fica de dispersi√≥n b√°sica\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x=\"x\", y=\"y\")\nplt.title(\"Relaci√≥n entre x y y\")\nplt.xlabel(\"Variable independiente (x)\")\nplt.ylabel(\"Variable dependiente (y)\")\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#funciones-con-type-hints",
    "href": "introduccion.html#funciones-con-type-hints",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "6. Funciones con Type Hints",
    "text": "6. Funciones con Type Hints\n\n6.1 Funci√≥n para Generar Datos\n\ndef generar_datos_lineales(\n    n: int = 100,\n    slope: float = 0.5,\n    intercept: float = 1.0,\n    noise_scale: float = 1.5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Genera datos sint√©ticos para regresi√≥n lineal.\n    \n    Par√°metros:\n    -----------\n    n : int\n        N√∫mero de puntos de datos a generar\n    slope : float\n        Pendiente de la relaci√≥n lineal\n    intercept : float\n        Intercepto de la relaci√≥n lineal\n    noise_scale : float\n        Desviaci√≥n est√°ndar del ruido gaussiano\n    \n    Retorna:\n    --------\n    pd.DataFrame\n        DataFrame con columnas 'x' y 'y'\n    \"\"\"\n    X = np.linspace(0, 10, n)\n    y = slope * X + intercept + np.random.normal(scale=noise_scale, size=n)\n    return pd.DataFrame({\"x\": X, \"y\": y})\n\n# Ejemplo de uso\ndf_nuevo = generar_datos_lineales(n=50, slope=2.0, intercept=5.0)\nprint(\"Datos generados:\")\nprint(df_nuevo.head())\n\nDatos generados:\n          x         y\n0  0.000000  6.962633\n1  0.204082  5.261446\n2  0.408163  6.349024\n3  0.612245  6.767730\n4  0.816327  5.768821\n\n\n\n\n6.2 Funci√≥n para Visualizaci√≥n\n\ndef crear_grafica_dispersion(\n    data_frame: pd.DataFrame,\n    x_col: str = \"x\",\n    y_col: str = \"y\",\n    titulo: str = \"Gr√°fica de Dispersi√≥n\",\n    nombre_eje_x: str = \"X\",\n    nombre_eje_y: str = \"Y\"\n) -&gt; None:\n    \"\"\"\n    Crea una gr√°fica de dispersi√≥n con formato personalizado.\n    \n    Par√°metros:\n    -----------\n    data_frame : pd.DataFrame\n        DataFrame que contiene los datos\n    x_col : str\n        Nombre de la columna para el eje x\n    y_col : str\n        Nombre de la columna para el eje y\n    titulo : str\n        T√≠tulo de la gr√°fica\n    nombre_eje_x : str\n        Etiqueta del eje x\n    nombre_eje_y : str\n        Etiqueta del eje y\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.scatterplot(data=data_frame, x=x_col, y=y_col)\n    plt.title(titulo)\n    plt.xlabel(nombre_eje_x)\n    plt.ylabel(nombre_eje_y)\n    plt.show()\n\n# Ejemplo de uso\ncrear_grafica_dispersion(\n    df_nuevo, \n    titulo=\"Datos Sint√©ticos Generados\",\n    nombre_eje_x=\"Variable Independiente\",\n    nombre_eje_y=\"Variable Dependiente\"\n)",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#regresi√≥n-lineal",
    "href": "introduccion.html#regresi√≥n-lineal",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "7. Regresi√≥n Lineal",
    "text": "7. Regresi√≥n Lineal\n\n7.1 Implementaci√≥n y Ajuste del Modelo\n\n# Generamos datos para trabajar\ndf = generar_datos_lineales(n=1000, slope=1.0, intercept=2.0, noise_scale=1.5)\n\n# 1. Crear el modelo de regresi√≥n lineal\nmodelo_lr = LinearRegression()\n\n# 2. Ajustar el modelo (entrenamiento)\n# Nota: sklearn necesita X como matriz (2D) y y como vector (1D)\nX_features = df[[\"x\"]]  # Matriz 2D\ny_target = df[\"y\"]     # Vector 1D\n\nmodelo_lr.fit(X_features, y_target)\n\nprint(f\"Coeficiente (pendiente): {modelo_lr.coef_[0]:.3f}\")\nprint(f\"Intercepto: {modelo_lr.intercept_:.3f}\")\n\nCoeficiente (pendiente): 1.006\nIntercepto: 1.935\n\n\n\n\n7.2 Predicciones y Evaluaci√≥n\n\n# 3. Hacer predicciones\ny_pred_lr = modelo_lr.predict(X_features)\n\n# 4. Evaluar el modelo\nmse_lr = mean_squared_error(y_target, y_pred_lr)\nprint(f\"Error Cuadr√°tico Medio (MSE): {mse_lr:.3f}\")\n\n# Agregar predicciones al DataFrame para visualizaci√≥n\ndf[\"y_pred_lr\"] = y_pred_lr\n\nError Cuadr√°tico Medio (MSE): 2.369\n\n\n\n\n7.3 Visualizaci√≥n del Modelo\n\n# Visualizaci√≥n de datos originales y l√≠nea de regresi√≥n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"x\", y=\"y\", alpha=0.6, label=\"Datos Originales\")\nsns.lineplot(data=df, x=\"x\", y=\"y_pred_lr\", color=\"red\", linewidth=2, label=\"Regresi√≥n Lineal\")\nplt.title(\"Regresi√≥n Lineal: Datos vs Predicciones\")\nplt.xlabel(\"Variable Independiente (x)\")\nplt.ylabel(\"Variable Dependiente (y)\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#k-nearest-neighbors-knn",
    "href": "introduccion.html#k-nearest-neighbors-knn",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "8. K-Nearest Neighbors (KNN)",
    "text": "8. K-Nearest Neighbors (KNN)\n\n8.1 Implementaci√≥n B√°sica de KNN\n\n# 1. Crear modelo KNN con k=10\nmodelo_knn = KNeighborsRegressor(n_neighbors=10)\n\n# 2. Ajustar el modelo\nmodelo_knn.fit(X_features, y_target)\n\n# 3. Hacer predicciones\ny_pred_knn = modelo_knn.predict(X_features)\n\n# 4. Evaluar el modelo\nmse_knn = mean_squared_error(y_target, y_pred_knn)\nprint(f\"MSE KNN (k=10): {mse_knn:.3f}\")\nprint(f\"MSE Regresi√≥n Lineal: {mse_lr:.3f}\")\n\n# Agregar predicciones al DataFrame\ndf[\"y_pred_knn10\"] = y_pred_knn\n\nMSE KNN (k=10): 2.055\nMSE Regresi√≥n Lineal: 2.369\n\n\n\n\n8.2 Funci√≥n para Evaluar Diferentes Valores de K\n\ndef evaluar_knn_diferentes_k(\n    X: pd.DataFrame, \n    y: pd.Series, \n    valores_k: list\n) -&gt; dict:\n    \"\"\"\n    Eval√∫a el rendimiento de KNN para diferentes valores de k.\n    \n    Par√°metros:\n    -----------\n    X : pd.DataFrame\n        Variables independientes\n    y : pd.Series\n        Variable dependiente\n    valores_k : list\n        Lista de valores de k a evaluar\n    \n    Retorna:\n    --------\n    dict\n        Diccionario con k como clave y MSE como valor\n    \"\"\"\n    resultados = {}\n    \n    for k in valores_k:\n        # Crear y ajustar modelo\n        knn = KNeighborsRegressor(n_neighbors=k)\n        knn.fit(X, y)\n        \n        # Predicciones y evaluaci√≥n\n        y_pred = knn.predict(X)\n        mse = mean_squared_error(y, y_pred)\n        \n        resultados[k] = mse\n        print(f\"k={k}: MSE={mse:.3f}\")\n    \n    return resultados\n\n# Evaluar diferentes valores de k\nvalores_k = [1, 5, 10, 20, 50, 100]\nprint(\"Evaluando diferentes valores de k:\")\nresultados_k = evaluar_knn_diferentes_k(X_features, y_target, valores_k)\n\nEvaluando diferentes valores de k:\nk=1: MSE=0.000\nk=5: MSE=1.837\nk=10: MSE=2.055\nk=20: MSE=2.206\nk=50: MSE=2.335\nk=100: MSE=2.367\n\n\n\n\n8.3 Visualizaci√≥n de Diferentes Valores de K\n\n# Crear DataFrame para comparar diferentes valores de k\ndf_comparacion = df[[\"x\", \"y\"]].copy()\n\n# Agregar predicciones para diferentes valores de k\nfor k in [1, 10, 100]:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_features, y_target)\n    df_comparacion[f\"y_pred_k{k}\"] = knn.predict(X_features)\n\n# Visualizaci√≥n comparativa\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, k in enumerate([1, 10, 100]):\n    ax = axes[i]\n    ax.scatter(df_comparacion[\"x\"], df_comparacion[\"y\"], alpha=0.6, label=\"Datos\")\n    ax.plot(df_comparacion[\"x\"], df_comparacion[f\"y_pred_k{k}\"], 'r-', linewidth=2, label=f\"KNN k={k}\")\n    ax.set_title(f\"KNN con k={k}\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#selecci√≥n-del-valor-√≥ptimo-de-k",
    "href": "introduccion.html#selecci√≥n-del-valor-√≥ptimo-de-k",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "9. Selecci√≥n del Valor √ìptimo de K",
    "text": "9. Selecci√≥n del Valor √ìptimo de K\n\n9.1 Evaluaci√≥n Sistem√°tica de K\n\n# Generar un dataset m√°s grande para la evaluaci√≥n\ndf_grande = generar_datos_lineales(n=1000, slope=2.0, intercept=3.0, noise_scale=2.0)\nX_grande = df_grande[[\"x\"]]\ny_grande = df_grande[\"y\"]\n\n# Rango de valores k a evaluar\nmax_k = min(200, len(df_grande) // 5)  # k m√°ximo razonable\nvalores_k_rango = np.arange(1, max_k, 10)\n\nprint(f\"Evaluando k desde 1 hasta {max_k-1} (cada 10 valores)\")\nprint(f\"Total de valores a evaluar: {len(valores_k_rango)}\")\n\nEvaluando k desde 1 hasta 199 (cada 10 valores)\nTotal de valores a evaluar: 20\n\n\n\n# Evaluaci√≥n de todos los valores de k\nerrores_k = []\nfor k in valores_k_rango:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_grande, y_grande)\n    y_pred = knn.predict(X_grande)\n    mse = mean_squared_error(y_grande, y_pred)\n    errores_k.append(mse)\n\n# Encontrar el k √≥ptimo\nk_optimo = valores_k_rango[np.argmin(errores_k)]\nerror_minimo = min(errores_k)\n\nprint(f\"Valor √≥ptimo de k: {k_optimo}\")\nprint(f\"MSE m√≠nimo: {error_minimo:.3f}\")\n\nValor √≥ptimo de k: 1\nMSE m√≠nimo: 0.000\n\n\n\n\n9.2 Visualizaci√≥n de la Curva de Error vs K\n\n# Gr√°fica de MSE vs k\nplt.figure(figsize=(10, 6))\nplt.plot(valores_k_rango, errores_k, 'b-', linewidth=2, marker='o', markersize=4)\nplt.axvline(x=k_optimo, color='r', linestyle='--', \n            label=f'k √≥ptimo = {k_optimo}\\nMSE = {error_minimo:.3f}')\nplt.xlabel('Valor de k')\nplt.ylabel('Error Cuadr√°tico Medio (MSE)')\nplt.title('Selecci√≥n del Valor √ìptimo de k en KNN')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#estimaci√≥n-del-error-real",
    "href": "introduccion.html#estimaci√≥n-del-error-real",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "10. Estimaci√≥n del Error Real",
    "text": "10. Estimaci√≥n del Error Real\n\n10.1 Divisi√≥n Train-Test\n\n# Generar dataset para validaci√≥n\nnp.random.seed(42)  # Para reproducibilidad\ndf_validacion = generar_datos_lineales(n=2000, slope=1.5, intercept=2.5, noise_scale=2.0)\n\n# Divisi√≥n train-test\nX_val = df_validacion[[\"x\"]]\ny_val = df_validacion[\"y\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_val, y_val, test_size=0.3, random_state=42\n)\n\nprint(f\"Datos de entrenamiento: {X_train.shape[0]}\")\nprint(f\"Datos de prueba: {X_test.shape[0]}\")\n\nDatos de entrenamiento: 1400\nDatos de prueba: 600\n\n\n\n\n10.2 Funci√≥n para Evaluaci√≥n Train-Test\n\ndef evaluar_modelo_train_test(\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n    k: int\n) -&gt; tuple:\n    \"\"\"\n    Eval√∫a KNN usando divisi√≥n train-test.\n    \n    Retorna:\n    --------\n    tuple\n        (error_entrenamiento, error_prueba)\n    \"\"\"\n    # Entrenar el modelo\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    # Predicciones\n    y_train_pred = knn.predict(X_train)\n    y_test_pred = knn.predict(X_test)\n    \n    # Errores\n    error_train = mean_squared_error(y_train, y_train_pred)\n    error_test = mean_squared_error(y_test, y_test_pred)\n    \n    return error_train, error_test\n\n# Evaluar diferentes valores de k\nvalores_k_test = [1, 5, 10, 20, 50, 100]\nerrores_train = []\nerrores_test = []\n\nprint(\"Evaluaci√≥n con divisi√≥n train-test:\")\nprint(\"-\" * 40)\nfor k in valores_k_test:\n    error_train, error_test = evaluar_modelo_train_test(X_train, X_test, y_train, y_test, k)\n    errores_train.append(error_train)\n    errores_test.append(error_test)\n    print(f\"k={k:2d} | Train MSE: {error_train:.3f} | Test MSE: {error_test:.3f}\")\n\nEvaluaci√≥n con divisi√≥n train-test:\n----------------------------------------\nk= 1 | Train MSE: 0.000 | Test MSE: 7.600\nk= 5 | Train MSE: 3.092 | Test MSE: 5.007\nk=10 | Train MSE: 3.436 | Test MSE: 4.425\nk=20 | Train MSE: 3.641 | Test MSE: 4.214\nk=50 | Train MSE: 3.820 | Test MSE: 4.016\nk=100 | Train MSE: 3.874 | Test MSE: 3.975\n\n\n\n\n10.3 Visualizaci√≥n de Sesgo vs Varianza\n\n# Gr√°fica de error de entrenamiento vs error de prueba\nplt.figure(figsize=(10, 6))\nplt.plot(valores_k_test, errores_train, 'b-o', linewidth=2, label='Error de Entrenamiento')\nplt.plot(valores_k_test, errores_test, 'r-o', linewidth=2, label='Error de Prueba')\nplt.xlabel('Valor de k')\nplt.ylabel('Error Cuadr√°tico Medio (MSE)')\nplt.title('Curva de Validaci√≥n: Error de Entrenamiento vs Error de Prueba')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')  # Escala logar√≠tmica para mejor visualizaci√≥n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.4 Comparaci√≥n Final: Regresi√≥n Lineal vs KNN √ìptimo\n\n# Encontrar el k √≥ptimo basado en error de prueba\nk_optimo_test = valores_k_test[np.argmin(errores_test)]\nprint(f\"Valor √≥ptimo de k (basado en error de prueba): {k_optimo_test}\")\n\n# Evaluar regresi√≥n lineal\nlr_final = LinearRegression()\nlr_final.fit(X_train, y_train)\ny_test_pred_lr = lr_final.predict(X_test)\nerror_test_lr = mean_squared_error(y_test, y_test_pred_lr)\n\n# Evaluar KNN √≥ptimo\nknn_final = KNeighborsRegressor(n_neighbors=k_optimo_test)\nknn_final.fit(X_train, y_train)\ny_test_pred_knn = knn_final.predict(X_test)\nerror_test_knn = mean_squared_error(y_test, y_test_pred_knn)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"COMPARACI√ìN FINAL - ERROR DE PRUEBA (ESTIMACI√ìN REAL)\")\nprint(\"=\"*50)\nprint(f\"Regresi√≥n Lineal:        {error_test_lr:.3f}\")\nprint(f\"KNN (k={k_optimo_test}):             {error_test_knn:.3f}\")\nprint(\"=\"*50)\n\nif error_test_knn &lt; error_test_lr:\n    mejora = ((error_test_lr - error_test_knn) / error_test_lr) * 100\n    print(f\"KNN es mejor por {mejora:.1f}%\")\nelse:\n    mejora = ((error_test_knn - error_test_lr) / error_test_knn) * 100\n    print(f\"Regresi√≥n Lineal es mejor por {mejora:.1f}%\")\n\nValor √≥ptimo de k (basado en error de prueba): 100\n\n==================================================\nCOMPARACI√ìN FINAL - ERROR DE PRUEBA (ESTIMACI√ìN REAL)\n==================================================\nRegresi√≥n Lineal:        3.887\nKNN (k=100):             3.975\n==================================================\nRegresi√≥n Lineal es mejor por 2.2%",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#resumen",
    "href": "introduccion.html#resumen",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "Resumen",
    "text": "Resumen\nEn este notebook hemos cubierto:\n\nNumPy: Creaci√≥n y manipulaci√≥n de arrays para datos num√©ricos\nPandas: Manipulaci√≥n de datos estructurados con DataFrames\nIteraci√≥n: T√©cnicas para iterar sobre diccionarios y listas\nVisualizaci√≥n: Creaci√≥n de gr√°ficas informativas con matplotlib/seaborn\nType Hints: Definici√≥n de funciones bien documentadas y tipadas\nRegresi√≥n Lineal: Implementaci√≥n, ajuste y evaluaci√≥n\nKNN: Implementaci√≥n y comparaci√≥n de diferentes valores de k\nSelecci√≥n de Hiperpar√°metros: T√©cnicas para encontrar el k √≥ptimo\nValidaci√≥n: Estimaci√≥n del error real usando divisi√≥n train-test\n\n\nConceptos Clave Aprendidos:\n\nSesgo vs Varianza: KNN con k peque√±o tiene alta varianza, k grande tiene alto sesgo\nValidaci√≥n: El error de entrenamiento subestima el error real\nSelecci√≥n de Modelos: Comparar diferentes algoritmos usando datos de prueba\nType Hints: Mejoran la legibilidad y mantenibilidad del c√≥digo",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "regresion_lineal_alumnos.html",
    "href": "regresion_lineal_alumnos.html",
    "title": "Descenso en gradiente con regresi√≥n lineal",
    "section": "",
    "text": "Implementaci√≥n multivariada\ndef generar_datos_lineales_multivariados(n_muestras: int, n_caracteristicas: int, ruido: float = 0.5) -&gt; tuple[pd.DataFrame, np.ndarray, float]:\n    \"\"\"\n    Genera un conjunto de datos sint√©tico para regresi√≥n lineal multivariada.\n\n    Args:\n        n_muestras (int): El n√∫mero de puntos de datos a generar (filas).\n        n_caracteristicas (int): El n√∫mero de variables independientes (caracter√≠sticas).\n        ruido (float): La desviaci√≥n est√°ndar del ruido gaussiano a a√±adir.\n                       Controla la dispersi√≥n de los puntos.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray, float]:\n            - Un DataFrame de pandas con las caracter√≠sticas (x1, x2, ...) y la variable objetivo (y).\n            - El array de coeficientes (pesos) reales que se usaron para generar los datos.\n            - El intercepto (sesgo) real que se us√≥.\n    \"\"\"\n    # 1. Generar las caracter√≠sticas (X) con valores aleatorios entre 0 y 10\n    X = 10 * np.random.rand(n_muestras, n_caracteristicas)\n\n    # 2. Generar coeficientes e intercepto reales aleatorios\n    # Estos son los \"verdaderos\" par√°metros que un modelo intentar√≠a encontrar.\n    coeficientes_reales = np.random.randn(n_caracteristicas) * 2\n    intercepto_real = np.random.randn() * 5\n\n    # 3. Generar el ruido gaussiano\n    # El ruido simula la variabilidad aleatoria en los datos del mundo real.\n    ruido_gaussiano = np.random.randn(n_muestras) * ruido\n\n    # 4. Calcular la variable objetivo (y) usando la ecuaci√≥n lineal\n    # y = (X ‚Ä¢ coeficientes) + intercepto + ruido\n    y = np.dot(X, coeficientes_reales) + intercepto_real + ruido_gaussiano\n\n    # 5. Formatear la salida en un DataFrame de pandas\n    nombres_columnas = [f'x{i+1}' for i in range(n_caracteristicas)]\n    datos = pd.DataFrame(X, columns=nombres_columnas)\n    datos['y'] = y\n\n    return datos, coeficientes_reales, intercepto_real\nnum_muestras = 200\nnum_caracteristicas = 10 \nnivel_ruido = 1.5\n\n# Generar los datos\ndatos_generados, coeficientes, intercepto = generar_datos_lineales_multivariados(\n    n_muestras=num_muestras,\n    n_caracteristicas=num_caracteristicas,\n    ruido=nivel_ruido\n)\ndef descenso_gradiente_multivariado(datos: pd.DataFrame, learning_rate: float, iteraciones: int) -&gt; tuple[np.ndarray, float, list]:\n    \"\"\"\n    Realiza el descenso de gradiente para una regresi√≥n lineal multivariada.\n\n    Args:\n        datos (pd.DataFrame): DataFrame que contiene las caracter√≠sticas y la variable objetivo 'y'.\n        learning_rate (float): La tasa de aprendizaje.\n        iteraciones (int): El n√∫mero de iteraciones para ejecutar el algoritmo.\n\n    Returns:\n        tuple[np.ndarray, float, list]:\n            - El array de coeficientes (pesos) optimizados.\n            - El intercepto (sesgo) optimizado.\n            - Una lista con el historial del Error Cuadr√°tico Medio (MSE) en cada iteraci√≥n.\n    \"\"\"\n    # 1. Preparar los datos\n    X = ...  # Matriz de caracter√≠sticas\n    y = datos['y'].values              # Vector de la variable objetivo\n    n_muestras, n_caracteristicas = ...\n\n    # 2. Inicializar par√°metros\n    coeficientes = np.zeros(n_caracteristicas)\n    intercepto = 0.0\n    historial_error = []\n\n    # 3. Iterar para optimizar los par√°metros\n    for i in range(iteraciones):\n        # Calcular las predicciones (producto punto de X y coeficientes)\n        # Ecuaci√≥n: y_pred = (X ‚Ä¢ coeficientes) + intercepto\n        y_pred = ...\n\n        # Calcular el Error Cuadr√°tico Medio (MSE) y guardarlo\n        error = ...\n        historial_error.append(error)\n\n        # Calcular los gradientes (derivadas parciales)\n        # El gradiente es la direcci√≥n de m√°ximo ascenso del error.\n        # Lo calculamos de forma vectorizada para eficiencia.\n        D_coeficientes = ...\n        D_intercepto = ...\n\n        # 4. Actualizar los par√°metros (moverse en direcci√≥n opuesta al gradiente)\n        coeficientes = ...\n        intercepto = ...\n\n    return coeficientes, intercepto, historial_error\nnum_muestras = 200\nnum_caracteristicas = 3\nnivel_ruido = 1.5\ndatos_generados, coef_reales, int_real = generar_datos_lineales_multivariados(\n    n_muestras=num_muestras,\n    n_caracteristicas=num_caracteristicas,\n    ruido=nivel_ruido\n)\n\nX_original = datos_generados.drop('y', axis=1)\ny = datos_generados['y']\n\nscaler = StandardScaler()\nX_escalado = scaler.fit_transform(X_original)\n\ndatos_escalados = pd.DataFrame(X_escalado, columns=X_original.columns)\ndatos_escalados['y'] = y.values\n\nlearning_rate = 0.01\niteraciones = 1000\ncoef_aprendidos, int_aprendido, error_hist = descenso_gradiente_multivariado(\n    datos=datos_escalados,\n    learning_rate=learning_rate,\n    iteraciones=iteraciones\n)\n\nmodelo_sklearn = LinearRegression()\nmodelo_sklearn.fit(X_escalado, y)\ncoef_sklearn = modelo_sklearn.coef_\nint_sklearn = modelo_sklearn.intercept_\nprint(\"--- Comparaci√≥n de Par√°metros (con datos escalados) ---\")\nprint(f\"Intercepto GD: {int_aprendido:.4f}  |  Intercepto Sklearn: {int_sklearn:.4f}\")\nfor i in range(num_caracteristicas):\n    print(f\"Coef. x{i+1} GD: {coef_aprendidos[i]:.4f} |  Coef. x{i+1} Sklearn: {coef_sklearn[i]:.4f}\")",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Descenso en gradiente con regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html",
    "href": "violaciones_supuestos_regresion.html",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "",
    "text": "1. Violaci√≥n del Supuesto de Linealidad\nEste notebook demuestra gr√°ficamente c√≥mo pueden violarse los cuatro supuestos principales de la regresi√≥n lineal:\nEl supuesto de linealidad requiere que la relaci√≥n entre X e Y sea lineal. Cuando esto se viola, un modelo lineal ser√° inadecuado.\ndef crear_datos_no_lineales(n=200, tipo='cuadratico', noise=1.0, seed=42):\n    \"\"\"Genera datos con relaciones no lineales\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(-3, 3, n)\n    \n    if tipo == 'cuadratico':\n        y = 2 * x**2 + 1 + np.random.normal(0, noise, n)\n        titulo = \"Relaci√≥n Cuadr√°tica\"\n    elif tipo == 'exponencial':\n        y = np.exp(x/2) + np.random.normal(0, noise, n)\n        titulo = \"Relaci√≥n Exponencial\"\n    elif tipo == 'sinusoidal':\n        y = 5 * np.sin(2*x) + x + np.random.normal(0, noise, n)\n        titulo = \"Relaci√≥n Sinusoidal\"\n    elif tipo == 'logaritmico':\n        x = np.linspace(0.1, 10, n)\n        y = 3 * np.log(x) + 2 + np.random.normal(0, noise, n)\n        titulo = \"Relaci√≥n Logar√≠tmica\"\n    \n    return x, y, titulo\n\n# Crear subplots para diferentes tipos de no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Violaciones del Supuesto de Linealidad', fontsize=16, fontweight='bold')\n\ntipos = ['cuadratico', 'exponencial', 'sinusoidal', 'logaritmico']\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    \n    # Gr√°fico\n    axes[row, col].scatter(x, y, alpha=0.6, label='Datos reales')\n    axes[row, col].plot(x, y_pred, 'r-', linewidth=2, label='Regresi√≥n lineal')\n    axes[row, col].set_title(titulo)\n    axes[row, col].set_xlabel('X')\n    axes[row, col].set_ylabel('Y')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n    \n    # Calcular R¬≤\n    r2 = lr.score(X_reshaped, y)\n    axes[row, col].text(0.05, 0.95, f'R¬≤ = {r2:.3f}', \n                        transform=axes[row, col].transAxes, \n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-linealidad",
    "href": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-linealidad",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "",
    "text": "Diagn√≥stico: Gr√°ficos de Residuos vs Valores Predichos\nUna forma de detectar no linealidad es examinar los residuos. Si hay patrones en los residuos, indica problemas con el modelo.\n\n# Crear gr√°ficos de residuos para detectar no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Gr√°ficos de Residuos: Detectando No Linealidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    residuos = y - y_pred\n    \n    # Gr√°fico de residuos\n    axes[row, col].scatter(y_pred, residuos, alpha=0.6)\n    axes[row, col].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[row, col].set_title(f'Residuos - {titulo}')\n    axes[row, col].set_xlabel('Valores Predichos')\n    axes[row, col].set_ylabel('Residuos')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä INTERPRETACI√ìN:\")\nprint(\"‚úÖ Residuos aleatorios alrededor de 0 = Linealidad OK\")\nprint(\"‚ùå Patrones en residuos = Violaci√≥n de linealidad\")\nprint(\"   - Curva: Relaci√≥n cuadr√°tica o polinomial\")\nprint(\"   - Tendencia: Relaci√≥n exponencial o logar√≠tmica\")\nprint(\"   - Oscilaci√≥n: Efectos c√≠clicos o sinusoidales\")\n\n\n\n\n\n\n\n\nüìä INTERPRETACI√ìN:\n‚úÖ Residuos aleatorios alrededor de 0 = Linealidad OK\n‚ùå Patrones en residuos = Violaci√≥n de linealidad\n   - Curva: Relaci√≥n cuadr√°tica o polinomial\n   - Tendencia: Relaci√≥n exponencial o logar√≠tmica\n   - Oscilaci√≥n: Efectos c√≠clicos o sinusoidales",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#soluci√≥n",
    "href": "violaciones_supuestos_regresion.html#soluci√≥n",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "Soluci√≥n",
    "text": "Soluci√≥n\n\n# Crear subplots para diferentes tipos de no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Violaciones del Supuesto de Linealidad', fontsize=16, fontweight='bold')\n\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    if tipo == \"cuadratico\":\n        X_reshaped = X_reshaped**2\n    if tipo == \"exponencial\":\n        X_reshaped = np.exp(X_reshaped)\n    if tipo == \"sinusoidal\":\n        X_reshaped = np.concatenate((np.sin(2*X_reshaped), X_reshaped), axis = 1)\n    if tipo == \"logaritmico\":\n        X_reshaped = np.log(X_reshaped)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    \n    # Gr√°fico\n    axes[row, col].scatter(x, y, alpha=0.6, label='Datos reales')\n    axes[row, col].plot(x, y_pred, 'r-', linewidth=2, label='Regresi√≥n lineal')\n    axes[row, col].set_title(titulo)\n    axes[row, col].set_xlabel('X')\n    axes[row, col].set_ylabel('Y')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n    \n    # Calcular R¬≤\n    r2 = lr.score(X_reshaped, y)\n    axes[row, col].text(0.05, 0.95, f'R¬≤ = {r2:.3f}', \n                        transform=axes[row, col].transAxes, \n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Crear gr√°ficos de residuos para detectar no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Gr√°ficos de Residuos: Detectando No Linealidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    if tipo == \"cuadratico\":\n        X_reshaped = X_reshaped**2\n    if tipo == \"exponencial\":\n        X_reshaped = np.exp(X_reshaped)\n    if tipo == \"sinusoidal\":\n        X_reshaped = np.concatenate((np.sin(2*X_reshaped), X_reshaped), axis = 1)\n    if tipo == \"logaritmico\":\n        X_reshaped = np.log(X_reshaped)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    residuos = y - y_pred\n    \n    # Gr√°fico de residuos\n    axes[row, col].scatter(y_pred, residuos, alpha=0.6)\n    axes[row, col].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[row, col].set_title(f'Residuos - {titulo}')\n    axes[row, col].set_xlabel('Valores Predichos')\n    axes[row, col].set_ylabel('Residuos')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-independencia",
    "href": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-independencia",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "2. Violaci√≥n del Supuesto de Independencia",
    "text": "2. Violaci√≥n del Supuesto de Independencia\nLos errores deben ser independientes entre s√≠. Esto es especialmente importante en datos de series temporales.\n\ndef crear_datos_correlacionados(n=200, correlacion=0.7, seed=42):\n    \"\"\"Genera datos con errores correlacionados (autocorrelaci√≥n)\"\"\"\n    np.random.seed(seed)\n    \n    # Datos independientes (correctos)\n    x = np.linspace(0, 10, n)\n    errores_independientes = np.random.normal(0, 2, n)\n    y_independiente = 2 + 1.5 * x + errores_independientes\n    \n    # Datos con autocorrelaci√≥n (violaci√≥n)\n    errores_correlacionados = np.zeros(n)\n    errores_correlacionados[0] = np.random.normal(0, 2)\n    \n    for i in range(1, n):\n        errores_correlacionados[i] = (correlacion * errores_correlacionados[i-1] + \n                                     np.sqrt(1 - correlacion**2) * np.random.normal(0, 2))\n    \n    y_correlacionado = 2 + 1.5 * x + errores_correlacionados\n    \n    return x, y_independiente, y_correlacionado, errores_independientes, errores_correlacionados\n\n# Generar datos\nx, y_indep, y_corr, err_indep, err_corr = crear_datos_correlacionados()\n\n# Ajustar modelos\nlr_indep = LinearRegression().fit(x.reshape(-1, 1), y_indep)\nlr_corr = LinearRegression().fit(x.reshape(-1, 1), y_corr)\n\ny_pred_indep = lr_indep.predict(x.reshape(-1, 1))\ny_pred_corr = lr_corr.predict(x.reshape(-1, 1))\n\nresiduos_indep = y_indep - y_pred_indep\nresiduos_corr = y_corr - y_pred_corr\n\n# Crear gr√°ficos\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Violaci√≥n del Supuesto de Independencia', fontsize=16, fontweight='bold')\n\n# Fila 1: Datos independientes (correctos)\naxes[0, 0].scatter(x, y_indep, alpha=0.6, color='blue')\naxes[0, 0].plot(x, y_pred_indep, 'r-', linewidth=2)\naxes[0, 0].set_title('Datos con Errores Independientes ‚úÖ')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('Y')\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(residuos_indep, 'o-', alpha=0.7, color='blue')\naxes[0, 1].axhline(y=0, color='red', linestyle='--')\naxes[0, 1].set_title('Residuos vs Tiempo')\naxes[0, 1].set_xlabel('Observaci√≥n')\naxes[0, 1].set_ylabel('Residuos')\naxes[0, 1].grid(True, alpha=0.3)\n\naxes[0, 2].scatter(residuos_indep[:-1], residuos_indep[1:], alpha=0.6, color='blue')\naxes[0, 2].set_title('Residuos(t) vs Residuos(t-1)')\naxes[0, 2].set_xlabel('Residuo en t-1')\naxes[0, 2].set_ylabel('Residuo en t')\naxes[0, 2].grid(True, alpha=0.3)\n\n# Calcular correlaci√≥n\ncorr_indep = np.corrcoef(residuos_indep[:-1], residuos_indep[1:])[0, 1]\naxes[0, 2].text(0.05, 0.95, f'r = {corr_indep:.3f}', \n                transform=axes[0, 2].transAxes,\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n# Fila 2: Datos correlacionados (violaci√≥n)\naxes[1, 0].scatter(x, y_corr, alpha=0.6, color='orange')\naxes[1, 0].plot(x, y_pred_corr, 'r-', linewidth=2)\naxes[1, 0].set_title('Datos con Errores Correlacionados ‚ùå')\naxes[1, 0].set_xlabel('X')\naxes[1, 0].set_ylabel('Y')\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(residuos_corr, 'o-', alpha=0.7, color='orange')\naxes[1, 1].axhline(y=0, color='red', linestyle='--')\naxes[1, 1].set_title('Residuos vs Tiempo')\naxes[1, 1].set_xlabel('Observaci√≥n')\naxes[1, 1].set_ylabel('Residuos')\naxes[1, 1].grid(True, alpha=0.3)\n\naxes[1, 2].scatter(residuos_corr[:-1], residuos_corr[1:], alpha=0.6, color='orange')\naxes[1, 2].set_title('Residuos(t) vs Residuos(t-1)')\naxes[1, 2].set_xlabel('Residuo en t-1')\naxes[1, 2].set_ylabel('Residuo en t')\naxes[1, 2].grid(True, alpha=0.3)\n\n# Calcular correlaci√≥n\ncorr_corr = np.corrcoef(residuos_corr[:-1], residuos_corr[1:])[0, 1]\naxes[1, 2].text(0.05, 0.95, f'r = {corr_corr:.3f}', \n                transform=axes[1, 2].transAxes,\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä INTERPRETACI√ìN:\")\nprint(f\"‚úÖ Errores independientes: correlaci√≥n = {corr_indep:.3f} (cerca de 0)\")\nprint(f\"‚ùå Errores correlacionados: correlaci√≥n = {corr_corr:.3f} (lejos de 0)\")\nprint(\"\\nüîç DIAGN√ìSTICO:\")\nprint(\"- Gr√°fico temporal: patrones o tendencias en residuos\")\nprint(\"- Autocorrelaci√≥n: correlaci√≥n significativa entre residuos consecutivos\")\nprint(\"- Prueba Durbin-Watson: estad√≠stico cerca de 2 = independencia\")\n\n\n\n\n\n\n\n\nüìä INTERPRETACI√ìN:\n‚úÖ Errores independientes: correlaci√≥n = -0.058 (cerca de 0)\n‚ùå Errores correlacionados: correlaci√≥n = 0.670 (lejos de 0)\n\nüîç DIAGN√ìSTICO:\n- Gr√°fico temporal: patrones o tendencias en residuos\n- Autocorrelaci√≥n: correlaci√≥n significativa entre residuos consecutivos\n- Prueba Durbin-Watson: estad√≠stico cerca de 2 = independencia",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-homocedasticidad",
    "href": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-homocedasticidad",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "3. Violaci√≥n del Supuesto de Homocedasticidad",
    "text": "3. Violaci√≥n del Supuesto de Homocedasticidad\nLa varianza de los errores debe ser constante. Cuando var√≠a, tenemos heterocedasticidad.\n\ndef crear_datos_heteroscedasticos(n=200, tipo='creciente', seed=42):\n    \"\"\"Genera datos con diferentes tipos de heterocedasticidad\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(1, 10, n)\n    \n    if tipo == 'homoscedastico':\n        # Varianza constante (correcto)\n        error = np.random.normal(0, 2, n)\n        titulo = \"Homocedasticidad (Correcto) ‚úÖ\"\n    elif tipo == 'creciente':\n        # Varianza aumenta con X\n        error = np.random.normal(0, 0.5 * x, n)\n        titulo = \"Heterocedasticidad Creciente ‚ùå\"\n    elif tipo == 'decreciente':\n        # Varianza disminuye con X\n        error = np.random.normal(0, 5 / x, n)\n        titulo = \"Heterocedasticidad Decreciente ‚ùå\"\n    elif tipo == 'embudo':\n        # Forma de embudo\n        error = np.random.normal(0, 0.1 + 0.8 * np.abs(x - 5.5), n)\n        titulo = \"Heterocedasticidad en Embudo ‚ùå\"\n    \n    y = 2 + 1.5 * x + error\n    return x, y, titulo\n\n# Crear diferentes tipos de heterocedasticidad\ntipos_het = ['homoscedastico', 'creciente', 'decreciente', 'embudo']\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\nfig.suptitle('Violaci√≥n del Supuesto de Homocedasticidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos_het):\n    x, y, titulo = crear_datos_heteroscedasticos(tipo=tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    # Gr√°fico de datos\n    color = 'green' if tipo == 'homoscedastico' else 'red'\n    axes[0, i].scatter(x, y, alpha=0.6, color=color)\n    axes[0, i].plot(x, y_pred, 'black', linewidth=2)\n    axes[0, i].set_title(titulo)\n    axes[0, i].set_xlabel('X')\n    axes[0, i].set_ylabel('Y')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Gr√°fico de residuos\n    axes[1, i].scatter(y_pred, residuos, alpha=0.6, color=color)\n    axes[1, i].axhline(y=0, color='black', linestyle='--')\n    axes[1, i].set_title(f'Residuos vs Predichos')\n    axes[1, i].set_xlabel('Valores Predichos')\n    axes[1, i].set_ylabel('Residuos')\n    axes[1, i].grid(True, alpha=0.3)\n    \n    # A√±adir l√≠neas de tendencia en residuos para visualizar heterocedasticidad\n    if tipo != 'homoscedastico':\n        z = np.polyfit(y_pred, np.abs(residuos), 1)\n        p = np.poly1d(z)\n        axes[1, i].plot(y_pred, p(y_pred), \"r--\", alpha=0.8)\n        axes[1, i].plot(y_pred, -p(y_pred), \"r--\", alpha=0.8)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä INTERPRETACI√ìN:\")\nprint(\"‚úÖ Homocedasticidad: Residuos con dispersi√≥n constante\")\nprint(\"‚ùå Heterocedasticidad: Residuos con dispersi√≥n variable\")\nprint(\"\\nüîç PATRONES COMUNES:\")\nprint(\"- Embudo creciente: Varianza aumenta con valores predichos\")\nprint(\"- Embudo decreciente: Varianza disminuye con valores predichos\")\nprint(\"- Forma de diamante: Varianza m√°xima en valores medios\")\n\n\n\n\n\n\n\n\nüìä INTERPRETACI√ìN:\n‚úÖ Homocedasticidad: Residuos con dispersi√≥n constante\n‚ùå Heterocedasticidad: Residuos con dispersi√≥n variable\n\nüîç PATRONES COMUNES:\n- Embudo creciente: Varianza aumenta con valores predichos\n- Embudo decreciente: Varianza disminuye con valores predichos\n- Forma de diamante: Varianza m√°xima en valores medios\n\n\n\nPrueba Estad√≠stica: Test de Breusch-Pagan\nUna prueba formal para detectar heterocedasticidad.\n\nfrom scipy.stats import chi2\n\ndef breusch_pagan_test(residuos, x):\n    \"\"\"Implementa el test de Breusch-Pagan para heterocedasticidad\"\"\"\n    n = len(residuos)\n    \n    # Regresi√≥n de residuos al cuadrado sobre X\n    residuos_cuadrados = residuos**2\n    lr_bp = LinearRegression()\n    lr_bp.fit(x.reshape(-1, 1), residuos_cuadrados)\n    \n    # R¬≤ de la regresi√≥n auxiliar\n    r2_bp = lr_bp.score(x.reshape(-1, 1), residuos_cuadrados)\n    \n    # Estad√≠stico de prueba\n    lm_statistic = n * r2_bp\n    \n    # P-value (distribuci√≥n chi-cuadrado con 1 grado de libertad)\n    p_value = 1 - chi2.cdf(lm_statistic, df=1)\n    \n    return lm_statistic, p_value\n\nprint(\"üß™ TEST DE BREUSCH-PAGAN PARA HETEROCEDASTICIDAD\\n\")\nprint(\"H‚ÇÄ: Homocedasticidad (varianza constante)\")\nprint(\"H‚ÇÅ: Heterocedasticidad (varianza no constante)\\n\")\n\nfor tipo in tipos_het:\n    x, y, titulo = crear_datos_heteroscedasticos(tipo=tipo)\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    residuos = y - lr.predict(x.reshape(-1, 1))\n    \n    lm_stat, p_val = breusch_pagan_test(residuos, x)\n    \n    resultado = \"RECHAZA H‚ÇÄ\" if p_val &lt; 0.05 else \"NO RECHAZA H‚ÇÄ\"\n    emoji = \"‚ùå\" if p_val &lt; 0.05 else \"‚úÖ\"\n    \n    print(f\"{emoji} {tipo.upper():15} | LM = {lm_stat:6.2f} | p-value = {p_val:.4f} | {resultado}\")\n\nprint(\"\\nüí° Interpretaci√≥n: p-value &lt; 0.05 indica heterocedasticidad\")\n\nüß™ TEST DE BREUSCH-PAGAN PARA HETEROCEDASTICIDAD\n\nH‚ÇÄ: Homocedasticidad (varianza constante)\nH‚ÇÅ: Heterocedasticidad (varianza no constante)\n\n‚úÖ HOMOSCEDASTICO  | LM =   0.00 | p-value = 0.9588 | NO RECHAZA H‚ÇÄ\n‚ùå CRECIENTE       | LM =  27.79 | p-value = 0.0000 | RECHAZA H‚ÇÄ\n‚ùå DECRECIENTE     | LM =  31.91 | p-value = 0.0000 | RECHAZA H‚ÇÄ\n‚úÖ EMBUDO          | LM =   0.14 | p-value = 0.7122 | NO RECHAZA H‚ÇÄ\n\nüí° Interpretaci√≥n: p-value &lt; 0.05 indica heterocedasticidad",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-normalidad",
    "href": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-normalidad",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "4. Violaci√≥n del Supuesto de Normalidad",
    "text": "4. Violaci√≥n del Supuesto de Normalidad\nLos errores deben seguir una distribuci√≥n normal. Esto es crucial para las pruebas de hip√≥tesis y intervalos de confianza.\n\ndef crear_datos_no_normales(n=200, tipo='normal', seed=42):\n    \"\"\"Genera datos con diferentes distribuciones de error\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(0, 10, n)\n    \n    if tipo == 'normal':\n        error = np.random.normal(0, 2, n)\n        titulo = \"Errores Normales ‚úÖ\"\n    elif tipo == 'asimetrico':\n        # Distribuci√≥n asim√©trica (exponencial)\n        error = np.random.exponential(2, n) - 2\n        titulo = \"Errores Asim√©tricos ‚ùå\"\n    elif tipo == 'colas_pesadas':\n        # Distribuci√≥n t con colas pesadas\n        error = stats.t.rvs(df=3, scale=2, size=n)\n        titulo = \"Errores con Colas Pesadas ‚ùå\"\n    elif tipo == 'bimodal':\n        # Distribuci√≥n bimodal\n        mask = np.random.binomial(1, 0.5, n).astype(bool)\n        error = np.where(mask, \n                        np.random.normal(-2, 1, n),\n                        np.random.normal(2, 1, n))\n        titulo = \"Errores Bimodales ‚ùå\"\n    \n    y = 2 + 1.5 * x + error\n    return x, y, error, titulo\n\n# Crear diferentes tipos de no normalidad\ntipos_norm = ['normal', 'asimetrico', 'colas_pesadas', 'bimodal']\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\nfig.suptitle('Violaci√≥n del Supuesto de Normalidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos_norm):\n    x, y, error_real, titulo = crear_datos_no_normales(tipo=tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    color = 'green' if tipo == 'normal' else 'red'\n    \n    # Fila 1: Datos originales\n    axes[0, i].scatter(x, y, alpha=0.6, color=color)\n    axes[0, i].plot(x, y_pred, 'black', linewidth=2)\n    axes[0, i].set_title(titulo)\n    axes[0, i].set_xlabel('X')\n    axes[0, i].set_ylabel('Y')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Fila 2: Histograma de residuos\n    axes[1, i].hist(residuos, bins=25, density=True, alpha=0.7, color=color, edgecolor='black')\n    \n    # Superponer distribuci√≥n normal te√≥rica\n    x_norm = np.linspace(residuos.min(), residuos.max(), 100)\n    y_norm = stats.norm.pdf(x_norm, residuos.mean(), residuos.std())\n    axes[1, i].plot(x_norm, y_norm, 'blue', linewidth=2, label='Normal te√≥rica')\n    \n    axes[1, i].set_title(f'Histograma de Residuos')\n    axes[1, i].set_xlabel('Residuos')\n    axes[1, i].set_ylabel('Densidad')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n    \n    # Fila 3: Q-Q plot\n    stats.probplot(residuos, dist=\"norm\", plot=axes[2, i])\n    axes[2, i].set_title(f'Q-Q Plot')\n    axes[2, i].grid(True, alpha=0.3)\n    \n    # Calcular test de normalidad\n    shapiro_stat, shapiro_p = stats.shapiro(residuos)\n    axes[2, i].text(0.05, 0.95, f'Shapiro p={shapiro_p:.3f}', \n                    transform=axes[2, i].transAxes,\n                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä INTERPRETACI√ìN:\")\nprint(\"‚úÖ Q-Q plot lineal + Shapiro p &gt; 0.05 = Normalidad\")\nprint(\"‚ùå Q-Q plot no lineal + Shapiro p &lt; 0.05 = No normalidad\")\nprint(\"\\nüîç PATRONES EN Q-Q PLOTS:\")\nprint(\"- Curva S: Asimetr√≠a\")\nprint(\"- Colas alejadas de la l√≠nea: Colas pesadas/ligeras\")\nprint(\"- M√∫ltiples segmentos: Multimodalidad\")\n\n\n\n\n\n\n\n\nüìä INTERPRETACI√ìN:\n‚úÖ Q-Q plot lineal + Shapiro p &gt; 0.05 = Normalidad\n‚ùå Q-Q plot no lineal + Shapiro p &lt; 0.05 = No normalidad\n\nüîç PATRONES EN Q-Q PLOTS:\n- Curva S: Asimetr√≠a\n- Colas alejadas de la l√≠nea: Colas pesadas/ligeras\n- M√∫ltiples segmentos: Multimodalidad\n\n\n\nResumen de Pruebas Estad√≠sticas para Normalidad\n\nfrom scipy.stats import jarque_bera, anderson\n\nprint(\"üß™ PRUEBAS DE NORMALIDAD EN RESIDUOS\\n\")\nprint(\"H‚ÇÄ: Los residuos siguen distribuci√≥n normal\")\nprint(\"H‚ÇÅ: Los residuos NO siguen distribuci√≥n normal\\n\")\nprint(f\"{'Tipo':15} | {'Shapiro':&gt;10} | {'Jarque-Bera':&gt;12} | {'Anderson':&gt;10} | {'Conclusi√≥n'}\")\nprint(\"-\" * 75)\n\nfor tipo in tipos_norm:\n    x, y, _, titulo = crear_datos_no_normales(tipo=tipo)\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    residuos = y - lr.predict(x.reshape(-1, 1))\n    \n    # Shapiro-Wilk test\n    shapiro_stat, shapiro_p = stats.shapiro(residuos)\n    \n    # Jarque-Bera test\n    jb_stat, jb_p = jarque_bera(residuos)\n    \n    # Anderson-Darling test\n    ad_stat, ad_critical, ad_significance = anderson(residuos, dist='norm')\n    ad_result = \"Rechaza\" if ad_stat &gt; ad_critical[2] else \"No rechaza\"  # 5% nivel\n    \n    # Conclusi√≥n general\n    tests_reject = sum([shapiro_p &lt; 0.05, jb_p &lt; 0.05, ad_stat &gt; ad_critical[2]])\n    \n    if tests_reject &gt;= 2:\n        conclusion = \"‚ùå No Normal\"\n    elif tests_reject == 1:\n        conclusion = \"‚ö†Ô∏è  Dudoso\"\n    else:\n        conclusion = \"‚úÖ Normal\"\n    \n    print(f\"{tipo:15} | {shapiro_p:10.4f} | {jb_p:12.4f} | {ad_result:&gt;10} | {conclusion}\")\n\nprint(\"\\nüí° Interpretaci√≥n: p-value &lt; 0.05 indica violaci√≥n de normalidad\")\nprint(\"üí° Anderson-Darling: Estad√≠stico &gt; valor cr√≠tico indica no normalidad\")\n\nüß™ PRUEBAS DE NORMALIDAD EN RESIDUOS\n\nH‚ÇÄ: Los residuos siguen distribuci√≥n normal\nH‚ÇÅ: Los residuos NO siguen distribuci√≥n normal\n\nTipo            |    Shapiro |  Jarque-Bera |   Anderson | Conclusi√≥n\n---------------------------------------------------------------------------\nnormal          |     0.7354 |       0.7293 | No rechaza | ‚úÖ Normal\nasimetrico      |     0.0000 |       0.0000 |    Rechaza | ‚ùå No Normal\ncolas_pesadas   |     0.0003 |       0.0000 |    Rechaza | ‚ùå No Normal\nbimodal         |     0.0000 |       0.0010 |    Rechaza | ‚ùå No Normal\n\nüí° Interpretaci√≥n: p-value &lt; 0.05 indica violaci√≥n de normalidad\nüí° Anderson-Darling: Estad√≠stico &gt; valor cr√≠tico indica no normalidad",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#consecuencias-de-violar-los-supuestos",
    "href": "violaciones_supuestos_regresion.html#consecuencias-de-violar-los-supuestos",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "5. Consecuencias de Violar los Supuestos",
    "text": "5. Consecuencias de Violar los Supuestos\nVamos a demostrar qu√© pasa cuando violamos cada supuesto en t√©rminos de: - Precisi√≥n de las predicciones - Validez de los intervalos de confianza - Confiabilidad de las pruebas de hip√≥tesis\n\ndef simular_consecuencias(n_simulaciones=1000, n_datos=100):\n    \"\"\"Simula las consecuencias de violar supuestos\"\"\"\n    \n    resultados = {\n        'correcto': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'no_lineal': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'heteroscedastico': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'no_normal': {'mse': [], 'coef_estimados': [], 'p_values': []}\n    }\n    \n    coef_verdadero = 1.5  # Coeficiente real\n    \n    for _ in range(n_simulaciones):\n        x = np.linspace(0, 10, n_datos)\n        X = x.reshape(-1, 1)\n        \n        # Caso correcto\n        y_correcto = 2 + coef_verdadero * x + np.random.normal(0, 1, n_datos)\n        lr_correcto = LinearRegression().fit(X, y_correcto)\n        \n        # Caso no lineal (usando modelo lineal en datos cuadr√°ticos)\n        y_no_lineal = 2 + 0.5 * x**2 + np.random.normal(0, 1, n_datos)\n        lr_no_lineal = LinearRegression().fit(X, y_no_lineal)\n        \n        # Caso heterosced√°stico\n        y_hetero = 2 + coef_verdadero * x + np.random.normal(0, 0.1 + 0.2 * x, n_datos)\n        lr_hetero = LinearRegression().fit(X, y_hetero)\n        \n        # Caso no normal (distribuci√≥n t)\n        y_no_normal = 2 + coef_verdadero * x + stats.t.rvs(df=3, scale=1, size=n_datos)\n        lr_no_normal = LinearRegression().fit(X, y_no_normal)\n        \n        # Calcular m√©tricas para datos de prueba\n        x_test = np.linspace(0, 10, 50).reshape(-1, 1)\n        y_test_verdadero = 2 + coef_verdadero * x_test.flatten()\n        \n        # MSE en datos de prueba\n        resultados['correcto']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_correcto.predict(x_test))\n        )\n        resultados['no_lineal']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_no_lineal.predict(x_test))\n        )\n        resultados['heteroscedastico']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_hetero.predict(x_test))\n        )\n        resultados['no_normal']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_no_normal.predict(x_test))\n        )\n        \n        # Coeficientes estimados\n        resultados['correcto']['coef_estimados'].append(lr_correcto.coef_[0])\n        resultados['no_lineal']['coef_estimados'].append(lr_no_lineal.coef_[0])\n        resultados['heteroscedastico']['coef_estimados'].append(lr_hetero.coef_[0])\n        resultados['no_normal']['coef_estimados'].append(lr_no_normal.coef_[0])\n    \n    return resultados\n\nprint(\"üîÑ Ejecutando simulaci√≥n (esto puede tomar unos segundos...)\")\nresultados_sim = simular_consecuencias(n_simulaciones=500)\n\n# Crear gr√°ficos de resultados\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Consecuencias de Violar Supuestos de Regresi√≥n', fontsize=16, fontweight='bold')\n\ntipos_casos = ['correcto', 'no_lineal', 'heteroscedastico', 'no_normal']\ncolores = ['green', 'red', 'orange', 'purple']\nnombres = ['Correcto ‚úÖ', 'No Lineal ‚ùå', 'Heterosced√°stico ‚ùå', 'No Normal ‚ùå']\n\n# MSE Distribuci√≥n\nmse_data = [resultados_sim[caso]['mse'] for caso in tipos_casos]\naxes[0, 0].boxplot(mse_data, labels=nombres)\naxes[0, 0].set_title('Distribuci√≥n del Error de Predicci√≥n (MSE)')\naxes[0, 0].set_ylabel('MSE')\naxes[0, 0].tick_params(axis='x', rotation=45)\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribuci√≥n de coeficientes estimados\ncoef_data = [resultados_sim[caso]['coef_estimados'] for caso in tipos_casos]\naxes[0, 1].boxplot(coef_data, labels=nombres)\naxes[0, 1].axhline(y=1.5, color='black', linestyle='--', label='Valor verdadero')\naxes[0, 1].set_title('Distribuci√≥n de Coeficientes Estimados')\naxes[0, 1].set_ylabel('Coeficiente Œ≤‚ÇÅ')\naxes[0, 1].tick_params(axis='x', rotation=45)\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Histograma comparativo de MSE\nfor i, (caso, color, nombre) in enumerate(zip(tipos_casos, colores, nombres)):\n    axes[1, 0].hist(resultados_sim[caso]['mse'], bins=30, alpha=0.6, \n                    color=color, label=nombre, density=True)\naxes[1, 0].set_title('Comparaci√≥n de Distribuciones de MSE')\naxes[1, 0].set_xlabel('MSE')\naxes[1, 0].set_ylabel('Densidad')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Estad√≠sticas resumidas\naxes[1, 1].axis('off')\ntabla_texto = \"RESUMEN ESTAD√çSTICO:\\n\\n\"\ntabla_texto += f\"{'Caso':15} | {'MSE Promedio':&gt;12} | {'Sesgo Œ≤‚ÇÅ':&gt;10}\\n\"\ntabla_texto += \"-\" * 45 + \"\\n\"\n\nfor caso, nombre in zip(tipos_casos, nombres):\n    mse_prom = np.mean(resultados_sim[caso]['mse'])\n    sesgo = np.mean(resultados_sim[caso]['coef_estimados']) - 1.5\n    tabla_texto += f\"{nombre:15} | {mse_prom:12.4f} | {sesgo:10.4f}\\n\"\n\naxes[1, 1].text(0.1, 0.9, tabla_texto, transform=axes[1, 1].transAxes, \n                fontfamily='monospace', fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüìä CONCLUSIONES:\")\nprint(\"‚úÖ Supuestos correctos: Estimaciones insesgadas y predicciones precisas\")\nprint(\"‚ùå No linealidad: Mayor error de predicci√≥n y estimaciones sesgadas\")\nprint(\"‚ùå Heterocedasticidad: Intervalos de confianza incorrectos\")\nprint(\"‚ùå No normalidad: Pruebas de hip√≥tesis no confiables\")\n\nüîÑ Ejecutando simulaci√≥n (esto puede tomar unos segundos...)\n\n\n\n\n\n\n\n\n\n\nüìä CONCLUSIONES:\n‚úÖ Supuestos correctos: Estimaciones insesgadas y predicciones precisas\n‚ùå No linealidad: Mayor error de predicci√≥n y estimaciones sesgadas\n‚ùå Heterocedasticidad: Intervalos de confianza incorrectos\n‚ùå No normalidad: Pruebas de hip√≥tesis no confiables",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#ejercicios-pr√°cticos",
    "href": "violaciones_supuestos_regresion.html#ejercicios-pr√°cticos",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "6. Ejercicios Pr√°cticos",
    "text": "6. Ejercicios Pr√°cticos\n\nEjercicio 1: Identificaci√≥n de Violaciones\nExamina los siguientes conjuntos de datos y determina qu√© supuestos se violan:\n\n# Generar datos de ejercicio\nnp.random.seed(123)\n\ndef generar_datos_ejercicio(tipo, n=150):\n    x = np.linspace(0, 10, n)\n    \n    if tipo == 'A':\n        # M√∫ltiples violaciones\n        y = 2 + 0.5 * x**1.5 + np.random.normal(0, 0.1 + 0.3 * x, n)\n    elif tipo == 'B':\n        # Solo heterocedasticidad\n        y = 1 + 2 * x + np.random.normal(0, 0.5 * np.sqrt(x + 1), n)\n    elif tipo == 'C':\n        # Solo no normalidad\n        y = 3 + 1.5 * x + stats.chi2.rvs(df=2, size=n) - 2\n    elif tipo == 'D':\n        # Datos correctos\n        y = 1 + 2 * x + np.random.normal(0, 1.5, n)\n    \n    return x, y\n\n# Crear ejercicios\nfig, axes = plt.subplots(4, 3, figsize=(18, 20))\nfig.suptitle('EJERCICIO: Identifica las Violaciones de Supuestos', fontsize=16, fontweight='bold')\n\ntipos_ejercicio = ['A', 'B', 'C', 'D']\n\nfor i, tipo in enumerate(tipos_ejercicio):\n    x, y = generar_datos_ejercicio(tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    # Columna 1: Datos y modelo\n    axes[i, 0].scatter(x, y, alpha=0.6)\n    axes[i, 0].plot(x, y_pred, 'r-', linewidth=2)\n    axes[i, 0].set_title(f'Conjunto {tipo}: Datos y Modelo')\n    axes[i, 0].set_xlabel('X')\n    axes[i, 0].set_ylabel('Y')\n    axes[i, 0].grid(True, alpha=0.3)\n    \n    # Columna 2: Residuos vs Predichos\n    axes[i, 1].scatter(y_pred, residuos, alpha=0.6)\n    axes[i, 1].axhline(y=0, color='red', linestyle='--')\n    axes[i, 1].set_title(f'Conjunto {tipo}: Residuos vs Predichos')\n    axes[i, 1].set_xlabel('Valores Predichos')\n    axes[i, 1].set_ylabel('Residuos')\n    axes[i, 1].grid(True, alpha=0.3)\n    \n    # Columna 3: Q-Q Plot\n    stats.probplot(residuos, dist=\"norm\", plot=axes[i, 2])\n    axes[i, 2].set_title(f'Conjunto {tipo}: Q-Q Plot')\n    axes[i, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"ü§î PREGUNTA: ¬øQu√© supuestos se violan en cada conjunto?\")\nprint(\"\\nüìù Analiza cada fila:\")\nprint(\"- Conjunto A: ¬øQu√© patrones observas?\")\nprint(\"- Conjunto B: ¬øLos residuos tienen varianza constante?\")\nprint(\"- Conjunto C: ¬øLos residuos siguen distribuci√≥n normal?\")\nprint(\"- Conjunto D: ¬øEste conjunto cumple todos los supuestos?\")\nprint(\"\\nüí° Pista: Examina los patrones en residuos y la forma del Q-Q plot\")\n\n\n\n\n\n\n\n\nü§î PREGUNTA: ¬øQu√© supuestos se violan en cada conjunto?\n\nüìù Analiza cada fila:\n- Conjunto A: ¬øQu√© patrones observas?\n- Conjunto B: ¬øLos residuos tienen varianza constante?\n- Conjunto C: ¬øLos residuos siguen distribuci√≥n normal?\n- Conjunto D: ¬øEste conjunto cumple todos los supuestos?\n\nüí° Pista: Examina los patrones en residuos y la forma del Q-Q plot",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#resumen-y-recomendaciones",
    "href": "violaciones_supuestos_regresion.html#resumen-y-recomendaciones",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "7. Resumen y Recomendaciones",
    "text": "7. Resumen y Recomendaciones\n\n¬øC√≥mo diagnosticar violaciones?\n\n\n\n\n\n\n\n\nSupuesto\nHerramientas de Diagn√≥stico\nQu√© Buscar\n\n\n\n\nLinealidad\nResiduos vs PredichosGr√°ficos parciales\nPatrones curvosTendencias sistem√°ticas\n\n\nIndependencia\nResiduos vs TiempoAutocorrelaci√≥n\nPatrones temporalesCorrelaci√≥n serial\n\n\nHomocedasticidad\nResiduos vs PredichosTest Breusch-Pagan\nForma de embudop-value &lt; 0.05\n\n\nNormalidad\nQ-Q PlotTest Shapiro-Wilk\nDesviaci√≥n de l√≠nea rectap-value &lt; 0.05\n\n\n\n\n\n¬øQu√© hacer cuando se violan?\n\n\n\n\n\n\n\nViolaci√≥n\nSoluciones Posibles\n\n\n\n\nNo Linealidad\n‚Ä¢ Transformaciones (log, cuadr√°tica)‚Ä¢ Modelos no lineales‚Ä¢ Splines, polinomios\n\n\nDependencia\n‚Ä¢ Modelos de series temporales‚Ä¢ Errores est√°ndar robustos‚Ä¢ GLS con estructura de correlaci√≥n\n\n\nHeterocedasticidad\n‚Ä¢ Transformaciones (log Y)‚Ä¢ Errores est√°ndar robustos‚Ä¢ M√≠nimos cuadrados ponderados\n\n\nNo Normalidad\n‚Ä¢ Transformaciones‚Ä¢ M√©todos no param√©tricos‚Ä¢ Modelos robustos\n\n\n\n\n\nüéØ Puntos Clave para Recordar\n\nLos gr√°ficos de residuos son tu mejor amigo para diagnosticar problemas\nLas violaciones no siempre son fatales - depende de tu objetivo\nPara predicci√≥n: La no normalidad es menos cr√≠tica\nPara inferencia: Todos los supuestos son importantes\nSiempre visualiza antes de modelar y despu√©s de ajustar\n\n¬°Felicidades! üéâ Ahora tienes las herramientas para identificar y entender las violaciones de los supuestos de regresi√≥n lineal.",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  }
]