[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Miner√≠a de Datos",
    "section": "",
    "text": "Temario\n\nIntroducci√≥n al aprendizaje de m√°quina\nPrincipios de aprendizaje supervisado\nRegresi√≥n lineal\nM√©todos de remuestreo y validaci√≥n cruzada\nPrincipios de Regularizaci√≥n\nProblemas de clasificaci√≥n, m√©tricas y evaluaci√≥n\n√Årboles, bosques aleatorios y boosting\nRedes neuronales\nM√©todos no supervisados\n\n\nEvaluaci√≥n\n\nTarea (20%)\nExamen parcial (20%)\nProyecto final (30%):\n\nEntrega (75%)\nExposici√≥n (25%)\n\n\nExamen final (30%)\n\nExistir√° una parte extra a los alumnos que contribuyan al aprendizaje de sus compa√±eros:\n\nContribuciones al repositorio: a√±adiendo redacci√≥n m√°s entendible, a√±adiendo ejemplos particulares a sus carreras, etc.\nActividad en el canal de Slack: contestando dudas de sus compa√±eros, iniciando discusiones para resolver problemas.\n\n\n\nProfesor\nNombre: Sa√∫l Caballero Ram√≠rez\nCorreo: saul.caballero.ramirez@gmail.com\nCorreo alternativo: saul@nixtla.io\nEl canal m√°s r√°pido y efectivo ser√° el siguiente canal de Slack. La idea de este canal es que puedan comunicarse entre ustedes para ayudarse a aprender y si necesitan de mi ayuda intentar√© contestar en un periodo corto de tiempo. Cualquier comportamiento inadecuado dentro de este foro ser√° penalizado por las reglas de convivencia del ITAM.\n\n\nReferencias principales\n\nAn Introduction to Statistical Learning, James et¬†al. (2023)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, y Jonathan Taylor. 2023. An Introduction to Statistical Learning: With Applications in Python. Springer Texts en Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Temario"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html",
    "href": "00-requerimientos-computacion.html",
    "title": "Requerimientos computacionales",
    "section": "",
    "text": "Git\nEste cap√≠tulo proporciona una gu√≠a completa para instalar y configurar las herramientas computacionales necesarias para el curso de Miner√≠a de Datos. Cubriremos la instalaci√≥n y uso b√°sico de Git, GitHub, y Conda en sistemas Windows y macOS.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#git",
    "href": "00-requerimientos-computacion.html#git",
    "title": "Requerimientos computacionales",
    "section": "",
    "text": "¬øQu√© es Git?\nGit es un sistema de control de versiones distribuido que permite rastrear cambios en archivos y coordinar el trabajo entre m√∫ltiples personas. Es esencial para el desarrollo de software y la gesti√≥n de proyectos de an√°lisis de datos.\nCaracter√≠sticas principales: - Control de versiones: mantiene un historial completo de cambios - Trabajo colaborativo: permite que m√∫ltiples personas trabajen en el mismo proyecto - Ramificaci√≥n (branching): facilita el desarrollo de caracter√≠sticas en paralelo - Respaldo distribuido: cada copia del repositorio es un respaldo completo\n\n\nInstalaci√≥n de Git\n\nWindows\n\nDescargar Git:\n\nVisita https://git-scm.com/download/win\nDescarga la versi√≥n m√°s reciente para Windows\n\nInstalaci√≥n:\n\nEjecuta el archivo descargado\nAcepta las opciones por defecto (recomendado para principiantes)\nImportante: aseg√∫rate de seleccionar ‚ÄúGit Bash Here‚Äù durante la instalaci√≥n\n\nVerificar instalaci√≥n:\ngit --version\n\n\n\nmacOS\nOpci√≥n 1: Usando Homebrew (recomendado)\n# Instalar Homebrew si no lo tienes\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Instalar Git\nbrew install git\nOpci√≥n 2: Descarga directa 1. Visita https://git-scm.com/download/mac 2. Descarga e instala el paquete\nOpci√≥n 3: Xcode Command Line Tools\nxcode-select --install\n\n\n\nConfiguraci√≥n inicial de Git\nDespu√©s de instalar Git, configura tu identidad:\ngit config --global user.name \"Tu Nombre\"\ngit config --global user.email \"tu.email@ejemplo.com\"\n\n\nComandos b√°sicos de Git\n\nInicializar un repositorio\n# Crear un nuevo repositorio\ngit init\n\n# Clonar un repositorio existente\ngit clone https://github.com/usuario/repositorio.git\n\n\nOperaciones b√°sicas\n# Ver el estado del repositorio\ngit status\n\n# A√±adir archivos al √°rea de staging\ngit add archivo.py\ngit add .  # A√±adir todos los archivos\n\n# Crear un commit\ngit commit -m \"Mensaje descriptivo del cambio\"\n\n# Ver el historial de commits\ngit log --oneline\n\n\nTrabajar con repositorios remotos\n# Ver repositorios remotos configurados\ngit remote -v\n\n# A√±adir un repositorio remoto\ngit remote add origin https://github.com/usuario/repositorio.git\n\n# Enviar cambios al repositorio remoto\ngit push origin main\n\n# Obtener cambios del repositorio remoto\ngit pull origin main",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#github",
    "href": "00-requerimientos-computacion.html#github",
    "title": "Requerimientos computacionales",
    "section": "GitHub",
    "text": "GitHub\n\n¬øQu√© es GitHub?\nGitHub es una plataforma de desarrollo colaborativo basada en Git que permite: - Hospedar repositorios de c√≥digo - Colaborar en proyectos - Realizar seguimiento de issues y bugs - Automatizar workflows - Crear documentaci√≥n con GitHub Pages\n\n\nCreaci√≥n de cuenta en GitHub\n\nVisita https://github.com\nHaz clic en ‚ÄúSign up‚Äù\nCompleta el formulario de registro\nVerifica tu email\nConfigura tu perfil\n\n\n\nConectar Git local con GitHub\n\nAutenticaci√≥n con Token Personal\n\nCrear un Personal Access Token:\n\nVe a GitHub ‚Üí Settings ‚Üí Developer settings ‚Üí Personal access tokens ‚Üí Tokens (classic)\nGenerate new token (classic)\nSelecciona los permisos necesarios: repo, workflow\nGuarda el token en un lugar seguro\n\nUsar el token:\n# Al hacer push por primera vez, usa tu username y el token como password\ngit push origin main\n\n\n\nAutenticaci√≥n con SSH (recomendado para uso avanzado)\n\nGenerar clave SSH:\nssh-keygen -t ed25519 -C \"tu.email@ejemplo.com\"\nA√±adir la clave a GitHub:\n# Copiar la clave p√∫blica\ncat ~/.ssh/id_ed25519.pub\n\nVe a GitHub ‚Üí Settings ‚Üí SSH and GPG keys ‚Üí New SSH key\nPega la clave p√∫blica\n\nProbar la conexi√≥n:\nssh -T git@github.com",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#pull-requests",
    "href": "00-requerimientos-computacion.html#pull-requests",
    "title": "Requerimientos computacionales",
    "section": "Pull Requests",
    "text": "Pull Requests\n\n¬øQu√© son los Pull Requests?\nLos Pull Requests (PRs) son una funcionalidad de GitHub que permite: - Proponer cambios a un repositorio - Revisar c√≥digo antes de integrarlo - Discutir modificaciones - Mantener un historial de cambios\n\n\nFlujo de trabajo con Pull Requests\n\n1. Crear una nueva rama\n# Crear y cambiar a una nueva rama\ngit checkout -b nueva-caracteristica\n\n# O usando el comando m√°s moderno\ngit switch -c nueva-caracteristica\n\n\n2. Realizar cambios y commits\n# Hacer cambios en tu c√≥digo\n# ...\n\n# A√±adir y commitear cambios\ngit add .\ngit commit -m \"Implementa nueva caracter√≠stica\"\n\n\n3. Subir la rama a GitHub\ngit push origin nueva-caracteristica\n\n\n4. Crear el Pull Request\n\nVe a tu repositorio en GitHub\nHaz clic en ‚ÄúCompare & pull request‚Äù\nA√±ade un t√≠tulo y descripci√≥n clara\nSelecciona los revisores si es necesario\nHaz clic en ‚ÄúCreate pull request‚Äù\n\n\n\n5. Proceso de revisi√≥n\n\nLos revisores pueden a√±adir comentarios\nPuedes hacer cambios adicionales con nuevos commits\nUna vez aprobado, el PR puede ser fusionado\n\n\n\n6. Fusionar y limpiar\n# Despu√©s de que se fusione el PR, actualiza tu rama principal\ngit checkout main\ngit pull origin main\n\n# Elimina la rama local\ngit branch -d nueva-caracteristica",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#conda",
    "href": "00-requerimientos-computacion.html#conda",
    "title": "Requerimientos computacionales",
    "section": "Conda",
    "text": "Conda\n\n¬øQu√© es Conda?\nConda es un gestor de paquetes y entornos de c√≥digo abierto que: - Instala, ejecuta y actualiza paquetes y sus dependencias - Crea, guarda, carga y cambia entre entornos - Funciona con cualquier lenguaje (Python, R, Ruby, Lua, Scala, Java, etc.) - Es multiplataforma (Windows, macOS, Linux)\n\n\nInstalaci√≥n de Conda\n\nOpci√≥n 1: Miniconda (recomendado)\nWindows: 1. Descargar desde https://docs.conda.io/en/latest/miniconda.html 2. Ejecutar el instalador 3. Seguir las instrucciones por defecto 4. Reiniciar la terminal\nmacOS:\n# Descargar e instalar con curl\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\n# Para Apple Silicon (M1/M2)\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nbash Miniconda3-latest-MacOSX-arm64.sh\n\n\nOpci√≥n 2: Anaconda (instalaci√≥n completa)\n\nDescargar desde https://www.anaconda.com/products/distribution\nSeguir las instrucciones de instalaci√≥n para tu sistema operativo\n\n\n\n\nVerificar la instalaci√≥n\nconda --version\nconda info",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#gesti√≥n-de-entornos-con-conda",
    "href": "00-requerimientos-computacion.html#gesti√≥n-de-entornos-con-conda",
    "title": "Requerimientos computacionales",
    "section": "Gesti√≥n de Entornos con Conda",
    "text": "Gesti√≥n de Entornos con Conda\n\n¬øPor qu√© usar entornos?\nLos entornos virtuales permiten: - Aislar dependencias entre proyectos - Usar diferentes versiones de Python/paquetes - Evitar conflictos entre bibliotecas - Reproducir entornos de trabajo\n\n\nComandos b√°sicos para entornos\n\nCrear entornos\n# Crear un entorno con Python espec√≠fico\nconda create -n mi_entorno python=3.10\n\n# Crear un entorno con paquetes espec√≠ficos\nconda create -n ciencia_datos python=3.10 numpy pandas matplotlib\n\n# Crear entorno desde un archivo\nconda env create -f environment.yml\n\n\nActivar y desactivar entornos\n# Activar un entorno\nconda activate mi_entorno\n\n# Desactivar el entorno actual\nconda deactivate\n\n# Listar entornos disponibles\nconda env list\n\n\nGestionar paquetes en entornos\n# Instalar paquetes\nconda install numpy pandas scikit-learn\n\n# Instalar desde conda-forge (recomendado)\nconda install -c conda-forge seaborn\n\n# Instalar m√∫ltiples paquetes\nconda install jupyter matplotlib seaborn\n\n# Listar paquetes instalados\nconda list\n\n# Actualizar paquetes\nconda update numpy\nconda update --all\n\n\nEliminar entornos\n# Eliminar un entorno\nconda env remove -n mi_entorno\n\n\n\nArchivo environment.yml\nPara reproducir entornos, crea un archivo environment.yml:\nname: mineria_datos\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - jupyter\n  - numpy\n  - pandas\n  - matplotlib\n  - seaborn\n  - scikit-learn\n  - pip\n  - pip:\n    - some-pip-package\nCrear el entorno desde el archivo:\nconda env create -f environment.yml\n\n\nMejores pr√°cticas\n\nUn entorno por proyecto: Crea un entorno espec√≠fico para cada proyecto\nUsa conda-forge: Preferir el canal conda-forge para paquetes actualizados\nExporta tus entornos: Mant√©n archivos environment.yml para reproducibilidad\nActualiza regularmente: Mant√©n tus entornos actualizados\nDocumenta dependencias: Incluye versiones espec√≠ficas cuando sea cr√≠tico\n\n\n\nSoluci√≥n de problemas comunes\n\nProblema: Conda no se reconoce en la terminal\nSoluci√≥n: Reinicia la terminal o a√±ade conda al PATH:\n# Windows (en Command Prompt como administrador)\nconda init cmd.exe\n\n# macOS/Linux\nconda init bash\n\n\nProblema: Conflictos de paquetes\nSoluci√≥n: Usa mamba (m√°s r√°pido) o especifica canales:\n# Instalar mamba\nconda install mamba -n base -c conda-forge\n\n# Usar mamba en lugar de conda\nmamba install numpy pandas\n\n\nProblema: Entorno no se activa\nSoluci√≥n: Verifica la instalaci√≥n y configuraci√≥n:\nconda info --envs\nconda config --show",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#configuraci√≥n-de-conda-en-diferentes-entornos",
    "href": "00-requerimientos-computacion.html#configuraci√≥n-de-conda-en-diferentes-entornos",
    "title": "Requerimientos computacionales",
    "section": "Configuraci√≥n de Conda en diferentes entornos",
    "text": "Configuraci√≥n de Conda en diferentes entornos\n\nConda en Windows con Git Bash\nGit Bash en Windows puede requerir configuraci√≥n adicional para funcionar correctamente con Conda:\n\n1. Configuraci√≥n inicial en Git Bash\nDespu√©s de instalar Miniconda en Windows, es posible que conda no est√© disponible en Git Bash por defecto:\n# Verificar si conda est√° disponible\nconda --version\nSi no funciona, necesitas inicializar conda para Git Bash:\n# Navega al directorio de instalaci√≥n de Miniconda (ajusta la ruta seg√∫n tu instalaci√≥n)\ncd /c/Users/TuUsuario/miniconda3/Scripts\n\n# Inicializar conda para bash\n./conda.exe init bash\n\n\n2. Alternativa: A√±adir conda al PATH manualmente\nSi la inicializaci√≥n no funciona, a√±ade conda manualmente al PATH en Git Bash:\n# A√±adir estas l√≠neas a tu archivo ~/.bashrc\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3:$PATH\"' &gt;&gt; ~/.bashrc\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3/Scripts:$PATH\"' &gt;&gt; ~/.bashrc\n\n# Recargar el archivo bashrc\nsource ~/.bashrc\n\n\n3. Verificar la configuraci√≥n\n# Reiniciar Git Bash y verificar\nconda --version\nconda info\n\n\n4. Trabajar con entornos en Git Bash\n# Crear y activar entorno\nconda create -n mineria_datos python=3.10\nconda activate mineria_datos\n\n# Si aparece un error de activaci√≥n, usa:\nsource activate mineria_datos\n\n\n\nTroubleshooting espec√≠fico para Windows\n\nProblema: ‚Äúconda: command not found‚Äù en Git Bash\nSoluciones:\n\nReinstalar Miniconda con ‚ÄúAdd to PATH‚Äù marcado\nUsar el Anaconda Prompt (m√°s confiable en Windows)\nConfigurar manualmente el PATH:\n\n# En ~/.bashrc, a√±ade:\nalias conda='/c/Users/TuUsuario/miniconda3/Scripts/conda.exe'\nalias python='/c/Users/TuUsuario/miniconda3/python.exe'\n\n\nProblema: Activaci√≥n de entorno no funciona\n# En lugar de conda activate, usa:\nsource /c/Users/TuUsuario/miniconda3/etc/profile.d/conda.sh\nconda activate mineria_datos",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#visual-studio-code",
    "href": "00-requerimientos-computacion.html#visual-studio-code",
    "title": "Requerimientos computacionales",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\n¬øPor qu√© usar VS Code para ciencia de datos?\nVisual Studio Code es un editor de c√≥digo ligero pero potente que ofrece: - Soporte excelente para Python y Jupyter notebooks - Integraci√≥n nativa con conda y entornos virtuales - Extensiones espec√≠ficas para ciencia de datos - Terminal integrado - Control de versiones Git integrado - IntelliSense y debugging avanzado\n\n\nInstalaci√≥n de Visual Studio Code\n\nWindows y macOS\n\nDescargar desde https://code.visualstudio.com/\nEjecutar el instalador siguiendo las instrucciones por defecto\nReiniciar el sistema si es necesario\n\n\n\n\nExtensiones esenciales para ciencia de datos\nInstala estas extensiones desde el marketplace de VS Code (Ctrl/Cmd + Shift + X):\n\nExtensiones obligatorias:\n- Python (Microsoft)\n- Jupyter (Microsoft)\n- Python Debugger (Microsoft)\n\n\nExtensiones recomendadas:\n- GitLens ‚Äî Git supercharged\n- Pylance (an√°lisis avanzado de Python)\n- autoDocstring - Python Docstring Generator\n- Python Indent\n- Bracket Pair Colorizer\n- Data Wrangler (Microsoft)\n\n\n\nConfiguraci√≥n de Python y Conda en VS Code\n\n1. Seleccionar el int√©rprete de Python\n\nAbre VS Code\nPresiona Ctrl+Shift+P (Windows) o Cmd+Shift+P (macOS)\nEscribe ‚ÄúPython: Select Interpreter‚Äù\nSelecciona el int√©rprete del entorno mineria_datos\n\nLa ruta deber√≠a ser similar a: - Windows: C:\\Users\\TuUsuario\\miniconda3\\envs\\mineria_datos\\python.exe - macOS: /Users/TuUsuario/miniconda3/envs/mineria_datos/bin/python\n\n\n2. Verificar la configuraci√≥n\nCrea un archivo de prueba test.py:\nimport sys\nprint(f\"Python path: {sys.executable}\")\nprint(f\"Python version: {sys.version}\")\n\n# Verificar paquetes\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nprint(\"¬°Todos los paquetes importados correctamente!\")\n\n\n3. Configurar terminal integrado\nEn VS Code, abre el terminal integrado (Ctrl+`` o View ‚Üí Terminal) y configura conda:\nWindows:\n# Si usas Git Bash en VS Code\nconda activate mineria_datos\nmacOS:\nconda activate mineria_datos\n\n\n\nTrabajar con Jupyter Notebooks en VS Code\n\n1. Crear un nuevo notebook\n\nCtrl+Shift+P ‚Üí ‚ÄúJupyter: Create New Jupyter Notebook‚Äù\nO crear un archivo con extensi√≥n .ipynb\n\n\n\n2. Seleccionar kernel\n\nEn la esquina superior derecha del notebook, haz clic en ‚ÄúSelect Kernel‚Äù\nSelecciona ‚ÄúPython Environments‚Äù\nEscoge el entorno mineria_datos\n\n\n\n3. Verificar configuraci√≥n del notebook\n# Primera celda del notebook\nimport sys\nprint(f\"Ejecut√°ndose en: {sys.executable}\")\n\n# Importar bibliotecas del curso\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nprint(\"Notebook configurado correctamente para el curso!\")\n\n\n\nConfiguraci√≥n avanzada de VS Code\n\nConfiguraci√≥n del workspace\nCrea un archivo .vscode/settings.json en tu directorio del proyecto:\n{\n    \"python.defaultInterpreterPath\": \"./mineria_datos/bin/python\",\n    \"python.terminal.activateEnvironment\": true,\n    \"jupyter.defaultKernel\": \"mineria_datos\",\n    \"files.associations\": {\n        \"*.qmd\": \"markdown\"\n    },\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"editor.formatOnSave\": true\n}\n\n\nAtajos de teclado √∫tiles\n\nCtrl+Shift+P: Command Palette\nCtrl+``: Toggle Terminal\nShift+Enter: Ejecutar celda de notebook\nCtrl+Enter: Ejecutar celda sin avanzar\nF5: Iniciar debugging\nCtrl+Shift+G: Control de versiones Git\n\n\n\n\nIntegraci√≥n con Git en VS Code\n\n1. Clonar repositorio\n\nCtrl+Shift+P ‚Üí ‚ÄúGit: Clone‚Äù\nPega la URL del repositorio\nSelecciona la carpeta destino\n\n\n\n2. Operaciones Git b√°sicas\n\nSource Control panel (Ctrl+Shift+G): Ver cambios\nStage changes: Hacer clic en el ‚Äú+‚Äù junto a los archivos\nCommit: Escribir mensaje y presionar Ctrl+Enter\nPush/Pull: Usar los botones en la barra de estado\n\n\n\n3. Crear branches\n\nClic en el nombre de la branch en la barra de estado\n‚ÄúCreate new branch‚Äù\nEscribir el nombre de la nueva branch",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#configuraci√≥n-completa-del-entorno-para-el-curso",
    "href": "00-requerimientos-computacion.html#configuraci√≥n-completa-del-entorno-para-el-curso",
    "title": "Requerimientos computacionales",
    "section": "Configuraci√≥n completa del entorno para el curso",
    "text": "Configuraci√≥n completa del entorno para el curso\n\nPasos de configuraci√≥n paso a paso\n\n1. Instalar las herramientas base\n\nGit (siguiendo las instrucciones de instalaci√≥n seg√∫n tu sistema operativo)\nConda (Miniconda recomendado)\nVisual Studio Code (opcional pero recomendado)\n\n\n\n2. Crear y configurar el entorno de conda\nEn cualquier terminal (Command Prompt, Git Bash, Terminal de macOS):\n# Crear el entorno del curso\nconda create -n mineria_datos python=3.10\n\n# Activar el entorno\nconda activate mineria_datos\nSi tienes problemas con conda activate en Windows Git Bash:\n# Alternativa para Windows Git Bash\nsource activate mineria_datos\n\n\n3. Instalar paquetes necesarios\nOpci√≥n recomendada (con mamba para mayor velocidad):\n# Instalar mamba primero\nconda install mamba -c conda-forge\n\n# Instalar todos los paquetes del curso\nmamba install jupyter numpy pandas matplotlib seaborn scikit-learn nbclient ipykernel pyyaml plotly -y\nOpci√≥n alternativa (solo con conda):\nconda install jupyter numpy pandas matplotlib seaborn scikit-learn nbclient ipykernel pyyaml plotly -y\n\n\n4. Configurar Jupyter para el entorno\n# Registrar el entorno como kernel de Jupyter\npython -m ipykernel install --user --name mineria_datos --display-name \"Python (Miner√≠a de Datos)\"\n\n# Verificar que el kernel se registr√≥ correctamente\njupyter kernelspec list\n\n\n5. Verificar la instalaci√≥n\n# Probar que todos los paquetes se importan correctamente\npython -c \"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport plotly\nprint('‚úÖ ¬°Todos los paquetes instalados correctamente!')\nprint(f'Python version: {pd.__version__}')\nprint(f'Pandas version: {pd.__version__}')\nprint(f'NumPy version: {np.__version__}')\nprint(f'Scikit-learn version: {sklearn.__version__}')\n\"\n\n\n\nConfiguraci√≥n espec√≠fica por entorno\n\nPara usuarios de Windows Git Bash\nSi planeas usar Git Bash como tu terminal principal:\n# A√±adir conda al PATH permanentemente\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3/Scripts:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Verificar configuraci√≥n\nconda info\n\n\nPara usuarios de Visual Studio Code\n\nInstalar VS Code y extensiones:\n\nPython (Microsoft)\nJupyter (Microsoft)\nPython Debugger (Microsoft)\n\nConfigurar el int√©rprete:\n\nCtrl+Shift+P ‚Üí ‚ÄúPython: Select Interpreter‚Äù\nSeleccionar el int√©rprete del entorno mineria_datos\n\nCrear archivo de configuraci√≥n del workspace:\n\nCrea .vscode/settings.json en tu directorio de proyecto:\n{\n    \"python.defaultInterpreterPath\": \"~/miniconda3/envs/mineria_datos/bin/python\",\n    \"python.terminal.activateEnvironment\": true,\n    \"jupyter.defaultKernel\": \"mineria_datos\",\n    \"files.associations\": {\n        \"*.qmd\": \"markdown\"\n    }\n}\n\n\nPara usuarios de Jupyter Lab (opcional)\nSi prefieres usar JupyterLab en lugar de notebooks en VS Code:\n# Instalar JupyterLab\nconda install jupyterlab\n\n# Iniciar JupyterLab\njupyter lab\n\n\n\nClonar el repositorio del curso\nUna vez configurado tu entorno, clona el repositorio del curso:\n# Clonar el repositorio (sustituye por la URL real del curso)\ngit clone [URL_DEL_REPOSITORIO_DEL_CURSO]\n\n# Navegar al directorio\ncd nombre-del-repositorio\n\n# Activar el entorno\nconda activate mineria_datos\n\n# Si hay un archivo environment.yml, √∫salo para instalar dependencias adicionales\nconda env update -f environment.yml\n\n\nFlujo de trabajo recomendado\n\nIniciar sesi√≥n de trabajo:\n# Activar entorno\nconda activate mineria_datos\n\n# Navegar al directorio del proyecto\ncd ruta/al/proyecto\n\n# Abrir VS Code (si lo usas)\ncode .\nPara cada tarea/proyecto:\n\nCrear una nueva branch en Git\nTrabajar en tus notebooks/scripts\nHacer commits regularmente\nCrear Pull Request cuando est√© listo\n\nMantener el entorno actualizado:\n# Actualizar paquetes peri√≥dicamente\nconda update --all\n\n\n\nVerificaci√≥n final\nEjecuta este script para verificar que todo est√° configurado correctamente:\n# test_setup.py\nimport sys\nimport subprocess\n\ndef test_environment():\n    print(\"üîç Verificando configuraci√≥n del entorno...\")\n    \n    # Verificar Python\n    print(f\"‚úÖ Python: {sys.version}\")\n    print(f\"‚úÖ Ejecutable: {sys.executable}\")\n    \n    # Verificar paquetes cr√≠ticos\n    required_packages = [\n        'pandas', 'numpy', 'matplotlib', 'seaborn', \n        'sklearn', 'jupyter', 'plotly'\n    ]\n    \n    for package in required_packages:\n        try:\n            __import__(package)\n            print(f\"‚úÖ {package}: disponible\")\n        except ImportError:\n            print(f\"‚ùå {package}: NO disponible\")\n    \n    # Verificar conda\n    try:\n        result = subprocess.run(['conda', '--version'], \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"‚úÖ Conda: {result.stdout.strip()}\")\n        else:\n            print(\"‚ùå Conda: no disponible\")\n    except FileNotFoundError:\n        print(\"‚ùå Conda: no encontrado en PATH\")\n    \n    # Verificar git\n    try:\n        result = subprocess.run(['git', '--version'], \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"‚úÖ Git: {result.stdout.strip()}\")\n        else:\n            print(\"‚ùå Git: no disponible\")\n    except FileNotFoundError:\n        print(\"‚ùå Git: no encontrado en PATH\")\n    \n    print(\"\\nüéâ ¬°Verificaci√≥n completa!\")\n\nif __name__ == \"__main__\":\n    test_environment()\nGuarda este script como test_setup.py y ejec√∫talo:\npython test_setup.py\n¬°Con esta configuraci√≥n completa, estar√°s listo para trabajar eficientemente en todos los proyectos del curso de Miner√≠a de Datos!",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "Introducci√≥n",
    "section": "",
    "text": "¬øQu√© es aprendizaje de m√°quina?\nM√©todos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempe√±o en alguna tarea o toma de decisi√≥n.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe tambi√©n aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisi√≥n afecta directa e inmediatamente al entorno.\nLas tareas m√°s apropiadas para este enfoque, en general, son aquellas en donde:",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#qu√©-es-aprendizaje-de-m√°quina",
    "href": "01-introduccion.html#qu√©-es-aprendizaje-de-m√°quina",
    "title": "Introducci√≥n",
    "section": "",
    "text": "Existe una cantidad considerable de datos relevantes para aprender a ejecutar la tarea.\nEl costo por errores al ejecutar la tarea es relativamente bajo (al menos comparado con alternativas).\nLa tarea se repite de manera m√°s o menos homog√©nea una cantidad grande de veces.\n\n\nEjemplos de tareas de aprendizaje:\n\nPredecir si un cliente de tarjeta de cr√©dito va a caer en impago en los pr√≥ximos doce meses.\nEstimar el ingreso mensual de un hogar a partir de las caracter√≠sticas de la vivienda, posesiones y equipamiento y localizaci√≥n geogr√°fica.\nDividir a los clientes de Netflix seg√∫n sus gustos.\nRecomendar art√≠culos a clientes de un programa de lealtad o servicio online.\nReconocer un tipos de documentos (identificaci√≥n, comprobante de domicilio, comprobante de ingresos) para acelerar el proceso de evaluaci√≥n de cr√©dito.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisi√©ramos obtener una respuesta barata, r√°pida, automatizada, y con suficiente precisi√≥n. Por ejemplo, reconocer caracteres en una placa de coche de una fotograf√≠a se puede hacer por personas, pero eso es lento y costoso. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisi√©ramos superar el desempe√±o actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisi√≥n de dar o no un pr√©stamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender m√°s del problema que nos interesa: estas soluciones forman parte de un ciclo de an√°lisis de datos donde podemos aprender de una forma m√°s concentrada cu√°les son caracter√≠sticas y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen est√°n vac√≠os, entonces es un cero, si el cr√©dito total es mayor al 50% del ingreso anual, declinar el pr√©stamo, etc). Las razones para no tomar un enfoque de reglas construidas ‚Äúa mano‚Äù:\n\nCuando conjuntos de reglas creadas a mano se desempe√±an mal (por ejemplo, para otorgar cr√©ditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser dif√≠ciles de mantener (por ejemplo, un corrector ortogr√°fico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¬øqu√© b√∫squedas www se enfocan en dar direcciones como resultados? ¬øc√≥mo filtrar comentarios no aceptables en foros?\nFinalmente, notamos que en estos problemas nuestro inter√©s principal no es entender qu√© variables influyen en otras (en el proceso natural o de negocio). Sin m√°s teor√≠a o dise√±o de datos, los m√©todos que utilizaremos explotan patrones en los datos que no necesariamente explican c√≥mo funcionan los sistemas de inter√©s.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "Introducci√≥n",
    "section": "Aprendizaje supervisado y no supervisado",
    "text": "Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fen√≥meno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de cr√©dito, utilizamos datos hist√≥ricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresi√≥n: cuando la salida es una variable num√©rica. El ejemplo de estimaci√≥n de ingreso es un problema de regresi√≥n\nProblemas de clasificaci√≥n: cuando la salida es una variable categ√≥rica. El ejemplo de detecci√≥n de d√≠gitos escritos a manos es un problema de clasificaci√≥n.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos m√°s vagos, y por lo mismo pueden ser m√°s dif√≠ciles.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introducci√≥n</span>"
    ]
  },
  {
    "objectID": "02-principios.html",
    "href": "02-principios.html",
    "title": "Principios de aprendizaje supervisado",
    "section": "",
    "text": "Definici√≥n de aprendizaje supervisado\nSupongamos que observamos una variable cuantitativa \\(Y \\in \\mathbb{R}\\) y tenemos \\(p\\) variables predictoras, \\(X_1, X_2, ..., X_p\\), las cuales denotaremos como \\(X = (X_1, X_2, ..., X_p)\\). Supongamos que existe alguna reluaci√≥n entre ellas y se puede expresar de la siguiente forma:\n\\[ Y = f(x) + \\epsilon\\]\nLa tarea del aprendizaje supervisado es aprender la funci√≥n \\(f\\). Existen dos razones por las cuales estimar \\(f\\): predicci√≥n e inferencia.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#definici√≥n-de-aprendizaje-supervisado",
    "href": "02-principios.html#definici√≥n-de-aprendizaje-supervisado",
    "title": "Principios de aprendizaje supervisado",
    "section": "",
    "text": "Funci√≥n \\(f\\): funci√≥n desconocida que relaciona a \\(X\\) con \\(Y\\). Representa la informaci√≥n sist√©mica que \\(X\\) aporta a \\(Y\\).\nError \\(\\epsilon\\): representa qu√© tan equivocados estamos con respecto al verdadero valor de \\(Y\\).\n\n\n\nPredicci√≥n\nEn muchas ocasiones existen un conjunto de variables \\(X\\) que est√°n listas para aprovecharse, sin embargo, puede que no se pueda obtener la variable \\(Y\\) de manera inmediata. En este sentido, podemos predecir la variable \\(Y\\) siguiendo la ecuaci√≥n:\n\\[\\hat{Y} = \\hat{f}(X)\\]\ndonde \\(\\hat{f}\\) representa nuestro estimador de \\(f\\) y \\(\\hat{Y}\\) es nuestra predicci√≥n de \\(Y\\). En este sentido \\(\\hat{f}\\) es una caja negra en el sentido en el que no nos preocupa cu√°l es la funci√≥n, sino que provee predicciones precisas para \\(Y\\).\nLa precisi√≥n de \\(\\hat{Y}\\) depende de dos cantidades:\n\nError reducible: En general, \\(\\hat{f}\\) no ser√° un estimador perfecto de \\(f\\) y esto introducir√° un error el cu√°l puede reducirse. Ejemplos: Introducir una estructura lineal cu√°ndo el problema tiene estructura cuadr√°tica, falta de variables explicativas, exceso de variables que no contribuyen a la predicci√≥n.\nError ireducible: La variable \\(Y\\) es una funci√≥n tambi√©n de \\(\\epsilon\\) y por definici√≥n nuestra predicci√≥n tendra un error inherente. Ejemplos: Predecir que comer√°n ma√±ana, determinar si llover√° o no, determinar cu√°ndo ocurrir√° un temblor, ¬øqui√©n ganar√° una elecci√≥n?.\n\n\\[ \\begin{align*}\n\\mathbb{E}[(Y-\\hat{Y})^2] &= \\mathbb{E}[(f(X) + \\epsilon -\\hat{f}(x))^2]\\\\\n&= \\underset{Reducible}{\\underbrace{\\mathbb{E}[(f(X) - \\hat{f}(x))^2]}} + \\underset{Irreducible}{\\underbrace{\\text{Var}(\\epsilon)}}\n\\end{align*}\\]\nEl objetivo del curso se enfoca en t√©cnicas para estimar \\(f\\) con el objectivo de minimizar el error reducible. Es importante tener en cuenta que el error irreducible siempre nos pondr√° una cota en la predicci√≥n de \\(Y\\).\n\n\nInferencia\nExisten problemas en donde nos interesa m√°s entender la relaci√≥n intrinseca que existe entre \\(Y\\) y \\(X\\). En esta situaci√≥n nuestro objetivo no es hacer predicci√≥n, entonces \\(\\hat{f}\\) ya no puede ser tratada como una caja negra. En este tipo de enfoque se contestan preguntas c√≥mo:\n\n¬øCu√°les son los predictores que se asocian con la variable \\(Y\\)?: Muchas veces solo un subconjunto de los datos \\(X\\) son los que realmente est√°n relacionados con \\(Y\\).\n¬øCu√°l es la relaci√≥n entre \\(Y\\) y \\(X_i\\)?\n¬øLa relaci√≥n entre \\(Y\\) y \\(X_i\\) es lineal o m√°s compleja?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#c√≥mo-estimar-f",
    "href": "02-principios.html#c√≥mo-estimar-f",
    "title": "Principios de aprendizaje supervisado",
    "section": "¬øC√≥mo estimar \\(f\\)?",
    "text": "¬øC√≥mo estimar \\(f\\)?\nAsumiremos que tenemos \\(n\\) datos diferentes estas observaciones ser√°n llamadas conjunto de entrenamiento. \\(x_{ij}\\) representa el valor del predictor \\(j\\) para la observaci√≥n \\(i\\), donde \\(i=1,2,...,n\\) y \\(j=1,2,...,p\\). \\(y_i\\) representa la variable respuesta de la observaci√≥n \\(i\\). Entonces nuestro conjunto de entrenamiento consiste en:\n\\[{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}\\]\ndonde \\(x_i=(x_{i1}, x_{i2}, ..., x_{ip})^T\\).\nNuestro objetivo es aplicar un m√©todo de aprendizaje en el conjunto de datos para poder estimar una funci√≥n desconocida de \\(f\\). Nos encantar√≠a encontrar una funci√≥n \\(\\hat{f}\\) de forma tal que \\(Y\\simeq \\hat{f}(X)\\) para cualquier observaci√≥n \\((X, Y)\\). Muchos de estos enfoque se pueden caracterizar como m√©todos param√©tricos o no param√©tricos.\n\nM√©todos param√©tricos\nLos m√©todos param√©tricos involucran un enfoque de dos pasos:\n\nHacemos un supuesto de la forma funci√≥n de \\(f\\). Por ejemplo, la m√°s sencilla es que \\(f\\) es linear en \\(\\beta\\):\n\n\\[ f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]\nUna vez haciendo haciendo el supuesto de linealidad el problema de estimar \\(f\\) es simplificado ya que en lugar de explorar el espacio funcional uno solo necesita estimar \\(p+1\\) coeficientes \\(\\beta_0, ..., \\beta_p\\).\n\nNecesitamos un proceso que utilice los datos de entrenamiento para ajustar u entrenar el modelo. El enfoque m√°s sencillo es el m√©todo de m√≠nimos cuadrados ordinarios (OLS):\n\n\\[\\underset{\\beta_0, \\beta_1, ..., \\beta_p}{min} \\sum_{i=1}^{N}(y_i - (\\beta_0 + \\beta_1 x_{i1} +\\beta_2 x_{i2} + ... + \\beta_p X_p))^2\\]\nEl enfoque basado en modelado se refiere a los modelos param√©tricos; reduce el problema de estimar \\(f\\) a estimar un conjunto de par√°metros. La desventaja potencial es que el modelo podr√≠a no ser igual a la verdadera \\(f\\) y tendremos malas estimaciones del valor de \\(y\\).\n\n\nM√©todos no param√©tricos",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html",
    "href": "03-regresion_lineal.html",
    "title": "Regresi√≥n lineal",
    "section": "",
    "text": "Regresi√≥n Lineal Simple\nComenzaremos con el caso m√°s sencillo: predecir una variable de resultado Y a partir de una √∫nica variable predictora X.\nEl modelo matem√°tico que queremos ajustar es una l√≠nea recta:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nDonde:\nNuestro objetivo üéØ es encontrar los mejores valores posibles para los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) usando los datos que tenemos.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresi√≥n-lineal-simple",
    "href": "03-regresion_lineal.html#regresi√≥n-lineal-simple",
    "title": "Regresi√≥n lineal",
    "section": "",
    "text": "\\(Y\\): La variable dependiente (lo que queremos predecir).\n\\(X\\): La variable independiente (nuestro predictor).\n\\(\\beta_0\\): El intercepto (el valor de \\(Y\\) cuando \\(X=0\\)).\n\\(\\beta_1\\): La pendiente (cu√°nto cambia \\(Y\\) por cada unidad que aumenta \\(X\\)).\n\\(\\epsilon\\): El t√©rmino de error (la parte de \\(Y\\) que nuestro modelo no puede explicar).\n\n\n\n¬øC√≥mo estimamos los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\)?\n‚ÄúMejor‚Äù para nosotros significa encontrar la l√≠nea que minimice la distancia vertical entre cada punto de dato y la propia l√≠nea. Espec√≠ficamente, minimizamos la Suma de los Errores al Cuadrado (SEC o Sum of Squared Errors, SSE).\nLa funci√≥n de costo (o p√©rdida) que queremos minimizar es:\n\\[J(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\\]\nTenemos dos m√©todos principales para encontrar los \\(\\beta\\) que minimizan esta funci√≥n:\n\nM√©todo 1: Las Ecuaciones Normales (La soluci√≥n anal√≠tica üß†)\nEste m√©todo utiliza c√°lculo para encontrar el m√≠nimo exacto de la funci√≥n de costo. Para ello, tomamos las derivadas parciales de \\(J\\) con respecto a \\(\\beta_0\\) y \\(\\beta_1\\), las igualamos a cero y resolvemos para los coeficientes.\n\n\n\n\n\n\nDerivada parcial con respecto a \\(\\beta_0\\):\n\n\n\n\n\n\\[\\frac{\\partial J}{\\partial \\beta_0} = \\sum_{i=1}^{n} -2(y_i - \\beta_0 - \\beta_1 x_i) = 0\\] \\[\\sum y_i - n\\beta_0 - \\beta_1 \\sum x_i = 0\\] \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\n\n\n\n\n\n\n\n\n\nDerivada parcial con respecto a \\(\\beta_1\\)\n\n\n\n\n\n\\[\\frac{\\partial J}{\\partial \\beta_1} = \\sum_{i=1}^{n} -2x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0\\] Sustituyendo \\(\\beta_0\\) de la primera ecuaci√≥n y resolviendo, llegamos a: \\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\n\n\n\nEstas f√≥rmulas nos dan los valores √≥ptimos y exactos de los coeficientes directamente a partir de los datos.\n\n\nM√©todo 2: Descenso en Gradiente (La soluci√≥n iterativa ‚öôÔ∏è)\nEste es un m√©todo computacional que nos ‚Äúacerca‚Äù progresivamente a la soluci√≥n. Es especialmente √∫til cuando tenemos una cantidad masiva de datos y calcular la soluci√≥n anal√≠tica es muy costoso.\nLa intuici√≥n: Imagina que est√°s en una monta√±a (la funci√≥n de costo) y quieres llegar al valle (el costo m√≠nimo). El Descenso en Gradiente te dice que mires a tu alrededor y des un paso en la direcci√≥n m√°s inclinada hacia abajo. Repites esto hasta llegar al fondo.\nEl algoritmo funciona as√≠:\n\nInicializa los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) con valores aleatorios (o en ceros).\nCalcula el gradiente de la funci√≥n de costo. El gradiente es un vector que apunta en la direcci√≥n del m√°ximo ascenso. Nosotros iremos en la direcci√≥n opuesta.\n\n\\(\\frac{\\partial J}{\\partial \\beta_0} = -2 \\sum (y_i - (\\beta_0 + \\beta_1 x_i))\\)\n\\(\\frac{\\partial J}{\\partial \\beta_1} = -2 \\sum x_i(y_i - (\\beta_0 + \\beta_1 x_i))\\)\n\nActualiza los coeficientes usando una tasa de aprendizaje (\\(\\alpha\\)), que controla el tama√±o del paso que damos.\n\n\\(\\beta_0 := \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0}\\)\n\\(\\beta_1 := \\beta_1 - \\alpha \\frac{\\partial J}{\\partial \\beta_1}\\)\n\nRepite los pasos 2 y 3 durante un n√∫mero determinado de iteraciones o hasta que el cambio en el costo sea muy peque√±o (convergencia).\n\n\n\n\n\n\n\nExplicacion visual",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#cu√°les-son-los-supuestos-de-la-regresi√≥n",
    "href": "03-regresion_lineal.html#cu√°les-son-los-supuestos-de-la-regresi√≥n",
    "title": "Regresi√≥n lineal",
    "section": "¬øCu√°les son los supuestos de la regresi√≥n? üßê",
    "text": "¬øCu√°les son los supuestos de la regresi√≥n? üßê\nPara que nuestro modelo sea confiable (es decir, para que los coeficientes y las predicciones tengan sentido), debemos cumplir con ciertos supuestos.\n\nLinealidad: La relaci√≥n entre \\(\\beta\\) y \\(Y\\) debe ser lineal.\n\n¬øPara qu√© sirve? Si la relaci√≥n no es lineal, nuestro modelo de l√≠nea recta ser√° intr√≠nsecamente incorrecto.\n\nIndependencia de los errores: Los errores (residuos) no deben estar correlacionados entre s√≠.\n\n¬øPara qu√© sirve? Es crucial para datos de series temporales. Si los errores est√°n correlacionados, la informaci√≥n de un error nos da pistas sobre el siguiente, lo cual viola la idea de que cada observaci√≥n es independiente.\n\nHomocedasticidad (Varianza constante de los errores): La varianza de los errores debe ser constante para todos los niveles de \\(X\\).\n\n¬øPara qu√© sirve? Si la varianza cambia (heterocedasticidad), nuestras predicciones ser√°n mejores para algunas partes de los datos que para otras, y los intervalos de confianza para los coeficientes ser√°n poco fiables. Visualmente, en un gr√°fico de residuos vs.¬†valores predichos, no queremos ver una forma de cono o embudo.\n\nNormalidad de los errores: Los errores deben seguir una distribuci√≥n normal con media cero.\n\n¬øPara qu√© sirve? Este supuesto es fundamental para poder realizar pruebas de hip√≥tesis sobre los coeficientes (como los p-values) y construir intervalos de confianza. Podemos verificarlo con un histograma de los residuos o un gr√°fico Q-Q.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#c√≥mo-evaluar-la-precisi√≥n-del-modelo",
    "href": "03-regresion_lineal.html#c√≥mo-evaluar-la-precisi√≥n-del-modelo",
    "title": "Regresi√≥n lineal",
    "section": "¬øC√≥mo evaluar la precisi√≥n del modelo? üìà",
    "text": "¬øC√≥mo evaluar la precisi√≥n del modelo? üìà\nUna vez que hemos ajustado el modelo, ¬øc√≥mo sabemos si es bueno?\n\nCoeficiente de Determinaci√≥n (\\(R^2\\))\nEl \\(R^2\\) mide la proporci√≥n de la varianza total en la variable dependiente (\\(Y\\)) que es explicada por nuestro modelo.\n\\[R^2 = 1 - \\frac{\\text{Suma de Errores al Cuadrado (SEC)}}{\\text{Suma Total de Cuadrados (STC)}} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\\]\n\n\\(R^2\\) var√≠a entre 0 y 1 (o 0% y 100%).\nUn \\(R^2\\) de 0.85 significa que el 85% de la variabilidad en \\(Y\\) puede ser explicada por \\(X\\).\nUn \\(R^2\\) m√°s alto generalmente indica un mejor ajuste del modelo.\n\n\n\np-values (Valores p)\nEl p-value nos ayuda a determinar si nuestra variable predictora \\(X\\) es estad√≠sticamente significativa. Responde a la pregunta: ¬øEs probable que la relaci√≥n que observamos entre \\(X\\) y \\(Y\\) haya ocurrido por puro azar?\n\nHip√≥tesis Nula (\\(H_0\\)): No hay relaci√≥n entre \\(X\\) y \\(Y\\) (es decir, \\(\\beta_1 = 0\\)).\nHip√≥tesis Alternativa (\\(H_a\\)): S√≠ hay una relaci√≥n entre \\(X\\) y \\(Y\\) (es decir, \\(\\beta_1 \\neq 0\\)).\n\nUn p-value peque√±o (t√≠picamente &lt; 0.05) nos da evidencia para rechazar la hip√≥tesis nula. Esto sugiere que nuestra variable \\(X\\) es un predictor √∫til para \\(Y\\).",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#m√©tricas-de-error-de-predicci√≥n",
    "href": "03-regresion_lineal.html#m√©tricas-de-error-de-predicci√≥n",
    "title": "Regresi√≥n lineal",
    "section": "M√©tricas de Error de Predicci√≥n",
    "text": "M√©tricas de Error de Predicci√≥n\nAdem√°s del \\(R^2\\), existen m√∫ltiples m√©tricas para evaluar qu√© tan bien predice nuestro modelo. Cada una tiene sus ventajas y casos de uso espec√≠ficos:\n\nError Cuadr√°tico Medio (MSE)\nEl MSE mide el promedio de los errores al cuadrado:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\n\nVentajas: Penaliza fuertemente errores grandes, diferenciable (√∫til para optimizaci√≥n)\nDesventajas: Sensible a valores at√≠picos, dif√≠cil de interpretar (unidades al cuadrado)\nCu√°ndo usar: Cuando errores grandes son especialmente costosos\n\n\n\nRa√≠z del Error Cuadr√°tico Medio (RMSE)\nEl RMSE es la ra√≠z cuadrada del MSE:\n\\[RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]\n\nVentajas: Mismas unidades que la variable objetivo, interpretable\nDesventajas: A√∫n sensible a valores at√≠picos\nInterpretaci√≥n: ‚ÄúEn promedio, nuestras predicciones se desv√≠an X unidades del valor real‚Äù\n\n\n\nError Absoluto Medio (MAE)\nEl MAE mide el promedio de los errores absolutos:\n\\[MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\]\n\nVentajas: Robusto a valores at√≠picos, f√°cil de interpretar\nDesventajas: No diferenciable en cero, trata todos los errores por igual\nCu√°ndo usar: Cuando hay valores at√≠picos o todos los errores tienen igual importancia\n\n\n\nError Porcentual Absoluto Medio (MAPE)\nEl MAPE expresa el error como porcentaje del valor real:\n\\[MAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\\]\n\nVentajas: Interpretable (% de error), adimensional, √∫til para comparar modelos en diferentes escalas\nDesventajas: Indefinido cuando \\(y_i = 0\\), asim√©trico (penaliza m√°s las sobreestimaciones)\nInterpretaci√≥n: ‚ÄúNuestras predicciones se desv√≠an en promedio X% del valor real‚Äù\nCu√°ndo usar: Para comparar precisi√≥n entre diferentes productos, regiones, o escalas\n\n\n\nError Porcentual Absoluto Medio Sim√©trico (SMAPE)\nEl SMAPE es una versi√≥n sim√©trica del MAPE:\n\\[SMAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}\\]\n\nVentajas: Sim√©trico, acotado entre 0% y 200%\nDesventajas: Puede ser contraintuitivo, no tan est√°ndar como MAPE\nCu√°ndo usar: Cuando queremos evitar el sesgo del MAPE hacia sobreestimaciones\n\n\n\nError Logar√≠tmico Cuadr√°tico Medio (MSLE)\nEl MSLE usa transformaci√≥n logar√≠tmica:\n\\[MSLE = \\frac{1}{n} \\sum_{i=1}^{n} (\\log(1 + y_i) - \\log(1 + \\hat{y}_i))^2\\]\n\nVentajas: Penaliza m√°s las subestimaciones que las sobreestimaciones\nDesventajas: Solo para valores positivos, menos interpretable\nCu√°ndo usar: Cuando subestimar es m√°s costoso que sobreestimar (ej: demanda de inventario)\n\n\n\n\\(R^2\\) Ajustado\nEl \\(R^2\\) ajustado penaliza por el n√∫mero de variables en el modelo:\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\nDonde \\(p\\) es el n√∫mero de predictores.\n\nVentajas: No aumenta autom√°ticamente al a√±adir variables\nCu√°ndo usar: Para comparar modelos con diferente n√∫mero de variables\nInterpretaci√≥n: Similar a \\(R^2\\) pero m√°s conservador\n\n\n¬øCu√°l m√©trica elegir?\nLa elecci√≥n de m√©trica depende del contexto del problema:\n\n\n\n\n\n\n\n\nM√©trica\nMejor para\nEvitar cuando\n\n\n\n\nRMSE\nErrores grandes son costosos\nHay muchos valores at√≠picos\n\n\nMAE\nErrores tienen igual importancia\nNecesitas diferenciabilidad\n\n\nMAPE\nComparar diferentes escalas\nHay valores cercanos a cero\n\n\nSMAPE\nComparar con simetr√≠a\nInterpretaci√≥n debe ser simple\n\n\nR¬≤\nExplicar variabilidad\nSolo importa precisi√≥n de predicci√≥n\n\n\n\n\n\n\n\n\n\nRecomendaci√≥n pr√°ctica\n\n\n\nUsa m√∫ltiples m√©tricas para evaluar tu modelo. Una combinaci√≥n t√≠pica ser√≠a: - RMSE para precisi√≥n general - MAPE para interpretabilidad de negocio\n- R¬≤ para explicaci√≥n de variabilidad",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresi√≥n-lineal-m√∫ltiple",
    "href": "03-regresion_lineal.html#regresi√≥n-lineal-m√∫ltiple",
    "title": "Regresi√≥n lineal",
    "section": "Regresi√≥n Lineal M√∫ltiple",
    "text": "Regresi√≥n Lineal M√∫ltiple\nAhora, ¬øqu√© pasa si tenemos m√∫ltiples predictores (\\(X_1, X_2, ..., X_p\\))? El modelo se expande:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\\]\nLa intuici√≥n es la misma, pero en lugar de ajustar una l√≠nea, estamos ajustando un hiperplano en un espacio multidimensional.\nPara manejar esto de forma elegante, usamos notaci√≥n matricial:\n\\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\nDonde: - \\(\\mathbf{y}\\) es el vector de observaciones. - \\(\\mathbf{X}\\) es la matriz de dise√±o (con una primera columna de unos para el intercepto). - \\(\\boldsymbol{\\beta}\\) es el vector de coeficientes. - \\(\\boldsymbol{\\epsilon}\\) es el vector de errores.\nLa funci√≥n de costo en forma matricial es: \\[J(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#transformaciones-comunes-en-modelos-lineales",
    "href": "03-regresion_lineal.html#transformaciones-comunes-en-modelos-lineales",
    "title": "Regresi√≥n lineal",
    "section": "Transformaciones Comunes en Modelos Lineales",
    "text": "Transformaciones Comunes en Modelos Lineales\nA veces, la relaci√≥n entre X e Y no es estrictamente lineal. Las transformaciones logar√≠tmicas nos permiten modelar relaciones no lineales y, adem√°s, ofrecen interpretaciones muy √∫tiles en t√©rminos de cambios porcentuales.\n\nModelo Log-Nivel (Transformaci√≥n en Y)\nEste modelo se usa cuando el efecto de X sobre Y no es absoluto, sino porcentual. Por ejemplo, c√≥mo un a√±o m√°s de educaci√≥n afecta el porcentaje de aumento salarial.\n\nEcuaci√≥n: \\(\\ln(Y) = \\beta_0 + \\beta_1 X + \\epsilon\\)\nInterpretaci√≥n: Un incremento de una unidad en \\(X\\) est√° asociado con un cambio de \\((100 \\cdot \\beta_1)\\%\\) en \\(Y\\).\n\n\n\n\n\n\n\nExplicaci√≥n Matem√°tica de la Aproximaci√≥n\n\n\n\n\n\nLa clave est√° en la propiedad del logaritmo y el c√°lculo. La derivada de \\(\\ln(Y)\\) con respecto a \\(X\\) es \\(\\beta_1\\): \\[\\frac{d(\\ln(Y))}{dX} = \\beta_1\\] Sabemos que \\(d(\\ln(Y)) = \\frac{dY}{Y}\\). Por tanto: \\[\\frac{dY/Y}{dX} = \\beta_1\\] Para cambios peque√±os (o discretos, \\(\\Delta\\)), podemos aproximar los diferenciales: \\[\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X}\\] Si consideramos un cambio unitario en X, \\(\\Delta X = 1\\), entonces: \\[\\beta_1 \\approx \\frac{\\Delta Y}{Y}\\] Esto significa que \\(\\beta_1\\) es la aproximaci√≥n del cambio porcentual en \\(Y\\) ante un cambio de una unidad en \\(X\\).\n\n\n\n\n\nModelo Nivel-Log (Transformaci√≥n en X)\nEste modelo es √∫til cuando el efecto de X sobre Y se reduce a medida que X aumenta (rendimientos decrecientes). Por ejemplo, el efecto de a√±adir presupuesto de marketing sobre las ventas.\n\nEcuaci√≥n: \\(Y = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\)\nInterpretaci√≥n: Un incremento del 1% en \\(X\\) est√° asociado con un cambio de \\((\\beta_1 / 100)\\) unidades en \\(Y\\).\n\n\n\n\n\n\n\nExplicaci√≥n Matem√°tica de la Aproximaci√≥n\n\n\n\n\n\nTomamos la derivada de \\(Y\\) con respecto a \\(\\ln(X)\\): \\[\\frac{dY}{d(\\ln(X))} = \\beta_1\\] Usando la regla de la cadena, sabemos que \\(d(\\ln(X)) = \\frac{dX}{X}\\). Sustituyendo: \\[\\frac{dY}{dX/X} = \\beta_1 \\implies dY = \\beta_1 \\frac{dX}{X}\\] Para cambios discretos, aproximamos: \\[\\Delta Y \\approx \\beta_1 \\frac{\\Delta X}{X}\\] Si consideramos un cambio del 1% en X, entonces \\(\\frac{\\Delta X}{X} = 0.01\\). La ecuaci√≥n se convierte en: \\[\\Delta Y \\approx \\beta_1 (0.01) = \\frac{\\beta_1}{100}\\] Esto significa que un cambio del 1% en \\(X\\) provoca un cambio de \\(\\beta_1/100\\) unidades en \\(Y\\).\n\n\n\n\n\nModelo Log-Log (Transformaci√≥n en X e Y)\nEste modelo es muy com√∫n en econom√≠a y modela la elasticidad constante entre dos variables.\n\nEcuaci√≥n: \\(\\ln(Y) = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\)\nInterpretaci√≥n: Un incremento del 1% en \\(X\\) est√° asociado con un cambio del \\(\\beta_1\\%\\) en \\(Y\\).\n\n\n\n\n\n\n\nExplicaci√≥n Matem√°tica de la Aproximaci√≥n\n\n\n\n\n\nEste caso combina los dos anteriores. \\(\\beta_1\\) es la derivada de \\(\\ln(Y)\\) con respecto a \\(\\ln(X)\\), que es la definici√≥n de elasticidad. \\[\\beta_1 = \\frac{d(\\ln(Y))}{d(\\ln(X))}\\] Usando las propiedades del c√°lculo que vimos antes: \\[\\beta_1 = \\frac{dY/Y}{dX/X}\\] Aproximando para cambios discretos: \\[\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X / X}\\] Esta es la definici√≥n de elasticidad: el cambio porcentual en \\(Y\\) dividido por el cambio porcentual en \\(X\\). Por lo tanto, si \\(X\\) cambia en un 1% (\\(\\Delta X / X = 0.01\\)), el cambio porcentual en \\(Y\\) (\\(\\Delta Y / Y\\)) ser√° aproximadamente \\(\\beta_1 \\times 0.01\\), es decir, un \\(\\beta_1\\%\\).",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresi√≥n-regularizada-penalizada",
    "href": "03-regresion_lineal.html#regresi√≥n-regularizada-penalizada",
    "title": "Regresi√≥n lineal",
    "section": "Regresi√≥n Regularizada (Penalizada) üéØ",
    "text": "Regresi√≥n Regularizada (Penalizada) üéØ\nHasta ahora hemos visto la regresi√≥n lineal cl√°sica, pero ¬øqu√© pasa cuando tenemos muchas variables o cuando nuestro modelo sufre de sobreajuste? Aqu√≠ es donde entran las t√©cnicas de regularizaci√≥n.\n\n¬øPor qu√© necesitamos regularizaci√≥n?\nLa regresi√≥n lineal ordinaria (OLS) puede presentar varios problemas:\n\nSobreajuste: Cuando tenemos muchas variables relativas al n√∫mero de observaciones\nMulticolinealidad: Variables predictoras altamente correlacionadas\nInestabilidad: Peque√±os cambios en los datos causan grandes cambios en los coeficientes\nInterpretabilidad: Demasiadas variables hacen dif√≠cil entender el modelo\n\nLa regularizaci√≥n a√±ade una penalizaci√≥n a la funci√≥n de costo para controlar la complejidad del modelo.\n\n\n\nRidge Regression (Regresi√≥n Ridge) üèîÔ∏è\nLa regresi√≥n Ridge a√±ade una penalizaci√≥n L2 (suma de cuadrados) a los coeficientes:\n\\[J_{Ridge}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\\]\nDonde: - \\(\\lambda &gt; 0\\) es el par√°metro de regularizaci√≥n - \\(\\sum_{j=1}^{p} \\beta_j^2\\) es la penalizaci√≥n L2\n\nCaracter√≠sticas de Ridge:\n‚úÖ Ventajas: - Reduce el sobreajuste - Maneja bien la multicolinealidad - Siempre tiene soluci√≥n √∫nica - Estabiliza los coeficientes\n‚ùå Desventajas: - NO elimina variables (coeficientes nunca son exactamente cero) - Dificulta la interpretabilidad - Requiere estandarizar las variables\n\n\nSoluci√≥n Anal√≠tica:\n\\[\\hat{\\boldsymbol{\\beta}}_{Ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nEl t√©rmino \\(\\lambda\\mathbf{I}\\) hace que la matriz sea invertible incluso con multicolinealidad.\n\n\n¬øC√≥mo elegir Œª?\n\nŒª = 0: Regresi√≥n ordinaria (sin penalizaci√≥n)\nŒª ‚Üí ‚àû: Todos los coeficientes ‚Üí 0\nŒª √≥ptimo: Se encuentra usando validaci√≥n cruzada\n\n\n\n\n\nLasso Regression (Least Absolute Shrinkage and Selection Operator) ‚úÇÔ∏è\nLa regresi√≥n Lasso usa penalizaci√≥n L1 (suma de valores absolutos):\n\\[J_{Lasso}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\\]\n\nCaracter√≠sticas de Lasso:\n‚úÖ Ventajas: - Selecci√≥n autom√°tica de variables (coeficientes = 0) - Modelos m√°s interpretables y simples - √ötil cuando muchas variables son irrelevantes\n‚ùå Desventajas: - Puede ser inestable con grupos de variables correlacionadas - Selecciona arbitrariamente entre variables correlacionadas - No tiene soluci√≥n anal√≠tica cerrada\n\n\nLa ‚ÄúMagia‚Äù de L1: ¬øPor qu√© produce ceros exactos?\nLa penalizaci√≥n L1 crea una regi√≥n factible con esquinas puntiagudas. La soluci√≥n √≥ptima tiende a ocurrir en estas esquinas, donde algunos coeficientes son exactamente cero.\n\n\n\n\n\n\nIntuici√≥n Geom√©trica\n\n\n\n\n\nImagina que est√°s minimizando una funci√≥n bajo la restricci√≥n de que \\(|\\beta_1| + |\\beta_2| \\leq t\\). Esta restricci√≥n forma un diamante en 2D. La funci√≥n objetivo forma elipses. La soluci√≥n est√° donde la elipse m√°s peque√±a toca el diamante, y esto frecuentemente ocurre en los v√©rtices (donde \\(\\beta_1 = 0\\) o \\(\\beta_2 = 0\\)).\n\n\n\n\n\n\n\nElastic Net: Lo Mejor de Ambos Mundos üï∏Ô∏è\nElastic Net combina las penalizaciones L1 y L2:\n\\[J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\\]\nO equivalentemente, con un par√°metro de mezcla \\(\\alpha\\):\n\\[J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\left[ \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right]\\]\nDonde: - \\(\\alpha \\in [0,1]\\) controla la mezcla entre L1 y L2 - \\(\\alpha = 0\\): Pure Ridge - \\(\\alpha = 1\\): Pure Lasso - \\(\\alpha = 0.5\\): Igual peso a ambas penalizaciones\n\nCaracter√≠sticas de Elastic Net:\n‚úÖ Ventajas: - Selecci√≥n de variables como Lasso - Estabilidad como Ridge - Maneja bien grupos de variables correlacionadas - M√°s flexible que Ridge o Lasso por separado\n‚ùå Desventajas: - Dos hiperpar√°metros para ajustar (\\(\\lambda\\) y \\(\\alpha\\)) - M√°s complejo computacionalmente\n\n\n\n\nComparaci√≥n Visual: Ridge vs Lasso vs Elastic Net\n\n\n\n\n\n\n\n\n\nAspecto\nRidge\nLasso\nElastic Net\n\n\n\n\nPenalizaci√≥n\nL2: \\(\\sum \\beta_j^2\\)\nL1: \\(\\sum |\\beta_j|\\)\nL1 + L2 combinadas\n\n\nSelecci√≥n de variables\n‚ùå No\n‚úÖ S√≠\n‚úÖ S√≠\n\n\nCoeficientes exactamente cero\n‚ùå No\n‚úÖ S√≠\n‚úÖ S√≠\n\n\nManejo de multicolinealidad\n‚úÖ Excelente\n‚ö†Ô∏è Problem√°tico\n‚úÖ Muy bueno\n\n\nEstabilidad\n‚úÖ Alta\n‚ö†Ô∏è Media\n‚úÖ Alta\n\n\nInterpretabilidad\n‚ö†Ô∏è Media\n‚úÖ Alta\n‚úÖ Alta\n\n\nCuando usar\nTodas las variables importan\nPocas variables importantes\nSituaciones mixtas\n\n\n\n\n\n¬øCu√°ndo usar cada m√©todo?\n\nUsa Ridge cuando:\n\nCrees que todas las variables contribuyen al modelo\nTienes multicolinealidad severa\nQuieres estabilizar coeficientes sin eliminar variables\nEl n√∫mero de observaciones es peque√±o relativo a variables\n\n\n\nUsa Lasso cuando:\n\nCrees que pocas variables son realmente importantes\nQuieres un modelo simple e interpretable\nNecesitas selecci√≥n autom√°tica de variables\nTienes muchas variables irrelevantes\n\n\n\nUsa Elastic Net cuando:\n\nNo est√°s seguro de cu√°ntas variables son importantes\nTienes grupos de variables correlacionadas\nQuieres balancear selecci√≥n y estabilidad\nEs tu primera opci√≥n cuando no conoces la estructura de los datos\n\n\n\n\n\nValidaci√≥n de Modelos y Selecci√≥n de Hiperpar√°metros\n\n¬øPor qu√© necesitamos dividir nuestros datos?\nCuando construimos modelos de machine learning, enfrentamos un dilema fundamental: ¬øc√≥mo sabemos si nuestro modelo funcionar√° bien con datos nuevos?\n\nEl Problema del Sobreajuste\nImagina que est√°s prepar√°ndote para un examen. Si solo estudias las preguntas exactas que aparecer√°n en el examen, podr√≠as obtener una calificaci√≥n perfecta. Pero si las preguntas cambian ligeramente, tu rendimiento se desplomar√≠a. Esto es sobreajuste: el modelo memoriza los datos de entrenamiento pero no generaliza.\n\n\n\nDivisi√≥n T√≠pica de Datos: Entrenamiento/Validaci√≥n/Prueba\nLa estrategia est√°ndar es dividir nuestros datos en tres conjuntos:\nüìä Dataset Completo (100%)\n‚îú‚îÄ‚îÄ üèãÔ∏è Entrenamiento (60%) - Para ajustar coeficientes\n‚îú‚îÄ‚îÄ üéØ Validaci√≥n (20%)     - Para seleccionar hiperpar√°metros  \n‚îî‚îÄ‚îÄ üß™ Prueba (20%)         - Para evaluaci√≥n final\n\nConjunto de Entrenamiento (60%)\n\nProp√≥sito: Ajustar los coeficientes \\(\\beta\\) del modelo\nAnalog√≠a: Los ejercicios que haces para aprender\n\n\n\nConjunto de Validaci√≥n (20%)\n\nProp√≥sito: Comparar diferentes hiperpar√°metros (como \\(\\lambda\\) en Ridge/Lasso)\nAnalog√≠a: Ex√°menes de pr√°ctica para decidir qu√© estrategia de estudio funciona mejor\n\n\n\nConjunto de Prueba (20%)\n\nProp√≥sito: Evaluaci√≥n final y honesta del modelo\nAnalog√≠a: El examen final real\n‚ö†Ô∏è Regla de Oro: ¬°Solo se usa UNA vez al final!\n\n\n\n\n¬øQu√© pasa si tenemos pocos datos?\nCuando nuestro dataset es peque√±o (&lt; 1000 observaciones), dividir en tres partes puede ser problem√°tico:\n‚ùå Problemas con datasets peque√±os: - Conjunto de entrenamiento muy peque√±o ‚Üí modelo pobre - Conjunto de validaci√≥n peque√±o ‚Üí selecci√≥n inestable de hiperpar√°metros - Conjunto de prueba peque√±o ‚Üí evaluaci√≥n poco confiable\nSoluci√≥n: ¬°Validaci√≥n Cruzada!\n\n\n\nValidaci√≥n Cruzada (Cross-Validation)\nLa validaci√≥n cruzada es una t√©cnica que maximiza el uso de nuestros datos limitados. En lugar de usar una sola divisi√≥n, usamos m√∫ltiples divisiones.\n\nValidaci√≥n Cruzada k-fold\nEl m√©todo m√°s com√∫n es k-fold cross-validation:\n\nDividir el dataset en \\(k\\) ‚Äúpliegues‚Äù (folds) de igual tama√±o\nRepetir \\(k\\) veces:\n\nUsar \\(k-1\\) pliegues para entrenamiento\nUsar 1 pliegue para validaci√≥n\n\nPromediar los resultados de las \\(k\\) evaluaciones\n\n\n\n\n\n\nVisualizaci√≥n de 5-Fold Cross Validation mostrando c√≥mo se dividen los datos en cada iteraci√≥n\n\n\n\n\n\n\nVentajas de la Validaci√≥n Cruzada\n‚úÖ Maximiza el uso de datos: Cada observaci√≥n se usa tanto para entrenamiento como validaci√≥n\n‚úÖ Estimaci√≥n m√°s robusta: Promedia m√∫ltiples evaluaciones independientes\n‚úÖ Reduce la varianza: Menos dependiente de una divisi√≥n particular\n‚úÖ Detecta inestabilidad: Si los resultados var√≠an mucho entre folds, el modelo es inestable\n\n\n\nValidaci√≥n Cruzada para Selecci√≥n de Hiperpar√°metros\nEn regresi√≥n regularizada, usamos CV para encontrar el mejor \\(\\lambda\\):\n\n\nüéØ SELECCI√ìN DE HIPERPAR√ÅMETROS CON VALIDACI√ìN CRUZADA\n============================================================\nPara cada valor de Œª:\n  1. Aplicar 5-fold CV\n  2. Calcular error promedio\n  3. Seleccionar Œª con menor error\n\n\n\n\n\nCurva de validaci√≥n mostrando c√≥mo seleccionar el hiperpar√°metro √≥ptimo Œª usando validaci√≥n cruzada\n\n\n\n\n\nüìà Resultado: Œª √≥ptimo = 0.1274\nüìâ Error de CV m√≠nimo = 0.4776\n\n\n\n\nProceso Completo de Validaci√≥n\nEl flujo completo para modelos regularizados es:\n1. üìä Dividir datos originales\n   ‚îî‚îÄ‚îÄ 80% para desarrollo (entrenamiento + validaci√≥n)\n   ‚îî‚îÄ‚îÄ 20% para prueba final (¬°NO TOCAR hasta el final!)\n\n2. üîÑ En el conjunto de desarrollo:\n   ‚îî‚îÄ‚îÄ Para cada Œª candidato:\n       ‚îú‚îÄ‚îÄ Aplicar k-fold CV\n       ‚îú‚îÄ‚îÄ Calcular error promedio\n       ‚îî‚îÄ‚îÄ Guardar resultado\n\n3. üéØ Seleccionar Œª con menor error de CV\n\n4. üèóÔ∏è Entrenar modelo final con Œª √≥ptimo en TODO el conjunto de desarrollo\n\n5. üß™ Evaluaci√≥n final en conjunto de prueba\n\n\nVariantes de Validaci√≥n Cruzada\n\nLeave-One-Out CV (LOOCV)\n\nk = n (n√∫mero de observaciones)\nVentaja: M√°ximo uso de datos para entrenamiento\nDesventaja: Computacionalmente costoso, alta varianza\n\n\n\nStratified CV\n\nPara problemas de clasificaci√≥n\nMantiene la proporci√≥n de clases en cada fold\n\n\n\nTime Series CV\n\nPara datos temporales\nRespeta el orden temporal (no mezcla futuro con pasado)",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regresi√≥n lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html",
    "href": "violaciones_supuestos_regresion.html",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "",
    "text": "1. Violaci√≥n del Supuesto de Linealidad\nEste notebook demuestra gr√°ficamente c√≥mo pueden violarse los cuatro supuestos principales de la regresi√≥n lineal:\nEl supuesto de linealidad requiere que la relaci√≥n entre X e Y sea lineal. Cuando esto se viola, un modelo lineal ser√° inadecuado.\ndef crear_datos_no_lineales(n=200, tipo='cuadratico', noise=1.0, seed=42):\n    \"\"\"Genera datos con relaciones no lineales\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(-3, 3, n)\n    \n    if tipo == 'cuadratico':\n        y = 2 * x**2 + 1 + np.random.normal(0, noise, n)\n        titulo = \"Relaci√≥n Cuadr√°tica\"\n    elif tipo == 'exponencial':\n        y = np.exp(x/2) + np.random.normal(0, noise, n)\n        titulo = \"Relaci√≥n Exponencial\"\n    elif tipo == 'sinusoidal':\n        y = 5 * np.sin(2*x) + x + np.random.normal(0, noise, n)\n        titulo = \"Relaci√≥n Sinusoidal\"\n    elif tipo == 'logaritmico':\n        x = np.linspace(0.1, 10, n)\n        y = 3 * np.log(x) + 2 + np.random.normal(0, noise, n)\n        titulo = \"Relaci√≥n Logar√≠tmica\"\n    \n    return x, y, titulo\n\n# Crear subplots para diferentes tipos de no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Violaciones del Supuesto de Linealidad', fontsize=16, fontweight='bold')\n\ntipos = ['cuadratico', 'exponencial', 'sinusoidal', 'logaritmico']\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    \n    # Gr√°fico\n    axes[row, col].scatter(x, y, alpha=0.6, label='Datos reales')\n    axes[row, col].plot(x, y_pred, 'r-', linewidth=2, label='Regresi√≥n lineal')\n    axes[row, col].set_title(titulo)\n    axes[row, col].set_xlabel('X')\n    axes[row, col].set_ylabel('Y')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n    \n    # Calcular R¬≤\n    r2 = lr.score(X_reshaped, y)\n    axes[row, col].text(0.05, 0.95, f'R¬≤ = {r2:.3f}', \n                        transform=axes[row, col].transAxes, \n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-linealidad",
    "href": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-linealidad",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "",
    "text": "Diagn√≥stico: Gr√°ficos de Residuos vs Valores Predichos\nUna forma de detectar no linealidad es examinar los residuos. Si hay patrones en los residuos, indica problemas con el modelo.\n\n# Crear gr√°ficos de residuos para detectar no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Gr√°ficos de Residuos: Detectando No Linealidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    residuos = y - y_pred\n    \n    # Gr√°fico de residuos\n    axes[row, col].scatter(y_pred, residuos, alpha=0.6)\n    axes[row, col].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[row, col].set_title(f'Residuos - {titulo}')\n    axes[row, col].set_xlabel('Valores Predichos')\n    axes[row, col].set_ylabel('Residuos')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä INTERPRETACI√ìN:\")\nprint(\"‚úÖ Residuos aleatorios alrededor de 0 = Linealidad OK\")\nprint(\"‚ùå Patrones en residuos = Violaci√≥n de linealidad\")\nprint(\"   - Curva: Relaci√≥n cuadr√°tica o polinomial\")\nprint(\"   - Tendencia: Relaci√≥n exponencial o logar√≠tmica\")\nprint(\"   - Oscilaci√≥n: Efectos c√≠clicos o sinusoidales\")\n\n\n\n\n\n\n\n\nüìä INTERPRETACI√ìN:\n‚úÖ Residuos aleatorios alrededor de 0 = Linealidad OK\n‚ùå Patrones en residuos = Violaci√≥n de linealidad\n   - Curva: Relaci√≥n cuadr√°tica o polinomial\n   - Tendencia: Relaci√≥n exponencial o logar√≠tmica\n   - Oscilaci√≥n: Efectos c√≠clicos o sinusoidales",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#soluci√≥n",
    "href": "violaciones_supuestos_regresion.html#soluci√≥n",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "Soluci√≥n",
    "text": "Soluci√≥n\n\n# Crear subplots para diferentes tipos de no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Violaciones del Supuesto de Linealidad', fontsize=16, fontweight='bold')\n\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    if tipo == \"cuadratico\":\n        X_reshaped = X_reshaped**2\n    if tipo == \"exponencial\":\n        X_reshaped = np.exp(X_reshaped)\n    if tipo == \"sinusoidal\":\n        X_reshaped = np.concatenate((np.sin(2*X_reshaped), X_reshaped), axis = 1)\n    if tipo == \"logaritmico\":\n        X_reshaped = np.log(X_reshaped)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    \n    # Gr√°fico\n    axes[row, col].scatter(x, y, alpha=0.6, label='Datos reales')\n    axes[row, col].plot(x, y_pred, 'r-', linewidth=2, label='Regresi√≥n lineal')\n    axes[row, col].set_title(titulo)\n    axes[row, col].set_xlabel('X')\n    axes[row, col].set_ylabel('Y')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n    \n    # Calcular R¬≤\n    r2 = lr.score(X_reshaped, y)\n    axes[row, col].text(0.05, 0.95, f'R¬≤ = {r2:.3f}', \n                        transform=axes[row, col].transAxes, \n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Crear gr√°ficos de residuos para detectar no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Gr√°ficos de Residuos: Detectando No Linealidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    if tipo == \"cuadratico\":\n        X_reshaped = X_reshaped**2\n    if tipo == \"exponencial\":\n        X_reshaped = np.exp(X_reshaped)\n    if tipo == \"sinusoidal\":\n        X_reshaped = np.concatenate((np.sin(2*X_reshaped), X_reshaped), axis = 1)\n    if tipo == \"logaritmico\":\n        X_reshaped = np.log(X_reshaped)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    residuos = y - y_pred\n    \n    # Gr√°fico de residuos\n    axes[row, col].scatter(y_pred, residuos, alpha=0.6)\n    axes[row, col].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[row, col].set_title(f'Residuos - {titulo}')\n    axes[row, col].set_xlabel('Valores Predichos')\n    axes[row, col].set_ylabel('Residuos')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-independencia",
    "href": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-independencia",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "2. Violaci√≥n del Supuesto de Independencia",
    "text": "2. Violaci√≥n del Supuesto de Independencia\nLos errores deben ser independientes entre s√≠. Esto es especialmente importante en datos de series temporales.\n\ndef crear_datos_correlacionados(n=200, correlacion=0.7, seed=42):\n    \"\"\"Genera datos con errores correlacionados (autocorrelaci√≥n)\"\"\"\n    np.random.seed(seed)\n    \n    # Datos independientes (correctos)\n    x = np.linspace(0, 10, n)\n    errores_independientes = np.random.normal(0, 2, n)\n    y_independiente = 2 + 1.5 * x + errores_independientes\n    \n    # Datos con autocorrelaci√≥n (violaci√≥n)\n    errores_correlacionados = np.zeros(n)\n    errores_correlacionados[0] = np.random.normal(0, 2)\n    \n    for i in range(1, n):\n        errores_correlacionados[i] = (correlacion * errores_correlacionados[i-1] + \n                                     np.sqrt(1 - correlacion**2) * np.random.normal(0, 2))\n    \n    y_correlacionado = 2 + 1.5 * x + errores_correlacionados\n    \n    return x, y_independiente, y_correlacionado, errores_independientes, errores_correlacionados\n\n# Generar datos\nx, y_indep, y_corr, err_indep, err_corr = crear_datos_correlacionados()\n\n# Ajustar modelos\nlr_indep = LinearRegression().fit(x.reshape(-1, 1), y_indep)\nlr_corr = LinearRegression().fit(x.reshape(-1, 1), y_corr)\n\ny_pred_indep = lr_indep.predict(x.reshape(-1, 1))\ny_pred_corr = lr_corr.predict(x.reshape(-1, 1))\n\nresiduos_indep = y_indep - y_pred_indep\nresiduos_corr = y_corr - y_pred_corr\n\n# Crear gr√°ficos\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Violaci√≥n del Supuesto de Independencia', fontsize=16, fontweight='bold')\n\n# Fila 1: Datos independientes (correctos)\naxes[0, 0].scatter(x, y_indep, alpha=0.6, color='blue')\naxes[0, 0].plot(x, y_pred_indep, 'r-', linewidth=2)\naxes[0, 0].set_title('Datos con Errores Independientes ‚úÖ')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('Y')\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(residuos_indep, 'o-', alpha=0.7, color='blue')\naxes[0, 1].axhline(y=0, color='red', linestyle='--')\naxes[0, 1].set_title('Residuos vs Tiempo')\naxes[0, 1].set_xlabel('Observaci√≥n')\naxes[0, 1].set_ylabel('Residuos')\naxes[0, 1].grid(True, alpha=0.3)\n\naxes[0, 2].scatter(residuos_indep[:-1], residuos_indep[1:], alpha=0.6, color='blue')\naxes[0, 2].set_title('Residuos(t) vs Residuos(t-1)')\naxes[0, 2].set_xlabel('Residuo en t-1')\naxes[0, 2].set_ylabel('Residuo en t')\naxes[0, 2].grid(True, alpha=0.3)\n\n# Calcular correlaci√≥n\ncorr_indep = np.corrcoef(residuos_indep[:-1], residuos_indep[1:])[0, 1]\naxes[0, 2].text(0.05, 0.95, f'r = {corr_indep:.3f}', \n                transform=axes[0, 2].transAxes,\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n# Fila 2: Datos correlacionados (violaci√≥n)\naxes[1, 0].scatter(x, y_corr, alpha=0.6, color='orange')\naxes[1, 0].plot(x, y_pred_corr, 'r-', linewidth=2)\naxes[1, 0].set_title('Datos con Errores Correlacionados ‚ùå')\naxes[1, 0].set_xlabel('X')\naxes[1, 0].set_ylabel('Y')\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(residuos_corr, 'o-', alpha=0.7, color='orange')\naxes[1, 1].axhline(y=0, color='red', linestyle='--')\naxes[1, 1].set_title('Residuos vs Tiempo')\naxes[1, 1].set_xlabel('Observaci√≥n')\naxes[1, 1].set_ylabel('Residuos')\naxes[1, 1].grid(True, alpha=0.3)\n\naxes[1, 2].scatter(residuos_corr[:-1], residuos_corr[1:], alpha=0.6, color='orange')\naxes[1, 2].set_title('Residuos(t) vs Residuos(t-1)')\naxes[1, 2].set_xlabel('Residuo en t-1')\naxes[1, 2].set_ylabel('Residuo en t')\naxes[1, 2].grid(True, alpha=0.3)\n\n# Calcular correlaci√≥n\ncorr_corr = np.corrcoef(residuos_corr[:-1], residuos_corr[1:])[0, 1]\naxes[1, 2].text(0.05, 0.95, f'r = {corr_corr:.3f}', \n                transform=axes[1, 2].transAxes,\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä INTERPRETACI√ìN:\")\nprint(f\"‚úÖ Errores independientes: correlaci√≥n = {corr_indep:.3f} (cerca de 0)\")\nprint(f\"‚ùå Errores correlacionados: correlaci√≥n = {corr_corr:.3f} (lejos de 0)\")\nprint(\"\\nüîç DIAGN√ìSTICO:\")\nprint(\"- Gr√°fico temporal: patrones o tendencias en residuos\")\nprint(\"- Autocorrelaci√≥n: correlaci√≥n significativa entre residuos consecutivos\")\nprint(\"- Prueba Durbin-Watson: estad√≠stico cerca de 2 = independencia\")\n\n\n\n\n\n\n\n\nüìä INTERPRETACI√ìN:\n‚úÖ Errores independientes: correlaci√≥n = -0.058 (cerca de 0)\n‚ùå Errores correlacionados: correlaci√≥n = 0.670 (lejos de 0)\n\nüîç DIAGN√ìSTICO:\n- Gr√°fico temporal: patrones o tendencias en residuos\n- Autocorrelaci√≥n: correlaci√≥n significativa entre residuos consecutivos\n- Prueba Durbin-Watson: estad√≠stico cerca de 2 = independencia",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-homocedasticidad",
    "href": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-homocedasticidad",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "3. Violaci√≥n del Supuesto de Homocedasticidad",
    "text": "3. Violaci√≥n del Supuesto de Homocedasticidad\nLa varianza de los errores debe ser constante. Cuando var√≠a, tenemos heterocedasticidad.\n\ndef crear_datos_heteroscedasticos(n=200, tipo='creciente', seed=42):\n    \"\"\"Genera datos con diferentes tipos de heterocedasticidad\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(1, 10, n)\n    \n    if tipo == 'homoscedastico':\n        # Varianza constante (correcto)\n        error = np.random.normal(0, 2, n)\n        titulo = \"Homocedasticidad (Correcto) ‚úÖ\"\n    elif tipo == 'creciente':\n        # Varianza aumenta con X\n        error = np.random.normal(0, 0.5 * x, n)\n        titulo = \"Heterocedasticidad Creciente ‚ùå\"\n    elif tipo == 'decreciente':\n        # Varianza disminuye con X\n        error = np.random.normal(0, 5 / x, n)\n        titulo = \"Heterocedasticidad Decreciente ‚ùå\"\n    elif tipo == 'embudo':\n        # Forma de embudo\n        error = np.random.normal(0, 0.1 + 0.8 * np.abs(x - 5.5), n)\n        titulo = \"Heterocedasticidad en Embudo ‚ùå\"\n    \n    y = 2 + 1.5 * x + error\n    return x, y, titulo\n\n# Crear diferentes tipos de heterocedasticidad\ntipos_het = ['homoscedastico', 'creciente', 'decreciente', 'embudo']\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\nfig.suptitle('Violaci√≥n del Supuesto de Homocedasticidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos_het):\n    x, y, titulo = crear_datos_heteroscedasticos(tipo=tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    # Gr√°fico de datos\n    color = 'green' if tipo == 'homoscedastico' else 'red'\n    axes[0, i].scatter(x, y, alpha=0.6, color=color)\n    axes[0, i].plot(x, y_pred, 'black', linewidth=2)\n    axes[0, i].set_title(titulo)\n    axes[0, i].set_xlabel('X')\n    axes[0, i].set_ylabel('Y')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Gr√°fico de residuos\n    axes[1, i].scatter(y_pred, residuos, alpha=0.6, color=color)\n    axes[1, i].axhline(y=0, color='black', linestyle='--')\n    axes[1, i].set_title(f'Residuos vs Predichos')\n    axes[1, i].set_xlabel('Valores Predichos')\n    axes[1, i].set_ylabel('Residuos')\n    axes[1, i].grid(True, alpha=0.3)\n    \n    # A√±adir l√≠neas de tendencia en residuos para visualizar heterocedasticidad\n    if tipo != 'homoscedastico':\n        z = np.polyfit(y_pred, np.abs(residuos), 1)\n        p = np.poly1d(z)\n        axes[1, i].plot(y_pred, p(y_pred), \"r--\", alpha=0.8)\n        axes[1, i].plot(y_pred, -p(y_pred), \"r--\", alpha=0.8)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä INTERPRETACI√ìN:\")\nprint(\"‚úÖ Homocedasticidad: Residuos con dispersi√≥n constante\")\nprint(\"‚ùå Heterocedasticidad: Residuos con dispersi√≥n variable\")\nprint(\"\\nüîç PATRONES COMUNES:\")\nprint(\"- Embudo creciente: Varianza aumenta con valores predichos\")\nprint(\"- Embudo decreciente: Varianza disminuye con valores predichos\")\nprint(\"- Forma de diamante: Varianza m√°xima en valores medios\")\n\n\n\n\n\n\n\n\nüìä INTERPRETACI√ìN:\n‚úÖ Homocedasticidad: Residuos con dispersi√≥n constante\n‚ùå Heterocedasticidad: Residuos con dispersi√≥n variable\n\nüîç PATRONES COMUNES:\n- Embudo creciente: Varianza aumenta con valores predichos\n- Embudo decreciente: Varianza disminuye con valores predichos\n- Forma de diamante: Varianza m√°xima en valores medios\n\n\n\nPrueba Estad√≠stica: Test de Breusch-Pagan\nUna prueba formal para detectar heterocedasticidad.\n\nfrom scipy.stats import chi2\n\ndef breusch_pagan_test(residuos, x):\n    \"\"\"Implementa el test de Breusch-Pagan para heterocedasticidad\"\"\"\n    n = len(residuos)\n    \n    # Regresi√≥n de residuos al cuadrado sobre X\n    residuos_cuadrados = residuos**2\n    lr_bp = LinearRegression()\n    lr_bp.fit(x.reshape(-1, 1), residuos_cuadrados)\n    \n    # R¬≤ de la regresi√≥n auxiliar\n    r2_bp = lr_bp.score(x.reshape(-1, 1), residuos_cuadrados)\n    \n    # Estad√≠stico de prueba\n    lm_statistic = n * r2_bp\n    \n    # P-value (distribuci√≥n chi-cuadrado con 1 grado de libertad)\n    p_value = 1 - chi2.cdf(lm_statistic, df=1)\n    \n    return lm_statistic, p_value\n\nprint(\"üß™ TEST DE BREUSCH-PAGAN PARA HETEROCEDASTICIDAD\\n\")\nprint(\"H‚ÇÄ: Homocedasticidad (varianza constante)\")\nprint(\"H‚ÇÅ: Heterocedasticidad (varianza no constante)\\n\")\n\nfor tipo in tipos_het:\n    x, y, titulo = crear_datos_heteroscedasticos(tipo=tipo)\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    residuos = y - lr.predict(x.reshape(-1, 1))\n    \n    lm_stat, p_val = breusch_pagan_test(residuos, x)\n    \n    resultado = \"RECHAZA H‚ÇÄ\" if p_val &lt; 0.05 else \"NO RECHAZA H‚ÇÄ\"\n    emoji = \"‚ùå\" if p_val &lt; 0.05 else \"‚úÖ\"\n    \n    print(f\"{emoji} {tipo.upper():15} | LM = {lm_stat:6.2f} | p-value = {p_val:.4f} | {resultado}\")\n\nprint(\"\\nüí° Interpretaci√≥n: p-value &lt; 0.05 indica heterocedasticidad\")\n\nüß™ TEST DE BREUSCH-PAGAN PARA HETEROCEDASTICIDAD\n\nH‚ÇÄ: Homocedasticidad (varianza constante)\nH‚ÇÅ: Heterocedasticidad (varianza no constante)\n\n‚úÖ HOMOSCEDASTICO  | LM =   0.00 | p-value = 0.9588 | NO RECHAZA H‚ÇÄ\n‚ùå CRECIENTE       | LM =  27.79 | p-value = 0.0000 | RECHAZA H‚ÇÄ\n‚ùå DECRECIENTE     | LM =  31.91 | p-value = 0.0000 | RECHAZA H‚ÇÄ\n‚úÖ EMBUDO          | LM =   0.14 | p-value = 0.7122 | NO RECHAZA H‚ÇÄ\n\nüí° Interpretaci√≥n: p-value &lt; 0.05 indica heterocedasticidad",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-normalidad",
    "href": "violaciones_supuestos_regresion.html#violaci√≥n-del-supuesto-de-normalidad",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "4. Violaci√≥n del Supuesto de Normalidad",
    "text": "4. Violaci√≥n del Supuesto de Normalidad\nLos errores deben seguir una distribuci√≥n normal. Esto es crucial para las pruebas de hip√≥tesis y intervalos de confianza.\n\ndef crear_datos_no_normales(n=200, tipo='normal', seed=42):\n    \"\"\"Genera datos con diferentes distribuciones de error\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(0, 10, n)\n    \n    if tipo == 'normal':\n        error = np.random.normal(0, 2, n)\n        titulo = \"Errores Normales ‚úÖ\"\n    elif tipo == 'asimetrico':\n        # Distribuci√≥n asim√©trica (exponencial)\n        error = np.random.exponential(2, n) - 2\n        titulo = \"Errores Asim√©tricos ‚ùå\"\n    elif tipo == 'colas_pesadas':\n        # Distribuci√≥n t con colas pesadas\n        error = stats.t.rvs(df=3, scale=2, size=n)\n        titulo = \"Errores con Colas Pesadas ‚ùå\"\n    elif tipo == 'bimodal':\n        # Distribuci√≥n bimodal\n        mask = np.random.binomial(1, 0.5, n).astype(bool)\n        error = np.where(mask, \n                        np.random.normal(-2, 1, n),\n                        np.random.normal(2, 1, n))\n        titulo = \"Errores Bimodales ‚ùå\"\n    \n    y = 2 + 1.5 * x + error\n    return x, y, error, titulo\n\n# Crear diferentes tipos de no normalidad\ntipos_norm = ['normal', 'asimetrico', 'colas_pesadas', 'bimodal']\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\nfig.suptitle('Violaci√≥n del Supuesto de Normalidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos_norm):\n    x, y, error_real, titulo = crear_datos_no_normales(tipo=tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    color = 'green' if tipo == 'normal' else 'red'\n    \n    # Fila 1: Datos originales\n    axes[0, i].scatter(x, y, alpha=0.6, color=color)\n    axes[0, i].plot(x, y_pred, 'black', linewidth=2)\n    axes[0, i].set_title(titulo)\n    axes[0, i].set_xlabel('X')\n    axes[0, i].set_ylabel('Y')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Fila 2: Histograma de residuos\n    axes[1, i].hist(residuos, bins=25, density=True, alpha=0.7, color=color, edgecolor='black')\n    \n    # Superponer distribuci√≥n normal te√≥rica\n    x_norm = np.linspace(residuos.min(), residuos.max(), 100)\n    y_norm = stats.norm.pdf(x_norm, residuos.mean(), residuos.std())\n    axes[1, i].plot(x_norm, y_norm, 'blue', linewidth=2, label='Normal te√≥rica')\n    \n    axes[1, i].set_title(f'Histograma de Residuos')\n    axes[1, i].set_xlabel('Residuos')\n    axes[1, i].set_ylabel('Densidad')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n    \n    # Fila 3: Q-Q plot\n    stats.probplot(residuos, dist=\"norm\", plot=axes[2, i])\n    axes[2, i].set_title(f'Q-Q Plot')\n    axes[2, i].grid(True, alpha=0.3)\n    \n    # Calcular test de normalidad\n    shapiro_stat, shapiro_p = stats.shapiro(residuos)\n    axes[2, i].text(0.05, 0.95, f'Shapiro p={shapiro_p:.3f}', \n                    transform=axes[2, i].transAxes,\n                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä INTERPRETACI√ìN:\")\nprint(\"‚úÖ Q-Q plot lineal + Shapiro p &gt; 0.05 = Normalidad\")\nprint(\"‚ùå Q-Q plot no lineal + Shapiro p &lt; 0.05 = No normalidad\")\nprint(\"\\nüîç PATRONES EN Q-Q PLOTS:\")\nprint(\"- Curva S: Asimetr√≠a\")\nprint(\"- Colas alejadas de la l√≠nea: Colas pesadas/ligeras\")\nprint(\"- M√∫ltiples segmentos: Multimodalidad\")\n\n\n\n\n\n\n\n\nüìä INTERPRETACI√ìN:\n‚úÖ Q-Q plot lineal + Shapiro p &gt; 0.05 = Normalidad\n‚ùå Q-Q plot no lineal + Shapiro p &lt; 0.05 = No normalidad\n\nüîç PATRONES EN Q-Q PLOTS:\n- Curva S: Asimetr√≠a\n- Colas alejadas de la l√≠nea: Colas pesadas/ligeras\n- M√∫ltiples segmentos: Multimodalidad\n\n\n\nResumen de Pruebas Estad√≠sticas para Normalidad\n\nfrom scipy.stats import jarque_bera, anderson\n\nprint(\"üß™ PRUEBAS DE NORMALIDAD EN RESIDUOS\\n\")\nprint(\"H‚ÇÄ: Los residuos siguen distribuci√≥n normal\")\nprint(\"H‚ÇÅ: Los residuos NO siguen distribuci√≥n normal\\n\")\nprint(f\"{'Tipo':15} | {'Shapiro':&gt;10} | {'Jarque-Bera':&gt;12} | {'Anderson':&gt;10} | {'Conclusi√≥n'}\")\nprint(\"-\" * 75)\n\nfor tipo in tipos_norm:\n    x, y, _, titulo = crear_datos_no_normales(tipo=tipo)\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    residuos = y - lr.predict(x.reshape(-1, 1))\n    \n    # Shapiro-Wilk test\n    shapiro_stat, shapiro_p = stats.shapiro(residuos)\n    \n    # Jarque-Bera test\n    jb_stat, jb_p = jarque_bera(residuos)\n    \n    # Anderson-Darling test\n    ad_stat, ad_critical, ad_significance = anderson(residuos, dist='norm')\n    ad_result = \"Rechaza\" if ad_stat &gt; ad_critical[2] else \"No rechaza\"  # 5% nivel\n    \n    # Conclusi√≥n general\n    tests_reject = sum([shapiro_p &lt; 0.05, jb_p &lt; 0.05, ad_stat &gt; ad_critical[2]])\n    \n    if tests_reject &gt;= 2:\n        conclusion = \"‚ùå No Normal\"\n    elif tests_reject == 1:\n        conclusion = \"‚ö†Ô∏è  Dudoso\"\n    else:\n        conclusion = \"‚úÖ Normal\"\n    \n    print(f\"{tipo:15} | {shapiro_p:10.4f} | {jb_p:12.4f} | {ad_result:&gt;10} | {conclusion}\")\n\nprint(\"\\nüí° Interpretaci√≥n: p-value &lt; 0.05 indica violaci√≥n de normalidad\")\nprint(\"üí° Anderson-Darling: Estad√≠stico &gt; valor cr√≠tico indica no normalidad\")\n\nüß™ PRUEBAS DE NORMALIDAD EN RESIDUOS\n\nH‚ÇÄ: Los residuos siguen distribuci√≥n normal\nH‚ÇÅ: Los residuos NO siguen distribuci√≥n normal\n\nTipo            |    Shapiro |  Jarque-Bera |   Anderson | Conclusi√≥n\n---------------------------------------------------------------------------\nnormal          |     0.7354 |       0.7293 | No rechaza | ‚úÖ Normal\nasimetrico      |     0.0000 |       0.0000 |    Rechaza | ‚ùå No Normal\ncolas_pesadas   |     0.0003 |       0.0000 |    Rechaza | ‚ùå No Normal\nbimodal         |     0.0000 |       0.0010 |    Rechaza | ‚ùå No Normal\n\nüí° Interpretaci√≥n: p-value &lt; 0.05 indica violaci√≥n de normalidad\nüí° Anderson-Darling: Estad√≠stico &gt; valor cr√≠tico indica no normalidad",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#consecuencias-de-violar-los-supuestos",
    "href": "violaciones_supuestos_regresion.html#consecuencias-de-violar-los-supuestos",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "5. Consecuencias de Violar los Supuestos",
    "text": "5. Consecuencias de Violar los Supuestos\nVamos a demostrar qu√© pasa cuando violamos cada supuesto en t√©rminos de: - Precisi√≥n de las predicciones - Validez de los intervalos de confianza - Confiabilidad de las pruebas de hip√≥tesis\n\ndef simular_consecuencias(n_simulaciones=1000, n_datos=100):\n    \"\"\"Simula las consecuencias de violar supuestos\"\"\"\n    \n    resultados = {\n        'correcto': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'no_lineal': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'heteroscedastico': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'no_normal': {'mse': [], 'coef_estimados': [], 'p_values': []}\n    }\n    \n    coef_verdadero = 1.5  # Coeficiente real\n    \n    for _ in range(n_simulaciones):\n        x = np.linspace(0, 10, n_datos)\n        X = x.reshape(-1, 1)\n        \n        # Caso correcto\n        y_correcto = 2 + coef_verdadero * x + np.random.normal(0, 1, n_datos)\n        lr_correcto = LinearRegression().fit(X, y_correcto)\n        \n        # Caso no lineal (usando modelo lineal en datos cuadr√°ticos)\n        y_no_lineal = 2 + 0.5 * x**2 + np.random.normal(0, 1, n_datos)\n        lr_no_lineal = LinearRegression().fit(X, y_no_lineal)\n        \n        # Caso heterosced√°stico\n        y_hetero = 2 + coef_verdadero * x + np.random.normal(0, 0.1 + 0.2 * x, n_datos)\n        lr_hetero = LinearRegression().fit(X, y_hetero)\n        \n        # Caso no normal (distribuci√≥n t)\n        y_no_normal = 2 + coef_verdadero * x + stats.t.rvs(df=3, scale=1, size=n_datos)\n        lr_no_normal = LinearRegression().fit(X, y_no_normal)\n        \n        # Calcular m√©tricas para datos de prueba\n        x_test = np.linspace(0, 10, 50).reshape(-1, 1)\n        y_test_verdadero = 2 + coef_verdadero * x_test.flatten()\n        \n        # MSE en datos de prueba\n        resultados['correcto']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_correcto.predict(x_test))\n        )\n        resultados['no_lineal']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_no_lineal.predict(x_test))\n        )\n        resultados['heteroscedastico']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_hetero.predict(x_test))\n        )\n        resultados['no_normal']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_no_normal.predict(x_test))\n        )\n        \n        # Coeficientes estimados\n        resultados['correcto']['coef_estimados'].append(lr_correcto.coef_[0])\n        resultados['no_lineal']['coef_estimados'].append(lr_no_lineal.coef_[0])\n        resultados['heteroscedastico']['coef_estimados'].append(lr_hetero.coef_[0])\n        resultados['no_normal']['coef_estimados'].append(lr_no_normal.coef_[0])\n    \n    return resultados\n\nprint(\"üîÑ Ejecutando simulaci√≥n (esto puede tomar unos segundos...)\")\nresultados_sim = simular_consecuencias(n_simulaciones=500)\n\n# Crear gr√°ficos de resultados\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Consecuencias de Violar Supuestos de Regresi√≥n', fontsize=16, fontweight='bold')\n\ntipos_casos = ['correcto', 'no_lineal', 'heteroscedastico', 'no_normal']\ncolores = ['green', 'red', 'orange', 'purple']\nnombres = ['Correcto ‚úÖ', 'No Lineal ‚ùå', 'Heterosced√°stico ‚ùå', 'No Normal ‚ùå']\n\n# MSE Distribuci√≥n\nmse_data = [resultados_sim[caso]['mse'] for caso in tipos_casos]\naxes[0, 0].boxplot(mse_data, labels=nombres)\naxes[0, 0].set_title('Distribuci√≥n del Error de Predicci√≥n (MSE)')\naxes[0, 0].set_ylabel('MSE')\naxes[0, 0].tick_params(axis='x', rotation=45)\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribuci√≥n de coeficientes estimados\ncoef_data = [resultados_sim[caso]['coef_estimados'] for caso in tipos_casos]\naxes[0, 1].boxplot(coef_data, labels=nombres)\naxes[0, 1].axhline(y=1.5, color='black', linestyle='--', label='Valor verdadero')\naxes[0, 1].set_title('Distribuci√≥n de Coeficientes Estimados')\naxes[0, 1].set_ylabel('Coeficiente Œ≤‚ÇÅ')\naxes[0, 1].tick_params(axis='x', rotation=45)\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Histograma comparativo de MSE\nfor i, (caso, color, nombre) in enumerate(zip(tipos_casos, colores, nombres)):\n    axes[1, 0].hist(resultados_sim[caso]['mse'], bins=30, alpha=0.6, \n                    color=color, label=nombre, density=True)\naxes[1, 0].set_title('Comparaci√≥n de Distribuciones de MSE')\naxes[1, 0].set_xlabel('MSE')\naxes[1, 0].set_ylabel('Densidad')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Estad√≠sticas resumidas\naxes[1, 1].axis('off')\ntabla_texto = \"RESUMEN ESTAD√çSTICO:\\n\\n\"\ntabla_texto += f\"{'Caso':15} | {'MSE Promedio':&gt;12} | {'Sesgo Œ≤‚ÇÅ':&gt;10}\\n\"\ntabla_texto += \"-\" * 45 + \"\\n\"\n\nfor caso, nombre in zip(tipos_casos, nombres):\n    mse_prom = np.mean(resultados_sim[caso]['mse'])\n    sesgo = np.mean(resultados_sim[caso]['coef_estimados']) - 1.5\n    tabla_texto += f\"{nombre:15} | {mse_prom:12.4f} | {sesgo:10.4f}\\n\"\n\naxes[1, 1].text(0.1, 0.9, tabla_texto, transform=axes[1, 1].transAxes, \n                fontfamily='monospace', fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüìä CONCLUSIONES:\")\nprint(\"‚úÖ Supuestos correctos: Estimaciones insesgadas y predicciones precisas\")\nprint(\"‚ùå No linealidad: Mayor error de predicci√≥n y estimaciones sesgadas\")\nprint(\"‚ùå Heterocedasticidad: Intervalos de confianza incorrectos\")\nprint(\"‚ùå No normalidad: Pruebas de hip√≥tesis no confiables\")\n\nüîÑ Ejecutando simulaci√≥n (esto puede tomar unos segundos...)\n\n\n\n\n\n\n\n\n\n\nüìä CONCLUSIONES:\n‚úÖ Supuestos correctos: Estimaciones insesgadas y predicciones precisas\n‚ùå No linealidad: Mayor error de predicci√≥n y estimaciones sesgadas\n‚ùå Heterocedasticidad: Intervalos de confianza incorrectos\n‚ùå No normalidad: Pruebas de hip√≥tesis no confiables",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#ejercicios-pr√°cticos",
    "href": "violaciones_supuestos_regresion.html#ejercicios-pr√°cticos",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "6. Ejercicios Pr√°cticos",
    "text": "6. Ejercicios Pr√°cticos\n\nEjercicio 1: Identificaci√≥n de Violaciones\nExamina los siguientes conjuntos de datos y determina qu√© supuestos se violan:\n\n# Generar datos de ejercicio\nnp.random.seed(123)\n\ndef generar_datos_ejercicio(tipo, n=150):\n    x = np.linspace(0, 10, n)\n    \n    if tipo == 'A':\n        # M√∫ltiples violaciones\n        y = 2 + 0.5 * x**1.5 + np.random.normal(0, 0.1 + 0.3 * x, n)\n    elif tipo == 'B':\n        # Solo heterocedasticidad\n        y = 1 + 2 * x + np.random.normal(0, 0.5 * np.sqrt(x + 1), n)\n    elif tipo == 'C':\n        # Solo no normalidad\n        y = 3 + 1.5 * x + stats.chi2.rvs(df=2, size=n) - 2\n    elif tipo == 'D':\n        # Datos correctos\n        y = 1 + 2 * x + np.random.normal(0, 1.5, n)\n    \n    return x, y\n\n# Crear ejercicios\nfig, axes = plt.subplots(4, 3, figsize=(18, 20))\nfig.suptitle('EJERCICIO: Identifica las Violaciones de Supuestos', fontsize=16, fontweight='bold')\n\ntipos_ejercicio = ['A', 'B', 'C', 'D']\n\nfor i, tipo in enumerate(tipos_ejercicio):\n    x, y = generar_datos_ejercicio(tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    # Columna 1: Datos y modelo\n    axes[i, 0].scatter(x, y, alpha=0.6)\n    axes[i, 0].plot(x, y_pred, 'r-', linewidth=2)\n    axes[i, 0].set_title(f'Conjunto {tipo}: Datos y Modelo')\n    axes[i, 0].set_xlabel('X')\n    axes[i, 0].set_ylabel('Y')\n    axes[i, 0].grid(True, alpha=0.3)\n    \n    # Columna 2: Residuos vs Predichos\n    axes[i, 1].scatter(y_pred, residuos, alpha=0.6)\n    axes[i, 1].axhline(y=0, color='red', linestyle='--')\n    axes[i, 1].set_title(f'Conjunto {tipo}: Residuos vs Predichos')\n    axes[i, 1].set_xlabel('Valores Predichos')\n    axes[i, 1].set_ylabel('Residuos')\n    axes[i, 1].grid(True, alpha=0.3)\n    \n    # Columna 3: Q-Q Plot\n    stats.probplot(residuos, dist=\"norm\", plot=axes[i, 2])\n    axes[i, 2].set_title(f'Conjunto {tipo}: Q-Q Plot')\n    axes[i, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"ü§î PREGUNTA: ¬øQu√© supuestos se violan en cada conjunto?\")\nprint(\"\\nüìù Analiza cada fila:\")\nprint(\"- Conjunto A: ¬øQu√© patrones observas?\")\nprint(\"- Conjunto B: ¬øLos residuos tienen varianza constante?\")\nprint(\"- Conjunto C: ¬øLos residuos siguen distribuci√≥n normal?\")\nprint(\"- Conjunto D: ¬øEste conjunto cumple todos los supuestos?\")\nprint(\"\\nüí° Pista: Examina los patrones en residuos y la forma del Q-Q plot\")\n\n\n\n\n\n\n\n\nü§î PREGUNTA: ¬øQu√© supuestos se violan en cada conjunto?\n\nüìù Analiza cada fila:\n- Conjunto A: ¬øQu√© patrones observas?\n- Conjunto B: ¬øLos residuos tienen varianza constante?\n- Conjunto C: ¬øLos residuos siguen distribuci√≥n normal?\n- Conjunto D: ¬øEste conjunto cumple todos los supuestos?\n\nüí° Pista: Examina los patrones en residuos y la forma del Q-Q plot",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#resumen-y-recomendaciones",
    "href": "violaciones_supuestos_regresion.html#resumen-y-recomendaciones",
    "title": "Violaciones de los Supuestos de Regresi√≥n Lineal",
    "section": "7. Resumen y Recomendaciones",
    "text": "7. Resumen y Recomendaciones\n\n¬øC√≥mo diagnosticar violaciones?\n\n\n\n\n\n\n\n\nSupuesto\nHerramientas de Diagn√≥stico\nQu√© Buscar\n\n\n\n\nLinealidad\nResiduos vs PredichosGr√°ficos parciales\nPatrones curvosTendencias sistem√°ticas\n\n\nIndependencia\nResiduos vs TiempoAutocorrelaci√≥n\nPatrones temporalesCorrelaci√≥n serial\n\n\nHomocedasticidad\nResiduos vs PredichosTest Breusch-Pagan\nForma de embudop-value &lt; 0.05\n\n\nNormalidad\nQ-Q PlotTest Shapiro-Wilk\nDesviaci√≥n de l√≠nea rectap-value &lt; 0.05\n\n\n\n\n\n¬øQu√© hacer cuando se violan?\n\n\n\n\n\n\n\nViolaci√≥n\nSoluciones Posibles\n\n\n\n\nNo Linealidad\n‚Ä¢ Transformaciones (log, cuadr√°tica)‚Ä¢ Modelos no lineales‚Ä¢ Splines, polinomios\n\n\nDependencia\n‚Ä¢ Modelos de series temporales‚Ä¢ Errores est√°ndar robustos‚Ä¢ GLS con estructura de correlaci√≥n\n\n\nHeterocedasticidad\n‚Ä¢ Transformaciones (log Y)‚Ä¢ Errores est√°ndar robustos‚Ä¢ M√≠nimos cuadrados ponderados\n\n\nNo Normalidad\n‚Ä¢ Transformaciones‚Ä¢ M√©todos no param√©tricos‚Ä¢ Modelos robustos\n\n\n\n\n\nüéØ Puntos Clave para Recordar\n\nLos gr√°ficos de residuos son tu mejor amigo para diagnosticar problemas\nLas violaciones no siempre son fatales - depende de tu objetivo\nPara predicci√≥n: La no normalidad es menos cr√≠tica\nPara inferencia: Todos los supuestos son importantes\nSiempre visualiza antes de modelar y despu√©s de ajustar\n\n¬°Felicidades! üéâ Ahora tienes las herramientas para identificar y entender las violaciones de los supuestos de regresi√≥n lineal.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Violaciones de los Supuestos de Regresi√≥n Lineal</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html",
    "href": "analisis_advertising_dataset.html",
    "title": "An√°lisis de Regresi√≥n Lineal con el Dataset Advertising",
    "section": "",
    "text": "Descripci√≥n del Dataset\nEl dataset Advertising es un conjunto de datos cl√°sico utilizado para ense√±ar conceptos de regresi√≥n lineal. Contiene informaci√≥n sobre presupuestos de publicidad en tres medios diferentes (TV, Radio y Peri√≥dico) y las ventas resultantes de productos.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lisis de Regresi√≥n Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#descripci√≥n-del-dataset",
    "href": "analisis_advertising_dataset.html#descripci√≥n-del-dataset",
    "title": "An√°lisis de Regresi√≥n Lineal con el Dataset Advertising",
    "section": "",
    "text": "Variables del dataset:\n\nTV: Presupuesto de publicidad en televisi√≥n (en miles de d√≥lares)\nRadio: Presupuesto de publicidad en radio (en miles de d√≥lares)\nNewspaper: Presupuesto de publicidad en peri√≥dicos (en miles de d√≥lares)\nSales: Ventas del producto (en miles de unidades)\n\nEl objetivo es predecir las ventas bas√°ndose en los presupuestos de publicidad, lo que permite a las empresas optimizar su inversi√≥n en marketing.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lisis de Regresi√≥n Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#importar-librer√≠as-y-descargar-datos",
    "href": "analisis_advertising_dataset.html#importar-librer√≠as-y-descargar-datos",
    "title": "An√°lisis de Regresi√≥n Lineal con el Dataset Advertising",
    "section": "1. Importar librer√≠as y descargar datos",
    "text": "1. Importar librer√≠as y descargar datos\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurar estilo de gr√°ficos\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n\n# Descargar y leer el dataset\nurl = 'https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/Advertising.csv'\nadvertising = pd.read_csv(url, index_col=0)\n\nprint(\"Dataset cargado exitosamente!\")\nprint(f\"Dimensiones del dataset: {advertising.shape}\")\nprint(f\"Columnas: {advertising.columns.tolist()}\")\n\nDataset cargado exitosamente!\nDimensiones del dataset: (200, 4)\nColumnas: ['TV', 'Radio', 'Newspaper', 'Sales']",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lisis de Regresi√≥n Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#an√°lisis-exploratorio-de-datos-eda",
    "href": "analisis_advertising_dataset.html#an√°lisis-exploratorio-de-datos-eda",
    "title": "An√°lisis de Regresi√≥n Lineal con el Dataset Advertising",
    "section": "2. An√°lisis Exploratorio de Datos (EDA)",
    "text": "2. An√°lisis Exploratorio de Datos (EDA)\n\n# Primeras filas del dataset\nadvertising.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n1\n230.1\n37.8\n69.2\n22.1\n\n\n2\n44.5\n39.3\n45.1\n10.4\n\n\n3\n17.2\n45.9\n69.3\n9.3\n\n\n4\n151.5\n41.3\n58.5\n18.5\n\n\n5\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n\n\n\n\n# Informaci√≥n general del dataset\nadvertising.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 200 entries, 1 to 200\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   TV         200 non-null    float64\n 1   Radio      200 non-null    float64\n 2   Newspaper  200 non-null    float64\n 3   Sales      200 non-null    float64\ndtypes: float64(4)\nmemory usage: 7.8 KB\n\n\n\n# Estad√≠sticas descriptivas\nadvertising.describe().round(2)\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\ncount\n200.00\n200.00\n200.00\n200.00\n\n\nmean\n147.04\n23.26\n30.55\n14.02\n\n\nstd\n85.85\n14.85\n21.78\n5.22\n\n\nmin\n0.70\n0.00\n0.30\n1.60\n\n\n25%\n74.38\n9.98\n12.75\n10.38\n\n\n50%\n149.75\n22.90\n25.75\n12.90\n\n\n75%\n218.82\n36.52\n45.10\n17.40\n\n\nmax\n296.40\n49.60\n114.00\n27.00\n\n\n\n\n\n\n\n\n# Verificar valores nulos\nprint(\"Valores nulos por columna:\")\nprint(advertising.isnull().sum())\n\nValores nulos por columna:\nTV           0\nRadio        0\nNewspaper    0\nSales        0\ndtype: int64\n\n\n\n# Matriz de correlaci√≥n\ncorrelation_matrix = advertising.corr()\nprint(\"\\nMatriz de correlaci√≥n:\")\nprint(correlation_matrix.round(3))\n\n\nMatriz de correlaci√≥n:\n              TV  Radio  Newspaper  Sales\nTV         1.000  0.055      0.057  0.782\nRadio      0.055  1.000      0.354  0.576\nNewspaper  0.057  0.354      1.000  0.228\nSales      0.782  0.576      0.228  1.000\n\n\n\n# Visualizaci√≥n de la matriz de correlaci√≥n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=1, fmt='.2f')\nplt.title('Matriz de Correlaci√≥n - Dataset Advertising', fontsize=16, pad=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Distribuci√≥n de las variables\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Distribuci√≥n de Variables', fontsize=16, y=1.02)\n\nvariables = ['TV', 'Radio', 'Newspaper', 'Sales']\ncolors = ['steelblue', 'coral', 'lightgreen', 'gold']\n\nfor idx, (ax, var, color) in enumerate(zip(axes.flat, variables, colors)):\n    ax.hist(advertising[var], bins=20, edgecolor='black', alpha=0.7, color=color)\n    ax.set_title(f'Distribuci√≥n de {var}', fontsize=12)\n    ax.set_xlabel(var)\n    ax.set_ylabel('Frecuencia')\n    ax.grid(True, alpha=0.3)\n    \n    # Agregar estad√≠sticas\n    mean_val = advertising[var].mean()\n    median_val = advertising[var].median()\n    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Media: {mean_val:.2f}')\n    ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Mediana: {median_val:.2f}')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Relaci√≥n entre cada variable predictora y las ventas\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfig.suptitle('Relaci√≥n entre Variables Predictoras y Ventas', fontsize=16, y=1.02)\n\npredictors = ['TV', 'Radio', 'Newspaper']\ncolors = ['steelblue', 'coral', 'lightgreen']\n\nfor ax, predictor, color in zip(axes, predictors, colors):\n    ax.scatter(advertising[predictor], advertising['Sales'], alpha=0.6, color=color, edgecolors='black', linewidth=0.5)\n    ax.set_xlabel(f'{predictor} (miles de d√≥lares)', fontsize=11)\n    ax.set_ylabel('Sales (miles de unidades)', fontsize=11)\n    ax.set_title(f'{predictor} vs Sales', fontsize=12)\n    ax.grid(True, alpha=0.3)\n    \n    # Agregar l√≠nea de tendencia\n    z = np.polyfit(advertising[predictor], advertising['Sales'], 1)\n    p = np.poly1d(z)\n    ax.plot(advertising[predictor].sort_values(), p(advertising[predictor].sort_values()), \n            \"r--\", alpha=0.8, linewidth=2)\n    \n    # Agregar correlaci√≥n\n    corr = advertising[[predictor, 'Sales']].corr().iloc[0, 1]\n    ax.text(0.05, 0.95, f'Correlaci√≥n: {corr:.3f}', transform=ax.transAxes, \n            fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pairplot para visualizar todas las relaciones\nplt.figure(figsize=(12, 10))\nsns.pairplot(advertising, diag_kind='kde', corner=True, \n             plot_kws={'alpha': 0.6, 'edgecolor': 'black', 'linewidth': 0.5})\nplt.suptitle('Matriz de Dispersi√≥n - Dataset Advertising', y=1.02, fontsize=16)\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 1200x1000 with 0 Axes&gt;",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lisis de Regresi√≥n Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#preparaci√≥n-de-datos-divisi√≥n-train-test",
    "href": "analisis_advertising_dataset.html#preparaci√≥n-de-datos-divisi√≥n-train-test",
    "title": "An√°lisis de Regresi√≥n Lineal con el Dataset Advertising",
    "section": "3. Preparaci√≥n de Datos: Divisi√≥n Train-Test",
    "text": "3. Preparaci√≥n de Datos: Divisi√≥n Train-Test\n\n# Separar caracter√≠sticas (X) y variable objetivo (y)\nX = advertising[['TV', 'Radio', 'Newspaper']]\ny = advertising['Sales']\n\nprint(\"Caracter√≠sticas (X):\")\nprint(X.head())\nprint(f\"\\nForma de X: {X.shape}\")\nprint(f\"Forma de y: {y.shape}\")\n\nCaracter√≠sticas (X):\n      TV  Radio  Newspaper\n1  230.1   37.8       69.2\n2   44.5   39.3       45.1\n3   17.2   45.9       69.3\n4  151.5   41.3       58.5\n5  180.8   10.8       58.4\n\nForma de X: (200, 3)\nForma de y: (200,)\n\n\n\n# Divisi√≥n en conjuntos de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Divisi√≥n de datos completada:\")\nprint(f\"Tama√±o del conjunto de entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Tama√±o del conjunto de prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n\nDivisi√≥n de datos completada:\nTama√±o del conjunto de entrenamiento: 160 muestras (80.0%)\nTama√±o del conjunto de prueba: 40 muestras (20.0%)\n\n\n\n# Estandarizaci√≥n de caracter√≠sticas para Ridge y Lasso\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"Datos estandarizados para Ridge y Lasso\")\nprint(f\"Media de X_train_scaled: {X_train_scaled.mean(axis=0).round(10)}\")\nprint(f\"Desviaci√≥n est√°ndar de X_train_scaled: {X_train_scaled.std(axis=0).round(2)}\")\n\nDatos estandarizados para Ridge y Lasso\nMedia de X_train_scaled: [-0. -0.  0.]\nDesviaci√≥n est√°ndar de X_train_scaled: [1. 1. 1.]",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lisis de Regresi√≥n Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#modelos-de-regresi√≥n",
    "href": "analisis_advertising_dataset.html#modelos-de-regresi√≥n",
    "title": "An√°lisis de Regresi√≥n Lineal con el Dataset Advertising",
    "section": "4. Modelos de Regresi√≥n",
    "text": "4. Modelos de Regresi√≥n\n\n4.1 Regresi√≥n Lineal Normal (OLS)\n\n# Entrenar modelo de regresi√≥n lineal\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# Predicciones\ny_pred_lr_train = lr_model.predict(X_train)\ny_pred_lr_test = lr_model.predict(X_test)\n\n# M√©tricas\nmse_lr_train = mean_squared_error(y_train, y_pred_lr_train)\nmse_lr_test = mean_squared_error(y_test, y_pred_lr_test)\nrmse_lr_train = np.sqrt(mse_lr_train)\nrmse_lr_test = np.sqrt(mse_lr_test)\nr2_lr_train = r2_score(y_train, y_pred_lr_train)\nr2_lr_test = r2_score(y_test, y_pred_lr_test)\nmae_lr_train = mean_absolute_error(y_train, y_pred_lr_train)\nmae_lr_test = mean_absolute_error(y_test, y_pred_lr_test)\n\n\nprint(\"=\"*50)\nprint(\"REGRESI√ìN LINEAL NORMAL (OLS)\")\nprint(\"=\"*50)\nprint(f\"\\nCoeficientes:\")\nfor feature, coef in zip(X.columns, lr_model.coef_):\n    print(f\"  {feature}: {coef:.6f}\")\nprint(f\"\\nIntercepto: {lr_model.intercept_:.6f}\")\nprint(f\"\\nM√©tricas en Entrenamiento:\")\nprint(f\"  RMSE: {rmse_lr_train:.4f}\")\nprint(f\"  R¬≤: {r2_lr_train:.4f}\")\nprint(f\"  MAE: {mae_lr_train:.4f}\")\nprint(f\"\\nM√©tricas en Prueba:\")\nprint(f\"  RMSE: {rmse_lr_test:.4f}\")\nprint(f\"  R¬≤: {r2_lr_test:.4f}\")\nprint(f\"  MAE: {mae_lr_test:.4f}\")\n\n==================================================\nREGRESI√ìN LINEAL NORMAL (OLS)\n==================================================\n\nCoeficientes:\n  TV: 0.044730\n  Radio: 0.189195\n  Newspaper: 0.002761\n\nIntercepto: 2.979067\n\nM√©tricas en Entrenamiento:\n  RMSE: 1.6447\n  R¬≤: 0.8957\n  MAE: 1.1985\n\nM√©tricas en Prueba:\n  RMSE: 1.7816\n  R¬≤: 0.8994\n  MAE: 1.4608\n\n\n\n\n4.2 Regresi√≥n Ridge\n\n# Probar diferentes valores de alpha\nalphas_ridge = [0.001, 0.01, 0.1, 1, 10, 100]\nridge_results = []\n\nfor alpha in alphas_ridge:\n    ridge_model = Ridge(alpha=alpha, random_state=42)\n    ridge_model.fit(X_train_scaled, y_train)\n    \n    y_pred_train = ridge_model.predict(X_train_scaled)\n    y_pred_test = ridge_model.predict(X_test_scaled)\n    \n    ridge_results.append({\n        'alpha': alpha,\n        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n        'r2_train': r2_score(y_train, y_pred_train),\n        'r2_test': r2_score(y_test, y_pred_test)\n    })\n\n# Convertir a DataFrame para mejor visualizaci√≥n\nridge_df = pd.DataFrame(ridge_results)\nprint(\"Resultados de Ridge con diferentes valores de alpha:\")\nprint(ridge_df.to_string(index=False))\n\nResultados de Ridge con diferentes valores de alpha:\n  alpha  rmse_train  rmse_test  r2_train  r2_test\n  0.001    1.644728   1.781605  0.895701 0.899437\n  0.010    1.644728   1.781654  0.895701 0.899432\n  0.100    1.644730   1.782143  0.895700 0.899377\n  1.000    1.644987   1.787220  0.895668 0.898803\n 10.000    1.667636   1.853716  0.892775 0.891132\n100.000    2.427323   2.786288  0.772832 0.754039\n\n\n\n# Seleccionar el mejor alpha basado en RMSE de prueba\nbest_alpha_ridge = ridge_df.loc[ridge_df['rmse_test'].idxmin(), 'alpha']\nprint(f\"\\nMejor alpha para Ridge: {best_alpha_ridge}\")\n\n# Entrenar modelo final con mejor alpha\nridge_model_best = Ridge(alpha=best_alpha_ridge, random_state=42)\nridge_model_best.fit(X_train_scaled, y_train)\n\n# Predicciones\ny_pred_ridge_train = ridge_model_best.predict(X_train_scaled)\ny_pred_ridge_test = ridge_model_best.predict(X_test_scaled)\n\n# M√©tricas\nmse_ridge_train = mean_squared_error(y_train, y_pred_ridge_train)\nmse_ridge_test = mean_squared_error(y_test, y_pred_ridge_test)\nrmse_ridge_train = np.sqrt(mse_ridge_train)\nrmse_ridge_test = np.sqrt(mse_ridge_test)\nr2_ridge_train = r2_score(y_train, y_pred_ridge_train)\nr2_ridge_test = r2_score(y_test, y_pred_ridge_test)\nmae_ridge_train = mean_absolute_error(y_train, y_pred_ridge_train)\nmae_ridge_test = mean_absolute_error(y_test, y_pred_ridge_test)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"REGRESI√ìN RIDGE (alpha={best_alpha_ridge})\")\nprint(\"=\"*50)\nprint(f\"\\nCoeficientes:\")\nfor feature, coef in zip(X.columns, ridge_model_best.coef_):\n    print(f\"  {feature}: {coef:.6f}\")\nprint(f\"\\nIntercepto: {ridge_model_best.intercept_:.6f}\")\nprint(f\"\\nM√©tricas en Entrenamiento:\")\nprint(f\"  RMSE: {rmse_ridge_train:.4f}\")\nprint(f\"  R¬≤: {r2_ridge_train:.4f}\")\nprint(f\"  MAE: {mae_ridge_train:.4f}\")\nprint(f\"\\nM√©tricas en Prueba:\")\nprint(f\"  RMSE: {rmse_ridge_test:.4f}\")\nprint(f\"  R¬≤: {r2_ridge_test:.4f}\")\nprint(f\"  MAE: {mae_ridge_test:.4f}\")\n\n\nMejor alpha para Ridge: 0.001\n\n==================================================\nREGRESI√ìN RIDGE (alpha=0.001)\n==================================================\n\nCoeficientes:\n  TV: 3.764174\n  Radio: 2.792288\n  Newspaper: 0.055983\n\nIntercepto: 14.100000\n\nM√©tricas en Entrenamiento:\n  RMSE: 1.6447\n  R¬≤: 0.8957\n  MAE: 1.1985\n\nM√©tricas en Prueba:\n  RMSE: 1.7816\n  R¬≤: 0.8994\n  MAE: 1.4608\n\n\n\n\n4.3 Regresi√≥n Lasso\n\n# Probar diferentes valores de alpha\nalphas_lasso = [0.001, 0.01, 0.1, 1, 10, 100]\nlasso_results = []\n\nfor alpha in alphas_lasso:\n    lasso_model = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n    lasso_model.fit(X_train_scaled, y_train)\n    \n    y_pred_train = lasso_model.predict(X_train_scaled)\n    y_pred_test = lasso_model.predict(X_test_scaled)\n    \n    lasso_results.append({\n        'alpha': alpha,\n        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n        'r2_train': r2_score(y_train, y_pred_train),\n        'r2_test': r2_score(y_test, y_pred_test),\n        'n_features': np.sum(lasso_model.coef_ != 0)\n    })\n\n# Convertir a DataFrame para mejor visualizaci√≥n\nlasso_df = pd.DataFrame(lasso_results)\nprint(\"Resultados de Lasso con diferentes valores de alpha:\")\nprint(lasso_df.to_string(index=False))\n\nResultados de Lasso con diferentes valores de alpha:\n  alpha  rmse_train  rmse_test  r2_train   r2_test  n_features\n  0.001    1.644728   1.781587  0.895701  0.899439           3\n  0.010    1.644799   1.781551  0.895692  0.899443           3\n  0.100    1.651293   1.791334  0.894867  0.898336           2\n  1.000    2.146055   2.396806  0.822428  0.817997           2\n 10.000    5.092764   5.631496  0.000000 -0.004757           0\n100.000    5.092764   5.631496  0.000000 -0.004757           0\n\n\n\n# Seleccionar el mejor alpha basado en RMSE de prueba\nbest_alpha_lasso = lasso_df.loc[lasso_df['rmse_test'].idxmin(), 'alpha']\nprint(f\"\\nMejor alpha para Lasso: {best_alpha_lasso}\")\n\n# Entrenar modelo final con mejor alpha\nlasso_model_best = Lasso(alpha=best_alpha_lasso, random_state=42, max_iter=10000)\nlasso_model_best.fit(X_train_scaled, y_train)\n\n# Predicciones\ny_pred_lasso_train = lasso_model_best.predict(X_train_scaled)\ny_pred_lasso_test = lasso_model_best.predict(X_test_scaled)\n\n# M√©tricas\nmse_lasso_train = mean_squared_error(y_train, y_pred_lasso_train)\nmse_lasso_test = mean_squared_error(y_test, y_pred_lasso_test)\nrmse_lasso_train = np.sqrt(mse_lasso_train)\nrmse_lasso_test = np.sqrt(mse_lasso_test)\nr2_lasso_train = r2_score(y_train, y_pred_lasso_train)\nr2_lasso_test = r2_score(y_test, y_pred_lasso_test)\nmae_lasso_train = mean_absolute_error(y_train, y_pred_lasso_train)\nmae_lasso_test = mean_absolute_error(y_test, y_pred_lasso_test)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"REGRESI√ìN LASSO (alpha={best_alpha_lasso})\")\nprint(\"=\"*50)\nprint(f\"\\nCoeficientes:\")\nfor feature, coef in zip(X.columns, lasso_model_best.coef_):\n    if coef != 0:\n        print(f\"  {feature}: {coef:.6f}\")\n    else:\n        print(f\"  {feature}: {coef:.6f} (eliminado)\")\nprint(f\"\\nIntercepto: {lasso_model_best.intercept_:.6f}\")\nprint(f\"\\nCaracter√≠sticas seleccionadas: {np.sum(lasso_model_best.coef_ != 0)} de {len(X.columns)}\")\nprint(f\"\\nM√©tricas en Entrenamiento:\")\nprint(f\"  RMSE: {rmse_lasso_train:.4f}\")\nprint(f\"  R¬≤: {r2_lasso_train:.4f}\")\nprint(f\"  MAE: {mae_lasso_train:.4f}\")\nprint(f\"\\nM√©tricas en Prueba:\")\nprint(f\"  RMSE: {rmse_lasso_test:.4f}\")\nprint(f\"  R¬≤: {r2_lasso_test:.4f}\")\nprint(f\"  MAE: {mae_lasso_test:.4f}\")\n\n\nMejor alpha para Lasso: 0.01\n\n==================================================\nREGRESI√ìN LASSO (alpha=0.01)\n==================================================\n\nCoeficientes:\n  TV: 3.754675\n  Radio: 2.785686\n  Newspaper: 0.048727\n\nIntercepto: 14.100000\n\nCaracter√≠sticas seleccionadas: 3 de 3\n\nM√©tricas en Entrenamiento:\n  RMSE: 1.6448\n  R¬≤: 0.8957\n  MAE: 1.1983\n\nM√©tricas en Prueba:\n  RMSE: 1.7816\n  R¬≤: 0.8994\n  MAE: 1.4590\n\n\n\n\n4.4 Ejercicio de clase\nAhora les toca implementar la validaci√≥n cruzada que se vio en Selecci√≥n de hiperpar√°metros",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lisis de Regresi√≥n Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#comparaci√≥n-de-modelos",
    "href": "analisis_advertising_dataset.html#comparaci√≥n-de-modelos",
    "title": "An√°lisis de Regresi√≥n Lineal con el Dataset Advertising",
    "section": "5. Comparaci√≥n de Modelos",
    "text": "5. Comparaci√≥n de Modelos\n\n# Crear tabla comparativa\ncomparison_data = {\n    'Modelo': ['Linear Regression', f'Ridge (Œ±={best_alpha_ridge})', f'Lasso (Œ±={best_alpha_lasso})'],\n    'RMSE Train': [rmse_lr_train, rmse_ridge_train, rmse_lasso_train],\n    'RMSE Test': [rmse_lr_test, rmse_ridge_test, rmse_lasso_test],\n    'R¬≤ Train': [r2_lr_train, r2_ridge_train, r2_lasso_train],\n    'R¬≤ Test': [r2_lr_test, r2_ridge_test, r2_lasso_test],\n    'MAE Test': [mae_lr_test, mae_ridge_test, mae_lasso_test]\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.round(4)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARACI√ìN DE MODELOS\")\nprint(\"=\"*70)\nprint(comparison_df.to_string(index=False))\n\n# Identificar el mejor modelo\nbest_model_idx = comparison_df['RMSE Test'].idxmin()\nbest_model_name = comparison_df.loc[best_model_idx, 'Modelo']\nprint(f\"\\nüèÜ Mejor modelo basado en RMSE de prueba: {best_model_name}\")\n\n\n======================================================================\nCOMPARACI√ìN DE MODELOS\n======================================================================\n           Modelo  RMSE Train  RMSE Test  R¬≤ Train  R¬≤ Test  MAE Test\nLinear Regression      1.6447     1.7816    0.8957   0.8994    1.4608\n  Ridge (Œ±=0.001)      1.6447     1.7816    0.8957   0.8994    1.4608\n   Lasso (Œ±=0.01)      1.6448     1.7816    0.8957   0.8994    1.4590\n\nüèÜ Mejor modelo basado en RMSE de prueba: Linear Regression\n\n\n\n# Visualizaci√≥n de m√©tricas\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfig.suptitle('Comparaci√≥n de M√©tricas entre Modelos', fontsize=16, y=1.05)\n\nmodels = comparison_df['Modelo'].tolist()\nx_pos = np.arange(len(models))\ncolors = ['steelblue', 'coral', 'lightgreen']\n\n# RMSE\nax1 = axes[0]\nwidth = 0.35\nbars1 = ax1.bar(x_pos - width/2, comparison_df['RMSE Train'], width, label='Train', color='lightblue', edgecolor='black')\nbars2 = ax1.bar(x_pos + width/2, comparison_df['RMSE Test'], width, label='Test', color='orange', edgecolor='black')\nax1.set_xlabel('Modelo')\nax1.set_ylabel('RMSE')\nax1.set_title('Root Mean Squared Error')\nax1.set_xticks(x_pos)\nax1.set_xticklabels(models, rotation=15, ha='right')\nax1.legend()\nax1.grid(True, alpha=0.3, axis='y')\n\n# R¬≤\nax2 = axes[1]\nbars3 = ax2.bar(x_pos - width/2, comparison_df['R¬≤ Train'], width, label='Train', color='lightblue', edgecolor='black')\nbars4 = ax2.bar(x_pos + width/2, comparison_df['R¬≤ Test'], width, label='Test', color='orange', edgecolor='black')\nax2.set_xlabel('Modelo')\nax2.set_ylabel('R¬≤')\nax2.set_title('Coeficiente de Determinaci√≥n (R¬≤)')\nax2.set_xticks(x_pos)\nax2.set_xticklabels(models, rotation=15, ha='right')\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\nax2.set_ylim([0, 1])\n\n# MAE\nax3 = axes[2]\nbars5 = ax3.bar(x_pos, comparison_df['MAE Test'], color=colors, edgecolor='black', alpha=0.7)\nax3.set_xlabel('Modelo')\nax3.set_ylabel('MAE')\nax3.set_title('Mean Absolute Error (Test)')\nax3.set_xticks(x_pos)\nax3.set_xticklabels(models, rotation=15, ha='right')\nax3.grid(True, alpha=0.3, axis='y')\n\n# Agregar valores en las barras\nfor bars in [bars1, bars2, bars3, bars4, bars5]:\n    for bar in bars:\n        height = bar.get_height()\n        if bars in [bars3, bars4]:  # Para R¬≤\n            ax = ax2\n        elif bars in [bars1, bars2]:  # Para RMSE\n            ax = ax1\n        else:  # Para MAE\n            ax = ax3\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualizaci√≥n de coeficientes\nfig, ax = plt.subplots(figsize=(12, 6))\n\nfeatures = X.columns.tolist()\nx_pos = np.arange(len(features))\nwidth = 0.25\n\n# Coeficientes de cada modelo\ncoef_lr = lr_model.coef_\ncoef_ridge = ridge_model_best.coef_\ncoef_lasso = lasso_model_best.coef_\n\nbars1 = ax.bar(x_pos - width, coef_lr, width, label='Linear Regression', color='steelblue', edgecolor='black')\nbars2 = ax.bar(x_pos, coef_ridge, width, label=f'Ridge (Œ±={best_alpha_ridge})', color='coral', edgecolor='black')\nbars3 = ax.bar(x_pos + width, coef_lasso, width, label=f'Lasso (Œ±={best_alpha_lasso})', color='lightgreen', edgecolor='black')\n\nax.set_xlabel('Caracter√≠sticas', fontsize=12)\nax.set_ylabel('Valor del Coeficiente', fontsize=12)\nax.set_title('Comparaci√≥n de Coeficientes entre Modelos', fontsize=14)\nax.set_xticks(x_pos)\nax.set_xticklabels(features)\nax.legend(loc='upper right')\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n\n# Agregar valores en las barras\nfor bars in [bars1, bars2, bars3]:\n    for bar in bars:\n        height = bar.get_height()\n        if abs(height) &gt; 0.001:\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{height:.3f}', ha='center', va='bottom' if height &gt; 0 else 'top', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Gr√°fico de predicciones vs valores reales para el mejor modelo\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nfig.suptitle('Predicciones vs Valores Reales (Conjunto de Prueba)', fontsize=16, y=1.02)\n\nmodels_pred = [\n    ('Linear Regression', y_pred_lr_test, 'steelblue'),\n    (f'Ridge (Œ±={best_alpha_ridge})', y_pred_ridge_test, 'coral'),\n    (f'Lasso (Œ±={best_alpha_lasso})', y_pred_lasso_test, 'lightgreen')\n]\n\nfor ax, (name, predictions, color) in zip(axes, models_pred):\n    ax.scatter(y_test, predictions, alpha=0.6, color=color, edgecolors='black', linewidth=0.5)\n    \n    # L√≠nea perfecta de predicci√≥n\n    min_val = min(y_test.min(), predictions.min())\n    max_val = max(y_test.max(), predictions.max())\n    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicci√≥n Perfecta')\n    \n    ax.set_xlabel('Valores Reales', fontsize=11)\n    ax.set_ylabel('Predicciones', fontsize=11)\n    ax.set_title(name, fontsize=12)\n    ax.grid(True, alpha=0.3)\n    ax.legend()\n    \n    # Agregar R¬≤ en el gr√°fico\n    r2 = r2_score(y_test, predictions)\n    ax.text(0.05, 0.95, f'R¬≤ = {r2:.4f}', transform=ax.transAxes,\n            fontsize=10, verticalalignment='top',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# An√°lisis de residuos\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('An√°lisis de Residuos', fontsize=16, y=1.02)\n\nmodels_resid = [\n    ('Linear Regression', y_pred_lr_test, 'steelblue'),\n    (f'Ridge (Œ±={best_alpha_ridge})', y_pred_ridge_test, 'coral'),\n    (f'Lasso (Œ±={best_alpha_lasso})', y_pred_lasso_test, 'lightgreen')\n]\n\nfor idx, (name, predictions, color) in enumerate(models_resid):\n    residuals = y_test - predictions\n    \n    # Gr√°fico de residuos vs predicciones\n    ax1 = axes[0, idx]\n    ax1.scatter(predictions, residuals, alpha=0.6, color=color, edgecolors='black', linewidth=0.5)\n    ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n    ax1.set_xlabel('Predicciones', fontsize=10)\n    ax1.set_ylabel('Residuos', fontsize=10)\n    ax1.set_title(f'{name} - Residuos vs Predicciones', fontsize=11)\n    ax1.grid(True, alpha=0.3)\n    \n    # Histograma de residuos\n    ax2 = axes[1, idx]\n    ax2.hist(residuals, bins=20, edgecolor='black', alpha=0.7, color=color)\n    ax2.set_xlabel('Residuos', fontsize=10)\n    ax2.set_ylabel('Frecuencia', fontsize=10)\n    ax2.set_title(f'{name} - Distribuci√≥n de Residuos', fontsize=11)\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    # Agregar l√≠nea de media\n    mean_resid = residuals.mean()\n    ax2.axvline(mean_resid, color='red', linestyle='--', linewidth=2, label=f'Media: {mean_resid:.3f}')\n    ax2.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lisis de Regresi√≥n Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#conclusiones",
    "href": "analisis_advertising_dataset.html#conclusiones",
    "title": "An√°lisis de Regresi√≥n Lineal con el Dataset Advertising",
    "section": "6. Conclusiones",
    "text": "6. Conclusiones\n\nResumen de Resultados:\n\nRegresi√≥n Lineal Normal (OLS):\n\nModelo base sin regularizaci√≥n\nUtiliza todos los predictores sin penalizaci√≥n\nPuede ser propenso al sobreajuste con m√°s caracter√≠sticas\n\nRidge Regression:\n\nAplica regularizaci√≥n L2 (penaliza la suma de los cuadrados de los coeficientes)\nMantiene todas las caracter√≠sticas pero reduce su magnitud\n√ötil cuando todas las caracter√≠sticas son potencialmente relevantes\n\nLasso Regression:\n\nAplica regularizaci√≥n L1 (penaliza la suma de los valores absolutos de los coeficientes)\nPuede llevar algunos coeficientes exactamente a cero (selecci√≥n de caracter√≠sticas)\n√ötil para identificar las caracter√≠sticas m√°s importantes\n\n\n\n\nObservaciones Clave:\n\nTV tiene la correlaci√≥n m√°s fuerte con las ventas\nRadio tambi√©n muestra una correlaci√≥n positiva significativa\nNewspaper tiene la correlaci√≥n m√°s d√©bil y puede ser eliminada por Lasso\nLos tres modelos tienen un rendimiento similar, lo que sugiere que el problema es relativamente simple\nLa regularizaci√≥n ayuda a prevenir el sobreajuste, especialmente con conjuntos de datos m√°s complejos\n\n\n# Interpretaci√≥n pr√°ctica del mejor modelo\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETACI√ìN PR√ÅCTICA DEL MODELO\")\nprint(\"=\"*70)\n\n# Usar coeficientes del modelo de regresi√≥n lineal para interpretaci√≥n\nprint(\"\\nInterpretaci√≥n de los coeficientes (Regresi√≥n Lineal):\")\nprint(\"-\" * 60)\nfor feature, coef in zip(X.columns, lr_model.coef_):\n    print(f\"\\n{feature}:\")\n    print(f\"  ‚Ä¢ Por cada $1000 adicionales invertidos en {feature}\")\n    print(f\"  ‚Ä¢ Las ventas aumentan en {coef*1000:.0f} unidades\")\n    print(f\"  ‚Ä¢ Coeficiente: {coef:.6f}\")\n\nprint(f\"\\nIntercepto: {lr_model.intercept_:.2f}\")\nprint(\"  ‚Ä¢ Ventas base esperadas sin inversi√≥n en publicidad (miles de unidades)\")\n\n# Ejemplo de predicci√≥n\nprint(\"\\n\" + \"=\"*70)\nprint(\"EJEMPLO DE PREDICCI√ìN\")\nprint(\"=\"*70)\nexample_budget = pd.DataFrame({\n    'TV': [150],\n    'Radio': [30],\n    'Newspaper': [10]\n})\n\nprediction_lr = lr_model.predict(example_budget)[0]\nprint(\"\\nPresupuesto de ejemplo:\")\nprint(f\"  ‚Ä¢ TV: ${example_budget['TV'][0]:,} mil\")\nprint(f\"  ‚Ä¢ Radio: ${example_budget['Radio'][0]:,} mil\")\nprint(f\"  ‚Ä¢ Newspaper: ${example_budget['Newspaper'][0]:,} mil\")\nprint(f\"\\nVentas predichas: {prediction_lr:.2f} mil unidades\")\nprint(f\"Equivalente a: {prediction_lr*1000:,.0f} unidades\")\n\n\n======================================================================\nINTERPRETACI√ìN PR√ÅCTICA DEL MODELO\n======================================================================\n\nInterpretaci√≥n de los coeficientes (Regresi√≥n Lineal):\n------------------------------------------------------------\n\nTV:\n  ‚Ä¢ Por cada $1000 adicionales invertidos en TV\n  ‚Ä¢ Las ventas aumentan en 45 unidades\n  ‚Ä¢ Coeficiente: 0.044730\n\nRadio:\n  ‚Ä¢ Por cada $1000 adicionales invertidos en Radio\n  ‚Ä¢ Las ventas aumentan en 189 unidades\n  ‚Ä¢ Coeficiente: 0.189195\n\nNewspaper:\n  ‚Ä¢ Por cada $1000 adicionales invertidos en Newspaper\n  ‚Ä¢ Las ventas aumentan en 3 unidades\n  ‚Ä¢ Coeficiente: 0.002761\n\nIntercepto: 2.98\n  ‚Ä¢ Ventas base esperadas sin inversi√≥n en publicidad (miles de unidades)\n\n======================================================================\nEJEMPLO DE PREDICCI√ìN\n======================================================================\n\nPresupuesto de ejemplo:\n  ‚Ä¢ TV: $150 mil\n  ‚Ä¢ Radio: $30 mil\n  ‚Ä¢ Newspaper: $10 mil\n\nVentas predichas: 15.39 mil unidades\nEquivalente a: 15,392 unidades",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>An√°lisis de Regresi√≥n Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html",
    "href": "ejercicio_wine_quality.html",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "",
    "text": "Descripci√≥n del Dataset\nEl dataset Wine Quality contiene resultados de an√°lisis fisicoqu√≠micos de vinos portugueses ‚ÄúVinho Verde‚Äù y su calidad evaluada por expertos. El objetivo es predecir la calidad del vino bas√°ndose en sus propiedades qu√≠micas.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#descripci√≥n-del-dataset",
    "href": "ejercicio_wine_quality.html#descripci√≥n-del-dataset",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "",
    "text": "Variables del dataset:\n\nfixed acidity: Acidez fija (g/L de √°cido tart√°rico)\nvolatile acidity: Acidez vol√°til (g/L de √°cido ac√©tico)\ncitric acid: √Åcido c√≠trico (g/L)\nresidual sugar: Az√∫car residual (g/L)\nchlorides: Cloruros (g/L de cloruro de sodio)\nfree sulfur dioxide: Di√≥xido de azufre libre (mg/L)\ntotal sulfur dioxide: Di√≥xido de azufre total (mg/L)\ndensity: Densidad (g/cm¬≥)\npH: pH del vino\nsulphates: Sulfatos (g/L de sulfato de potasio)\nalcohol: Contenido de alcohol (% vol)\nquality: Calidad del vino (puntuaci√≥n de 0-10) - Variable objetivo\n\nEn este ejercicio, trabajar√°s con el dataset de vinos tintos y aplicar√°s diferentes t√©cnicas de regresi√≥n para predecir la calidad del vino.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#importar-librer√≠as-y-cargar-datos",
    "href": "ejercicio_wine_quality.html#importar-librer√≠as-y-cargar-datos",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "1. Importar librer√≠as y cargar datos",
    "text": "1. Importar librer√≠as y cargar datos\n\n# Importar las librer√≠as necesarias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurar estilo de gr√°ficos\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Configurar seed para reproducibilidad\nnp.random.seed(42)\n\n\n# Cargar el dataset de vinos tintos\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\nwine_data = pd.read_csv(url, sep=';')\n\nprint(\"Dataset cargado exitosamente!\")\nprint(f\"Dimensiones del dataset: {wine_data.shape}\")\nprint(f\"\\nColumnas del dataset:\")\nprint(wine_data.columns.tolist())",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#an√°lisis-exploratorio-de-datos-eda",
    "href": "ejercicio_wine_quality.html#an√°lisis-exploratorio-de-datos-eda",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "2. An√°lisis Exploratorio de Datos (EDA)",
    "text": "2. An√°lisis Exploratorio de Datos (EDA)\n\nEjercicio 2.1: Exploraci√≥n inicial\nCompleta el an√°lisis exploratorio inicial del dataset.\n\n# Mostrar las primeras filas del dataset\nwine_data.head()\n\n\n# TODO: Muestra la informaci√≥n general del dataset (tipos de datos, valores no nulos)\n# Tu c√≥digo aqu√≠\n\n\n# TODO: Calcula y muestra las estad√≠sticas descriptivas del dataset\n# Tu c√≥digo aqu√≠\n\n\n# TODO: Verifica si hay valores nulos en el dataset\n# Tu c√≥digo aqu√≠\n\n\n\nEjercicio 2.2: An√°lisis de la variable objetivo\n\n# Analizar la distribuci√≥n de la calidad del vino\nplt.figure(figsize=(10, 6))\nwine_data['quality'].value_counts().sort_index().plot(kind='bar', color='steelblue', edgecolor='black')\nplt.xlabel('Calidad del Vino', fontsize=12)\nplt.ylabel('Frecuencia', fontsize=12)\nplt.title('Distribuci√≥n de la Calidad del Vino', fontsize=14)\nplt.xticks(rotation=0)\nplt.grid(axis='y', alpha=0.3)\n\n# Agregar estad√≠sticas\nmean_quality = wine_data['quality'].mean()\nmedian_quality = wine_data['quality'].median()\nplt.axhline(y=wine_data['quality'].value_counts().mean(), color='red', \n            linestyle='--', label=f'Media de frecuencia')\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(f\"Estad√≠sticas de la calidad del vino:\")\nprint(f\"Media: {mean_quality:.2f}\")\nprint(f\"Mediana: {median_quality:.2f}\")\nprint(f\"Desviaci√≥n est√°ndar: {wine_data['quality'].std():.2f}\")\n\n\n\nEjercicio 2.3: Matriz de correlaci√≥n\n\n# TODO: Calcula la matriz de correlaci√≥n y visual√≠zala con un heatmap\n# Pista: Usa sns.heatmap() con annot=True para mostrar los valores\n# Tu c√≥digo aqu√≠\n\nplt.figure(figsize=(14, 10))\n# Completa el c√≥digo para crear el heatmap\n\n\n# TODO: Identifica y muestra las 5 variables m√°s correlacionadas con 'quality'\n# Tu c√≥digo aqu√≠\n\n\n\nEjercicio 2.4: Visualizaci√≥n de relaciones\n\n# Visualizar las 4 variables m√°s correlacionadas con quality\ntop_features = ['alcohol', 'volatile acidity', 'citric acid', 'sulphates']\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Relaci√≥n entre Variables Principales y Calidad del Vino', fontsize=16)\n\nfor idx, (ax, feature) in enumerate(zip(axes.flat, top_features)):\n    # TODO: Crea un scatter plot para cada variable vs quality\n    # Agrega una l√≠nea de tendencia\n    # Tu c√≥digo aqu√≠\n    pass\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#preparaci√≥n-de-datos",
    "href": "ejercicio_wine_quality.html#preparaci√≥n-de-datos",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "3. Preparaci√≥n de Datos",
    "text": "3. Preparaci√≥n de Datos\n\n# Separar caracter√≠sticas (X) y variable objetivo (y)\nX = wine_data.drop('quality', axis=1)\ny = wine_data['quality']\n\nprint(f\"Forma de X: {X.shape}\")\nprint(f\"Forma de y: {y.shape}\")\nprint(f\"\\nCaracter√≠sticas: {X.columns.tolist()}\")\n\n\n# TODO: Divide los datos en conjuntos de entrenamiento y prueba\n# Usa test_size=0.2 y random_state=42\n# Tu c√≥digo aqu√≠\n\n# X_train, X_test, y_train, y_test = ...\n\n# print(f\"Tama√±o del conjunto de entrenamiento: ...\")\n# print(f\"Tama√±o del conjunto de prueba: ...\")\n\n\n# TODO: Estandariza las caracter√≠sticas\n# Recuerda: ajusta el scaler solo con los datos de entrenamiento\n# Tu c√≥digo aqu√≠\n\n# scaler = StandardScaler()\n# X_train_scaled = ...\n# X_test_scaled = ...",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#validaci√≥n-cruzada-para-selecci√≥n-de-hiperpar√°metros",
    "href": "ejercicio_wine_quality.html#validaci√≥n-cruzada-para-selecci√≥n-de-hiperpar√°metros",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "4. Validaci√≥n Cruzada para Selecci√≥n de Hiperpar√°metros",
    "text": "4. Validaci√≥n Cruzada para Selecci√≥n de Hiperpar√°metros\nLa validaci√≥n cruzada es fundamental para seleccionar los mejores hiperpar√°metros sin usar el conjunto de prueba.\n\nEjercicio 4.1: Implementaci√≥n manual de validaci√≥n cruzada\n\n# Ejemplo: Validaci√≥n cruzada manual para Ridge\ndef manual_cross_validation(X, y, alpha, n_folds=5):\n    \"\"\"\n    Implementa validaci√≥n cruzada manualmente para Ridge regression\n    \"\"\"\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    scores = []\n    \n    for train_idx, val_idx in kf.split(X):\n        # Dividir datos\n        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # Estandarizar\n        scaler_cv = StandardScaler()\n        X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n        X_val_cv_scaled = scaler_cv.transform(X_val_cv)\n        \n        # Entrenar modelo\n        model = Ridge(alpha=alpha)\n        model.fit(X_train_cv_scaled, y_train_cv)\n        \n        # Evaluar\n        y_pred = model.predict(X_val_cv_scaled)\n        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n        scores.append(rmse)\n    \n    return np.mean(scores), np.std(scores)\n\n# Probar diferentes valores de alpha\nalphas_to_test = [0.001, 0.01, 0.1, 1, 10, 100]\ncv_results_manual = []\n\nprint(\"Validaci√≥n Cruzada Manual para Ridge Regression:\")\nprint(\"-\" * 50)\n\nfor alpha in alphas_to_test:\n    mean_rmse, std_rmse = manual_cross_validation(X_train.values, y_train, alpha)\n    cv_results_manual.append({'alpha': alpha, 'mean_rmse': mean_rmse, 'std_rmse': std_rmse})\n    print(f\"Alpha: {alpha:7.3f} | RMSE: {mean_rmse:.4f} (+/- {std_rmse:.4f})\")\n\n# TODO: Identifica el mejor alpha basado en el RMSE medio m√°s bajo\n# Tu c√≥digo aqu√≠\n\n\n\nEjercicio 4.2: Usar RidgeCV para validaci√≥n cruzada autom√°tica\n\n# TODO: Usa RidgeCV para encontrar autom√°ticamente el mejor alpha\n# Pista: RidgeCV tiene un par√°metro 'alphas' y 'cv'\n# Tu c√≥digo aqu√≠\n\n# alphas = np.logspace(-3, 3, 100)  # 100 valores entre 0.001 y 1000\n# ridge_cv = RidgeCV(...)\n# ridge_cv.fit(...)\n\n# print(f\"Mejor alpha encontrado por RidgeCV: ...\")\n\n\n\nEjercicio 4.3: GridSearchCV para b√∫squeda exhaustiva\n\n# Ejemplo completo con GridSearchCV para Ridge\nfrom sklearn.model_selection import GridSearchCV\n\n# Definir par√°metros a buscar\nparam_grid_ridge = {\n    'alpha': np.logspace(-3, 3, 20)  # 20 valores entre 0.001 y 1000\n}\n\n# Crear modelo base\nridge_base = Ridge(random_state=42)\n\n# TODO: Implementa GridSearchCV\n# Usa cv=5, scoring='neg_mean_squared_error'\n# Tu c√≥digo aqu√≠\n\n# grid_search_ridge = GridSearchCV(...)\n# grid_search_ridge.fit(...)\n\n# print(f\"Mejor alpha: ...\")\n# print(f\"Mejor score (RMSE): ...\")\n\n\n# TODO: Visualiza los resultados de la validaci√≥n cruzada\n# Crea un gr√°fico que muestre c√≥mo cambia el RMSE con diferentes valores de alpha\n# Tu c√≥digo aqu√≠\n\nplt.figure(figsize=(10, 6))\n# Completa el c√≥digo para visualizar los resultados",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#modelos-de-regresi√≥n",
    "href": "ejercicio_wine_quality.html#modelos-de-regresi√≥n",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "5. Modelos de Regresi√≥n",
    "text": "5. Modelos de Regresi√≥n\n\n5.1 Regresi√≥n Lineal Normal\n\n# TODO: Implementa y entrena un modelo de regresi√≥n lineal\n# Tu c√≥digo aqu√≠\n\n# lr_model = LinearRegression()\n# lr_model.fit(...)\n\n# Hacer predicciones\n# y_pred_lr_train = ...\n# y_pred_lr_test = ...\n\n# Calcular m√©tricas\n# rmse_lr_train = ...\n# rmse_lr_test = ...\n# r2_lr_train = ...\n# r2_lr_test = ...\n\n# print(\"Regresi√≥n Lineal Normal:\")\n# print(f\"RMSE Train: ...\")\n# print(f\"RMSE Test: ...\")\n# print(f\"R¬≤ Train: ...\")\n# print(f\"R¬≤ Test: ...\")\n\n\n\n5.2 Ridge Regression con mejor alpha de CV\n\n# TODO: Entrena Ridge con el mejor alpha encontrado por validaci√≥n cruzada\n# Tu c√≥digo aqu√≠\n\n# best_alpha_ridge = ...  # Usa el mejor alpha de la secci√≥n anterior\n# ridge_model = Ridge(alpha=best_alpha_ridge)\n# ridge_model.fit(...)\n\n# Predicciones y m√©tricas\n# ...\n\n\n\n5.3 Lasso Regression con validaci√≥n cruzada\n\n# TODO: Implementa LassoCV para encontrar el mejor alpha autom√°ticamente\n# Tu c√≥digo aqu√≠\n\n# alphas_lasso = np.logspace(-3, 1, 100)\n# lasso_cv = LassoCV(...)\n# lasso_cv.fit(...)\n\n# print(f\"Mejor alpha para Lasso: ...\")\n# print(f\"N√∫mero de caracter√≠sticas seleccionadas: ...\")\n\n# Predicciones y m√©tricas\n# ...\n\n\n# TODO: Identifica qu√© caracter√≠sticas fueron eliminadas por Lasso\n# Tu c√≥digo aqu√≠",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#comparaci√≥n-de-modelos",
    "href": "ejercicio_wine_quality.html#comparaci√≥n-de-modelos",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "6. Comparaci√≥n de Modelos",
    "text": "6. Comparaci√≥n de Modelos\n\n# TODO: Crea una tabla comparativa con todos los modelos\n# Incluye: RMSE Train, RMSE Test, R¬≤ Train, R¬≤ Test, MAE Test\n# Tu c√≥digo aqu√≠\n\n# comparison_data = {\n#     'Modelo': [...],\n#     'RMSE Train': [...],\n#     'RMSE Test': [...],\n#     'R¬≤ Train': [...],\n#     'R¬≤ Test': [...],\n#     'MAE Test': [...]\n# }\n\n# comparison_df = pd.DataFrame(comparison_data)\n# print(comparison_df)\n\n\n# TODO: Crea visualizaciones para comparar los modelos\n# 1. Gr√°fico de barras comparando RMSE\n# 2. Gr√°fico de barras comparando R¬≤\n# Tu c√≥digo aqu√≠",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#an√°lisis-de-residuos",
    "href": "ejercicio_wine_quality.html#an√°lisis-de-residuos",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "7. An√°lisis de Residuos",
    "text": "7. An√°lisis de Residuos\n\n# TODO: Para el mejor modelo, crea:\n# 1. Gr√°fico de residuos vs predicciones\n# 2. Histograma de residuos\n# 3. Q-Q plot de residuos\n# Tu c√≥digo aqu√≠",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#importancia-de-caracter√≠sticas",
    "href": "ejercicio_wine_quality.html#importancia-de-caracter√≠sticas",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "8. Importancia de Caracter√≠sticas",
    "text": "8. Importancia de Caracter√≠sticas\n\n# TODO: Visualiza los coeficientes de los tres modelos en un mismo gr√°fico\n# Esto te ayudar√° a entender qu√© caracter√≠sticas son m√°s importantes\n# Tu c√≥digo aqu√≠",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#validaci√≥n-cruzada-final-del-mejor-modelo",
    "href": "ejercicio_wine_quality.html#validaci√≥n-cruzada-final-del-mejor-modelo",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "9. Validaci√≥n Cruzada Final del Mejor Modelo",
    "text": "9. Validaci√≥n Cruzada Final del Mejor Modelo\n\n# TODO: Realiza validaci√≥n cruzada con 10 folds del mejor modelo\n# Reporta la media y desviaci√≥n est√°ndar del RMSE\n# Tu c√≥digo aqu√≠",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#conclusiones-y-preguntas-de-reflexi√≥n",
    "href": "ejercicio_wine_quality.html#conclusiones-y-preguntas-de-reflexi√≥n",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "10. Conclusiones y Preguntas de Reflexi√≥n",
    "text": "10. Conclusiones y Preguntas de Reflexi√≥n\n\nPreguntas para responder:\n\n¬øCu√°l modelo tuvo el mejor desempe√±o? ¬øPor qu√© crees que fue as√≠?\n\nTu respuesta:\n\n¬øQu√© caracter√≠sticas son las m√°s importantes para predecir la calidad del vino?\n\nTu respuesta:\n\n¬øObservas se√±ales de sobreajuste en alg√∫n modelo? ¬øC√≥mo lo identificaste?\n\nTu respuesta:\n\n¬øC√≥mo cambi√≥ el rendimiento de Ridge y Lasso con diferentes valores de alpha?\n\nTu respuesta:\n\n¬øQu√© ventajas observaste al usar validaci√≥n cruzada para seleccionar hiperpar√°metros?\n\nTu respuesta:\n\nSi Lasso elimin√≥ algunas caracter√≠sticas, ¬øcrees que esto mejor√≥ o empeor√≥ el modelo? ¬øPor qu√©?\n\nTu respuesta:\n\n¬øQu√© otros pasos podr√≠as tomar para mejorar el rendimiento del modelo?\n\nTu respuesta:",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#ejercicio-extra-ingenier√≠a-de-caracter√≠sticas",
    "href": "ejercicio_wine_quality.html#ejercicio-extra-ingenier√≠a-de-caracter√≠sticas",
    "title": "Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality",
    "section": "Ejercicio Extra: Ingenier√≠a de Caracter√≠sticas",
    "text": "Ejercicio Extra: Ingenier√≠a de Caracter√≠sticas\n\nDesaf√≠o:\nIntenta mejorar el rendimiento del modelo creando nuevas caracter√≠sticas:\n\nCrea interacciones entre variables (ej: alcohol √ó pH)\nCrea caracter√≠sticas polinomiales\nAgrupa la calidad en categor√≠as (baja: 3-4, media: 5-6, alta: 7-8) y √∫sala como caracter√≠stica\nCrea ratios entre caracter√≠sticas relacionadas\n\n\n# TODO: Implementa ingenier√≠a de caracter√≠sticas y eval√∫a si mejora el modelo\n# Tu c√≥digo aqu√≠",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Ejercicio: An√°lisis de Regresi√≥n con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html",
    "href": "04-clasificacion.html",
    "title": "Clasificaci√≥n",
    "section": "",
    "text": "Introducci√≥n al Problema de Clasificaci√≥n\nEn los cap√≠tulos anteriores hemos trabajado con problemas de regresi√≥n, donde la variable respuesta \\(Y\\) es cuantitativa (continua). En este cap√≠tulo estudiaremos los problemas de clasificaci√≥n, donde la variakbble respuesta \\(Y\\) es cualitativa (categ√≥rica o discreta).",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html#introducci√≥n-al-problema-de-clasificaci√≥n",
    "href": "04-clasificacion.html#introducci√≥n-al-problema-de-clasificaci√≥n",
    "title": "Clasificaci√≥n",
    "section": "",
    "text": "Definici√≥n Formal\nUn problema de clasificaci√≥n consiste en asignar una observaci√≥n \\(\\mathbf{x} = (x_1, x_2, ..., x_p)\\) a una de \\(K\\) clases o categor√≠as posibles. Formalmente:\n\nEntrada: Un vector de caracter√≠sticas \\(\\mathbf{x} \\in \\mathbb{R}^p\\)\nSalida: Una etiqueta de clase \\(y \\in \\mathcal{C} = \\{C_1, C_2, ..., C_K\\}\\)\n\nDonde \\(\\mathcal{C}\\) es el conjunto finito de clases posibles.\n\n\nEjemplos de Problemas de Clasificaci√≥n\n\nClasificaci√≥n binaria (\\(K=2\\)):\n\nDetecci√≥n de spam en correos electr√≥nicos (spam/no spam)\nDiagn√≥stico m√©dico (enfermo/sano)\nAprobaci√≥n de cr√©dito (aprobado/rechazado)\n\nClasificaci√≥n multiclase (\\(K&gt;2\\)):\n\nReconocimiento de d√≠gitos escritos a mano (0-9)\nClasificaci√≥n de tipos de flores (setosa/versicolor/virginica)\nCategorizaci√≥n de noticias (deportes/pol√≠tica/tecnolog√≠a/etc.)\n\n\n\n\nObjetivo del Aprendizaje\nEl objetivo es aprender una funci√≥n de clasificaci√≥n \\(f: \\mathbb{R}^p \\rightarrow \\mathcal{C}\\) que minimice el error de clasificaci√≥n esperado:\n\\[\\mathbb{E}[L(Y, f(\\mathbf{X}))]\\]\nDonde \\(L\\) es una funci√≥n de p√©rdida. La funci√≥n de p√©rdida m√°s com√∫n es la p√©rdida 0-1:\n\\[L_{0-1}(y, \\hat{y}) = \\begin{cases}\n0 & \\text{si } y = \\hat{y} \\\\\n1 & \\text{si } y \\neq \\hat{y}\n\\end{cases}\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html#funciones-de-p√©rdida-en-clasificaci√≥n",
    "href": "04-clasificacion.html#funciones-de-p√©rdida-en-clasificaci√≥n",
    "title": "Clasificaci√≥n",
    "section": "Funciones de P√©rdida en Clasificaci√≥n",
    "text": "Funciones de P√©rdida en Clasificaci√≥n\nAunque la p√©rdida 0-1 es intuitiva y directamente relacionada con la tasa de error, presenta limitaciones importantes: no es diferenciable y no proporciona informaci√≥n sobre la confianza de las predicciones. Por esto, en la pr√°ctica se utilizan funciones de p√©rdida alternativas que trabajan con probabilidades.\n\nClasificaci√≥n Binaria: P√©rdidas Probabil√≠sticas\nPara clasificaci√≥n binaria, donde \\(y \\in \\{0, 1\\}\\), consideramos predicciones probabil√≠sticas \\(\\hat{p} = P(\\hat{Y} = 1 | \\mathbf{x})\\). Las funciones de p√©rdida m√°s importantes son:\n\nP√©rdida de Brier (Brier Score)\nLa p√©rdida de Brier o p√©rdida cuadr√°tica mide el error cuadr√°tico medio entre las probabilidades predichas y los valores reales:\n\\[L_{\\text{Brier}}(y, \\hat{p}) = (y - \\hat{p})^2\\]\nPara un conjunto de \\(n\\) observaciones:\n\\[\\text{Brier Score} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{p}_i)^2\\]\nPropiedades:\n\nRango: \\([0, 1]\\) (menor es mejor)\nEs una regla de puntuaci√≥n propia (proper scoring rule)\nPenaliza fuertemente predicciones confiadas pero incorrectas\nSe puede descomponer en: calibraci√≥n + refinamiento\n\n\n\nP√©rdida Logar√≠tmica (Log Loss o Entrop√≠a Cruzada Binaria)\nLa p√©rdida logar√≠tmica mide la distancia entre la distribuci√≥n verdadera y la predicha usando la divergencia de Kullback-Leibler:\n\\[L_{\\text{log}}(y, \\hat{p}) = -[y \\log(\\hat{p}) + (1-y) \\log(1-\\hat{p})]\\]\nEquivalentemente: \\[L_{\\text{log}}(y, \\hat{p}) = \\begin{cases}\n-\\log(\\hat{p}) & \\text{si } y = 1 \\\\\n-\\log(1-\\hat{p}) & \\text{si } y = 0\n\\end{cases}\\]\nPara un conjunto de observaciones:\n\\[\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i)]\\]\nPropiedades:\n\nRango: \\([0, \\infty)\\) (menor es mejor)\nTambi√©n es una regla de puntuaci√≥n propia\nPenaliza infinitamente predicciones completamente incorrectas (\\(\\hat{p} = 0\\) cuando \\(y = 1\\))\nEs la funci√≥n objetivo en regresi√≥n log√≠stica\n\n\n\n\nComparaci√≥n de Funciones de P√©rdida\n\n\n\n\n\nFunciones de perdidas para clasificaci√≥n\n\n\n\n\n\n\nReglas de Puntuaci√≥n Propias\nUna regla de puntuaci√≥n propia (proper scoring rule) es una funci√≥n de p√©rdida que incentiva al modelo a reportar sus verdaderas probabilidades. Formalmente, una funci√≥n \\(S(p, y)\\) es propia si:\n\\[\\mathbb{E}_{Y \\sim p^*}[S(p^*, Y)] \\leq \\mathbb{E}_{Y \\sim p^*}[S(p, Y)]\\]\nDonde \\(p^*\\) es la distribuci√≥n verdadera. Tanto la p√©rdida de Brier como la log loss son propias, mientras que la p√©rdida 0-1 no lo es.\n\n\nVentajas y Desventajas\nP√©rdida de Brier:\n\n‚úì Interpretaci√≥n directa como MSE de probabilidades\n‚úì Acotada en \\([0,1]\\)\n‚úì Menos sensible a predicciones extremas incorrectas\n‚úó Menos utilizada en optimizaci√≥n de modelos\n\nP√©rdida Logar√≠tmica:\n\n‚úì Base te√≥rica s√≥lida (teor√≠a de informaci√≥n)\n‚úì Funci√≥n objetivo natural para muchos modelos (log√≠stica, redes neuronales)\n‚úì Diferenciable y convexa\n‚úó No acotada superiormente\n‚úó Muy sensible a predicciones extremas incorrectas",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html#modelos-para-clasificaci√≥n-binaria",
    "href": "04-clasificacion.html#modelos-para-clasificaci√≥n-binaria",
    "title": "Clasificaci√≥n",
    "section": "Modelos para Clasificaci√≥n Binaria",
    "text": "Modelos para Clasificaci√≥n Binaria\n\nClasificador de Bayes para el Caso Binario\nEl clasificador de Bayes es el clasificador √≥ptimo te√≥rico que minimiza el error de clasificaci√≥n. Para el caso binario con clases \\(\\{0, 1\\}\\), clasifica seg√∫n:\n\\[\\hat{y}(\\mathbf{x}) = \\begin{cases}\n1 & \\text{si } P(Y = 1 | \\mathbf{X} = \\mathbf{x}) &gt; 0.5 \\\\\n0 & \\text{si } P(Y = 1 | \\mathbf{X} = \\mathbf{x}) \\leq 0.5\n\\end{cases}\\]\nO m√°s generalmente, con un umbral \\(\\tau\\):\n\\[\\hat{y}(\\mathbf{x}) = \\mathbb{1}[P(Y = 1 | \\mathbf{X} = \\mathbf{x}) &gt; \\tau]\\]\n\nEstimaci√≥n mediante el Teorema de Bayes\nUsando el teorema de Bayes:\n\\[P(Y = k | \\mathbf{X} = \\mathbf{x}) = \\frac{P(\\mathbf{X} = \\mathbf{x} | Y = k) \\cdot P(Y = k)}{P(\\mathbf{X} = \\mathbf{x})}\\]\nDonde:\n\n\\(P(Y = k)\\) es la probabilidad a priori de la clase \\(k\\)\n\\(P(\\mathbf{X} = \\mathbf{x} | Y = k)\\) es la verosimilitud de observar \\(\\mathbf{x}\\) dado que pertenece a la clase \\(k\\)\n\\(P(\\mathbf{X} = \\mathbf{x})\\) es la evidencia (constante de normalizaci√≥n)\n\nComo \\(P(\\mathbf{X} = \\mathbf{x})\\) es igual para todas las clases, la decisi√≥n se basa en:\n\\[\\hat{y} = \\arg\\max_k P(\\mathbf{X} = \\mathbf{x} | Y = k) \\cdot P(Y = k)\\]\n\n\nNaive Bayes: Simplificando el Problema\nEl problema principal del clasificador de Bayes es estimar \\(P(\\mathbf{X} = \\mathbf{x} | Y = k)\\) en alta dimensi√≥n. Con \\(p\\) caracter√≠sticas, necesitamos estimar la distribuci√≥n conjunta de todas las variables, lo cual es computacionalmente intratable cuando \\(p\\) es grande.\nEl clasificador Naive Bayes resuelve este problema mediante una asunci√≥n de independencia condicional: asume que las caracter√≠sticas son condicionalmente independientes dada la clase:\n\\[P(\\mathbf{X} = \\mathbf{x} | Y = k) = P(x_1, x_2, ..., x_p | Y = k) = \\prod_{j=1}^{p} P(x_j | Y = k)\\]\nEsta asunci√≥n, aunque ‚Äúingenua‚Äù (naive), simplifica enormemente el c√°lculo y funciona sorprendentemente bien en la pr√°ctica.\n\n\nTipos de Naive Bayes\nDependiendo del tipo de caracter√≠sticas, existen diferentes variantes:\n\n1. Gaussian Naive Bayes (caracter√≠sticas continuas)\nAsume que las caracter√≠sticas siguen una distribuci√≥n normal dentro de cada clase:\n\\[P(x_j | Y = k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{jk}^2}} \\exp\\left(-\\frac{(x_j - \\mu_{jk})^2}{2\\sigma_{jk}^2}\\right)\\]\nDonde \\(\\mu_{jk}\\) y \\(\\sigma_{jk}^2\\) son la media y varianza de la caracter√≠stica \\(j\\) en la clase \\(k\\).\n\n\n2. Multinomial Naive Bayes (caracter√≠sticas discretas/conteos)\nUtilizado para datos de conteo (ej. frecuencia de palabras en clasificaci√≥n de texto):\n\\[P(\\mathbf{x} | Y = k) = \\frac{N_k!}{\\prod_j x_j!} \\prod_{j=1}^{p} \\theta_{jk}^{x_j}\\]\nDonde \\(\\theta_{jk}\\) es la probabilidad de la caracter√≠stica \\(j\\) en la clase \\(k\\).\n\n\n3. Bernoulli Naive Bayes (caracter√≠sticas binarias)\nPara caracter√≠sticas binarias (presencia/ausencia):\n\\[P(\\mathbf{x} | Y = k) = \\prod_{j=1}^{p} \\theta_{jk}^{x_j} (1-\\theta_{jk})^{1-x_j}\\]\n\n\n\nVentajas y Desventajas de Naive Bayes\nVentajas:\n\n‚úì R√°pido de entrenar y predecir\n‚úì Funciona bien con pocos datos de entrenamiento\n‚úì Maneja naturalmente m√∫ltiples clases\n‚úì Robusto ante caracter√≠sticas irrelevantes\n‚úì Proporciona estimaciones de probabilidad\n\nDesventajas:\n\n‚úó La asunci√≥n de independencia es frecuentemente violada\n‚úó Puede dar estimaciones de probabilidad sesgadas\n‚úó Sensible a la maldici√≥n de la dimensionalidad con Gaussian NB\n\n\n\nEjemplos en Python\n\n1. Ejemplo B√°sico: Gaussian Naive Bayes\nComenzamos con un ejemplo simple de clasificaci√≥n binaria usando Gaussian Naive Bayes:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pandas as pd\n\n# Generar datos sint√©ticos para clasificaci√≥n binaria\nnp.random.seed(42)\nX, y = make_classification(\n    n_samples=300,\n    n_features=2,        # 2 caracter√≠sticas para visualizaci√≥n f√°cil\n    n_informative=2,     # Ambas caracter√≠sticas son informativas\n    n_redundant=0,       # Sin caracter√≠sticas redundantes\n    n_clusters_per_class=2,  # 2 grupos por clase\n    flip_y=0.05,         # 5% de ruido en las etiquetas\n    class_sep=0.8,       # Separaci√≥n entre clases\n    random_state=42\n)\n\n# Dividir en conjunto de entrenamiento (70%) y prueba (30%)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nprint(\"Dimensiones de los datos:\")\nprint(f\"  Entrenamiento: {X_train.shape}\")\nprint(f\"  Prueba: {X_test.shape}\")\n\nDimensiones de los datos:\n  Entrenamiento: (210, 2)\n  Prueba: (90, 2)\n\n\n\n# Crear y entrenar el modelo Gaussian Naive Bayes\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\n# Realizar predicciones\ny_pred = gnb.predict(X_test)\ny_proba = gnb.predict_proba(X_test)\n\n# Evaluar el modelo\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Exactitud (Accuracy): {accuracy:.3f}\")\n\n# Matriz de confusi√≥n\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nMatriz de Confusi√≥n:\")\nprint(pd.DataFrame(cm,\n                   columns=['Predicho 0', 'Predicho 1'],\n                   index=['Real 0', 'Real 1']))\n\n# Par√°metros aprendidos por el modelo\nprint(\"\\n\" + \"=\" * 50)\nprint(\"PAR√ÅMETROS APRENDIDOS\")\nprint(\"=\" * 50)\nprint(f\"\\nProbabilidades a priori (prior):\")\nprint(f\"  P(Y=0) = {gnb.class_prior_[0]:.3f}\")\nprint(f\"  P(Y=1) = {gnb.class_prior_[1]:.3f}\")\n\nprint(f\"\\nMedias de cada caracter√≠stica por clase:\")\nfor i, clase in enumerate([0, 1]):\n    print(f\"  Clase {clase}: Œº‚ÇÅ={gnb.theta_[i, 0]:.3f}, Œº‚ÇÇ={gnb.theta_[i, 1]:.3f}\")\n\nprint(f\"\\nVarianzas de cada caracter√≠stica por clase:\")\nfor i, clase in enumerate([0, 1]):\n    print(f\"  Clase {clase}: œÉ¬≤‚ÇÅ={gnb.var_[i, 0]:.3f}, œÉ¬≤‚ÇÇ={gnb.var_[i, 1]:.3f}\")\n\nExactitud (Accuracy): 0.844\n\nMatriz de Confusi√≥n:\n        Predicho 0  Predicho 1\nReal 0          46           8\nReal 1           6          30\n\n==================================================\nPAR√ÅMETROS APRENDIDOS\n==================================================\n\nProbabilidades a priori (prior):\n  P(Y=0) = 0.481\n  P(Y=1) = 0.519\n\nMedias de cada caracter√≠stica por clase:\n  Clase 0: Œº‚ÇÅ=-0.072, Œº‚ÇÇ=-0.693\n  Clase 1: Œº‚ÇÅ=0.048, Œº‚ÇÇ=0.726\n\nVarianzas de cada caracter√≠stica por clase:\n  Clase 0: œÉ¬≤‚ÇÅ=0.872, œÉ¬≤‚ÇÇ=0.544\n  Clase 1: œÉ¬≤‚ÇÅ=1.275, œÉ¬≤‚ÇÇ=0.654\n\n\n\n\n2. Visualizaci√≥n de la Frontera de Decisi√≥n\n\n# Funci√≥n auxiliar para visualizar fronteras de decisi√≥n\ndef visualizar_clasificador(X, y, classifier, title):\n    \"\"\"\n    Visualiza la frontera de decisi√≥n de un clasificador\n\n    Par√°metros:\n    - X: caracter√≠sticas (n_samples, 2)\n    - y: etiquetas (n_samples,)\n    - classifier: modelo entrenado\n    - title: t√≠tulo del gr√°fico\n    \"\"\"\n    h = 0.02  # Tama√±o del paso en la malla\n\n    # Crear una malla de puntos para evaluar el clasificador\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Predecir probabilidades para cada punto de la malla\n    Z = classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n    Z = Z.reshape(xx.shape)\n\n    # Crear la visualizaci√≥n\n    plt.figure(figsize=(12, 5))\n\n    # Panel 1: Datos y frontera\n    plt.subplot(1, 2, 1)\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu_r', levels=20)\n    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolors='black',\n                s=50, label='Clase 0', alpha=0.7)\n    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolors='black',\n                s=50, label='Clase 1', alpha=0.7)\n    plt.xlabel('Caracter√≠stica 1')\n    plt.ylabel('Caracter√≠stica 2')\n    plt.title(f'{title} - Frontera de Decisi√≥n')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Panel 2: Mapa de probabilidades\n    plt.subplot(1, 2, 2)\n    contour = plt.contourf(xx, yy, Z, levels=20, cmap='RdBu_r', alpha=0.8)\n    plt.colorbar(contour, label='P(Y=1|X)')\n    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolors='black',\n                s=30, alpha=0.5)\n    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolors='black',\n                s=30, alpha=0.5)\n    plt.xlabel('Caracter√≠stica 1')\n    plt.ylabel('Caracter√≠stica 2')\n    plt.title(f'{title} - Probabilidades')\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n# Visualizar nuestro modelo entrenado\nvisualizar_clasificador(X_train, y_train, gnb, 'Gaussian Naive Bayes')\n\n\n\n\nFrontera de decisi√≥n de Gaussian Naive Bayes\n\n\n\n\n\n\n3. Comparaci√≥n de Variantes de Naive Bayes\nAhora comparemos las tres variantes principales de Naive Bayes:\n\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Preparar diferentes versiones de los datos para cada variante\n\n# 1. Gaussian NB: usa los datos originales\nX_gaussian = X_train.copy()\n\n# 2. Multinomial NB: necesita valores no negativos (frecuencias)\nscaler = MinMaxScaler()\nX_multinomial = scaler.fit_transform(X_train) + 0.1  # Asegurar valores positivos\n\n# 3. Bernoulli NB: necesita valores binarios\nX_bernoulli = (X_train &gt; np.median(X_train, axis=0)).astype(float)\n\nprint(\"Forma de los datos para cada variante:\")\nprint(f\"  Gaussian: {X_gaussian.shape} - Valores continuos\")\nprint(f\"  Multinomial: {X_multinomial.shape} - Valores positivos\")\nprint(f\"  Bernoulli: {X_bernoulli.shape} - Valores binarios\")\n\n# Mostrar ejemplos de los primeros 3 datos\nprint(\"\\nEjemplo de transformaci√≥n (primeras 3 muestras, primera caracter√≠stica):\")\nprint(f\"  Original: {X_gaussian[:3, 0]}\")\nprint(f\"  Multinomial: {X_multinomial[:3, 0]}\")\nprint(f\"  Bernoulli: {X_bernoulli[:3, 0]}\")\n\nForma de los datos para cada variante:\n  Gaussian: (210, 2) - Valores continuos\n  Multinomial: (210, 2) - Valores positivos\n  Bernoulli: (210, 2) - Valores binarios\n\nEjemplo de transformaci√≥n (primeras 3 muestras, primera caracter√≠stica):\n  Original: [-0.98221929  1.48740486  0.62625557]\n  Multinomial: [0.37211974 0.9009729  0.71656365]\n  Bernoulli: [0. 1. 1.]\n\n\n\n# Entrenar las tres variantes\nmodelos = {\n    'Gaussian NB': (GaussianNB(), X_gaussian),\n    'Multinomial NB': (MultinomialNB(), X_multinomial),\n    'Bernoulli NB': (BernoulliNB(), X_bernoulli)\n}\n\nresultados = {}\n\nfor nombre, (modelo, X_train_variant) in modelos.items():\n    # Entrenar\n    modelo.fit(X_train_variant, y_train)\n\n    # Preparar datos de prueba seg√∫n la variante\n    if nombre == 'Gaussian NB':\n        X_test_variant = X_test\n    elif nombre == 'Multinomial NB':\n        X_test_variant = scaler.transform(X_test) + 0.1\n    else:  # Bernoulli\n        X_test_variant = (X_test &gt; np.median(X_train, axis=0)).astype(float)\n\n    # Predecir\n    y_pred = modelo.predict(X_test_variant)\n\n    # Guardar resultados\n    resultados[nombre] = {\n        'modelo': modelo,\n        'accuracy': accuracy_score(y_test, y_pred),\n        'y_pred': y_pred\n    }\n\n    print(f\"\\n{nombre}:\")\n    print(f\"  Accuracy: {resultados[nombre]['accuracy']:.3f}\")\n\n\nGaussian NB:\n  Accuracy: 0.844\n\nMultinomial NB:\n  Accuracy: 0.544\n\nBernoulli NB:\n  Accuracy: 0.844\n\n\n\n# Visualizar comparaci√≥n de resultados\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Gr√°fico de barras de accuracy\nnombres = list(resultados.keys())\naccuracies = [resultados[n]['accuracy'] for n in nombres]\n\nbars = axes[0].bar(nombres, accuracies, color=['blue', 'green', 'red'], alpha=0.7)\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Comparaci√≥n de Exactitud')\naxes[0].set_ylim([0, 1])\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# A√±adir valores en las barras\nfor bar, acc in zip(bars, accuracies):\n    height = bar.get_height()\n    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n                 f'{acc:.3f}', ha='center', va='bottom')\n\n# Matrices de confusi√≥n\nfrom sklearn.metrics import confusion_matrix\n\naxes[1].axis('off')\nfor i, nombre in enumerate(nombres):\n    cm = confusion_matrix(y_test, resultados[nombre]['y_pred'])\n\n    # Crear subtabla\n    ax_sub = plt.subplot2grid((1, 6), (0, 4 + i*2//3), colspan=2//3 + 1)\n    ax_sub.imshow(cm, cmap='Blues', aspect='auto')\n    ax_sub.set_title(f'{nombre.split()[0]} NB', fontsize=9)\n\n    # A√±adir texto en cada celda\n    for (i, j), val in np.ndenumerate(cm):\n        ax_sub.text(j, i, str(val), ha='center', va='center')\n\n    if i == 0:\n        ax_sub.set_ylabel('Real', fontsize=8)\n    ax_sub.set_xlabel('Pred', fontsize=8)\n    ax_sub.set_xticks([0, 1])\n    ax_sub.set_yticks([0, 1])\n    ax_sub.tick_params(labelsize=8)\n\nplt.suptitle('Comparaci√≥n de Variantes de Naive Bayes', y=1.05)\nplt.tight_layout()\nplt.show()\n\n\n\n\nComparaci√≥n de variantes de Naive Bayes\n\n\n\n\n\n\n4. Ejemplo Pr√°ctico: Clasificaci√≥n de Texto\n\n# Simular un conjunto de datos de texto\n# Imaginemos que tenemos documentos con conteo de palabras\n\nprint(\"=\" * 60)\nprint(\"EJEMPLO: CLASIFICACI√ìN DE DOCUMENTOS\")\nprint(\"=\" * 60)\n\n# Crear datos simulados de texto\nnp.random.seed(42)\nn_docs = 100\nn_palabras = 10\n\n# Nombres de las \"palabras\" para mejor interpretaci√≥n\npalabras = ['tecnolog√≠a', 'computadora', 'software', 'datos', 'algoritmo',\n            'deporte', 'equipo', 'juego', 'campeonato', 'jugador']\n\n# Crear matriz de frecuencias\n# Clase 0: documentos sobre tecnolog√≠a (m√°s palabras 0-4)\n# Clase 1: documentos sobre deportes (m√°s palabras 5-9)\nX_text = np.random.poisson(1, (n_docs, n_palabras))\ny_text = np.array([0] * 50 + [1] * 50)  # 50 docs de cada clase\n\n# Sesgar frecuencias seg√∫n la clase\nX_text[:50, :5] *= 3   # Docs de tecnolog√≠a: m√°s palabras t√©cnicas\nX_text[50:, 5:] *= 3   # Docs de deportes: m√°s palabras deportivas\n\n# Crear DataFrame para mejor visualizaci√≥n\ndf_text = pd.DataFrame(X_text, columns=palabras)\ndf_text['clase'] = y_text\ndf_text['tipo_documento'] = df_text['clase'].map({0: 'Tecnolog√≠a', 1: 'Deportes'})\n\nprint(\"\\nPrimeros 5 documentos:\")\nprint(df_text.head())\n\nprint(\"\\nEstad√≠sticas por clase:\")\nprint(df_text.groupby('tipo_documento')[palabras].mean().round(2))\n\n============================================================\nEJEMPLO: CLASIFICACI√ìN DE DOCUMENTOS\n============================================================\n\nPrimeros 5 documentos:\n   tecnolog√≠a  computadora  software  datos  algoritmo  deporte  equipo  \\\n0           3            6         0      0          9        2       0   \n1           0            3         0      0          3        0       1   \n2           9            0         3      0          3        1       1   \n3           0            0         0      0          3        1       0   \n4           0            9         0      0          0        2       0   \n\n   juego  campeonato  jugador  clase tipo_documento  \n0      0           0        1      0     Tecnolog√≠a  \n1      0           1        0      0     Tecnolog√≠a  \n2      1           0        5      0     Tecnolog√≠a  \n3      1           1        2      0     Tecnolog√≠a  \n4      0           0        3      0     Tecnolog√≠a  \n\nEstad√≠sticas por clase:\n                tecnolog√≠a  computadora  software  datos  algoritmo  deporte  \\\ntipo_documento                                                                 \nDeportes              1.16         1.02      1.00   1.12       1.02     3.12   \nTecnolog√≠a            2.82         2.94      2.22   2.46       2.28     1.12   \n\n                equipo  juego  campeonato  jugador  \ntipo_documento                                      \nDeportes           3.0   2.76        3.12     3.36  \nTecnolog√≠a         0.9   0.68        0.92     1.20  \n\n\n\n# Dividir datos de texto\nX_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n    X_text, y_text, test_size=0.3, random_state=42, stratify=y_text\n)\n\n# Entrenar Multinomial Naive Bayes (ideal para datos de conteo)\nmnb_text = MultinomialNB(alpha=1.0)  # alpha: par√°metro de suavizado Laplace\nmnb_text.fit(X_text_train, y_text_train)\n\n# Predicciones\ny_pred_text = mnb_text.predict(X_text_test)\ny_proba_text = mnb_text.predict_proba(X_text_test)\n\n# Evaluaci√≥n\nprint(\"Resultados de Clasificaci√≥n de Texto:\")\nprint(f\"Accuracy: {accuracy_score(y_text_test, y_pred_text):.3f}\")\n\n# Matriz de confusi√≥n\ncm_text = confusion_matrix(y_text_test, y_pred_text)\nprint(\"\\nMatriz de Confusi√≥n:\")\nprint(pd.DataFrame(cm_text,\n                   columns=['Pred Tecnolog√≠a', 'Pred Deportes'],\n                   index=['Real Tecnolog√≠a', 'Real Deportes']))\n\n# Importancia de las palabras\nprint(\"\\nImportancia de palabras por clase (log-probabilidades):\")\nlog_probs = mnb_text.feature_log_prob_\nimportancia_df = pd.DataFrame(log_probs.T,\n                               columns=['Tecnolog√≠a', 'Deportes'],\n                               index=palabras)\nprint(importancia_df.round(3))\n\nResultados de Clasificaci√≥n de Texto:\nAccuracy: 0.833\n\nMatriz de Confusi√≥n:\n                 Pred Tecnolog√≠a  Pred Deportes\nReal Tecnolog√≠a               14              1\nReal Deportes                  4             11\n\nImportancia de palabras por clase (log-probabilidades):\n             Tecnolog√≠a  Deportes\ntecnolog√≠a       -1.924    -2.854\ncomputadora      -1.703    -3.112\nsoftware         -2.162    -3.028\ndatos            -1.960    -2.950\nalgoritmo        -1.997    -3.112\ndeporte          -2.582    -1.894\nequipo           -3.034    -1.947\njuego            -3.188    -2.064\ncampeonato       -3.108    -1.748\njugador          -2.653    -1.843\n\n\n\n# Visualizar importancia de palabras\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Diferencia en log-probabilidades (palabras m√°s discriminativas)\ndiff_log_prob = log_probs[0] - log_probs[1]  # Tecnolog√≠a - Deportes\nindices_sorted = np.argsort(diff_log_prob)\n\n# Panel 1: Palabras m√°s importantes para cada clase\ny_pos = np.arange(len(palabras))\naxes[0].barh(y_pos, diff_log_prob[indices_sorted],\n             color=['red' if x &lt; 0 else 'blue' for x in diff_log_prob[indices_sorted]],\n             alpha=0.7)\naxes[0].set_yticks(y_pos)\naxes[0].set_yticklabels([palabras[i] for i in indices_sorted])\naxes[0].set_xlabel('Diferencia en log-probabilidad\\n(‚Üê Deportes | Tecnolog√≠a ‚Üí)')\naxes[0].set_title('Palabras Discriminativas')\naxes[0].grid(True, alpha=0.3, axis='x')\naxes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\n# Panel 2: Matriz de probabilidades\nim = axes[1].imshow(np.exp(log_probs), cmap='YlOrRd', aspect='auto')\naxes[1].set_xticks(range(len(palabras)))\naxes[1].set_xticklabels(palabras, rotation=45, ha='right')\naxes[1].set_yticks([0, 1])\naxes[1].set_yticklabels(['Tecnolog√≠a', 'Deportes'])\naxes[1].set_title('Probabilidades de Palabras por Clase')\nplt.colorbar(im, ax=axes[1], label='Probabilidad')\n\n# A√±adir valores en la matriz\nfor i in range(2):\n    for j in range(len(palabras)):\n        text = axes[1].text(j, i, f'{np.exp(log_probs[i, j]):.2f}',\n                           ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nImportancia de palabras en clasificaci√≥n de texto\n\n\n\n\n\n\n5. Ejemplo con Dataset Real: Iris\n\nfrom sklearn.datasets import load_iris\n\n# Cargar dataset Iris\niris = load_iris()\nX_iris = iris.data[:, [0, 2]]  # Usar solo 2 caracter√≠sticas para visualizaci√≥n\ny_iris = iris.target\nnombres_clases = iris.target_names\nnombres_features = [iris.feature_names[0], iris.feature_names[2]]\n\nprint(\"Dataset Iris:\")\nprint(f\"  N√∫mero de muestras: {X_iris.shape[0]}\")\nprint(f\"  N√∫mero de caracter√≠sticas: {X_iris.shape[1]}\")\nprint(f\"  Clases: {nombres_clases}\")\nprint(f\"  Caracter√≠sticas usadas: {nombres_features}\")\n\n# Dividir datos\nX_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n)\n\n# Entrenar Gaussian Naive Bayes\ngnb_iris = GaussianNB()\ngnb_iris.fit(X_iris_train, y_iris_train)\n\n# Predicciones\ny_pred_iris = gnb_iris.predict(X_iris_test)\naccuracy_iris = accuracy_score(y_iris_test, y_pred_iris)\n\nprint(f\"\\nAccuracy en Iris: {accuracy_iris:.3f}\")\n\n# Visualizaci√≥n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Panel 1: Datos y fronteras de decisi√≥n\nh = .02\nx_min, x_max = X_iris[:, 0].min() - 1, X_iris[:, 0].max() + 1\ny_min, y_max = X_iris[:, 1].min() - 1, X_iris[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = gnb_iris.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\naxes[0].contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\nscatter = axes[0].scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris,\n                          cmap='viridis', edgecolors='black', s=50)\naxes[0].set_xlabel(nombres_features[0])\naxes[0].set_ylabel(nombres_features[1])\naxes[0].set_title('Gaussian NB - Dataset Iris (3 clases)')\naxes[0].grid(True, alpha=0.3)\n\n# A√±adir leyenda\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=plt.cm.viridis(i/2), label=nombres_clases[i])\n                   for i in range(3)]\naxes[0].legend(handles=legend_elements, loc='upper right')\n\n# Panel 2: Matriz de confusi√≥n\ncm_iris = confusion_matrix(y_iris_test, y_pred_iris)\nim = axes[1].imshow(cm_iris, cmap='Blues', aspect='auto')\naxes[1].set_xticks(range(3))\naxes[1].set_yticks(range(3))\naxes[1].set_xticklabels(nombres_clases)\naxes[1].set_yticklabels(nombres_clases)\naxes[1].set_xlabel('Predicci√≥n')\naxes[1].set_ylabel('Valor Real')\naxes[1].set_title(f'Matriz de Confusi√≥n (Accuracy: {accuracy_iris:.3f})')\n\n# A√±adir valores\nfor (i, j), val in np.ndenumerate(cm_iris):\n    axes[1].text(j, i, str(val), ha='center', va='center',\n                 color='white' if val &gt; cm_iris.max()/2 else 'black')\n\nplt.colorbar(im, ax=axes[1])\nplt.tight_layout()\nplt.show()\n\nDataset Iris:\n  N√∫mero de muestras: 150\n  N√∫mero de caracter√≠sticas: 2\n  Clases: ['setosa' 'versicolor' 'virginica']\n  Caracter√≠sticas usadas: ['sepal length (cm)', 'petal length (cm)']\n\nAccuracy en Iris: 0.911\n\n\n\n\n\nClasificaci√≥n multiclase con Naive Bayes en dataset Iris\n\n\n\n\n\n\n6. Implementaci√≥n Desde Cero de Gaussian Naive Bayes\nPara comprender mejor el funcionamiento interno del algoritmo, vamos a implementar Gaussian Naive Bayes paso a paso:\n\nclass GaussianNBDesdesCero:\n    \"\"\"\n    Implementaci√≥n educativa de Gaussian Naive Bayes\n\n    Esta clase implementa el algoritmo paso a paso para\n    fines pedag√≥gicos.\n    \"\"\"\n\n    def __init__(self):\n        self.clases = None\n        self.priors = {}        # P(Y=k) para cada clase k\n        self.medias = {}        # Œº para cada clase y caracter√≠stica\n        self.varianzas = {}     # œÉ¬≤ para cada clase y caracter√≠stica\n\n    def entrenar(self, X, y):\n        \"\"\"\n        Fase de entrenamiento: calcular estad√≠sticas\n\n        Par√°metros:\n        - X: matriz de caracter√≠sticas (n_muestras, n_caracter√≠sticas)\n        - y: vector de etiquetas (n_muestras,)\n        \"\"\"\n        self.clases = np.unique(y)\n        n_muestras = len(y)\n        n_caracteristicas = X.shape[1]\n\n        print(f\"Entrenando con {n_muestras} muestras y {n_caracteristicas} caracter√≠sticas\")\n        print(f\"Clases encontradas: {self.clases}\")\n\n        for clase in self.clases:\n            # Filtrar datos de esta clase\n            X_clase = X[y == clase]\n            n_clase = len(X_clase)\n\n            # Calcular probabilidad a priori P(Y=clase)\n            self.priors[clase] = n_clase / n_muestras\n\n            # Calcular media y varianza para cada caracter√≠stica\n            self.medias[clase] = np.mean(X_clase, axis=0)\n            self.varianzas[clase] = np.var(X_clase, axis=0) + 1e-9  # Evitar divisi√≥n por cero\n\n            print(f\"\\nClase {clase}: {n_clase} muestras ({self.priors[clase]:.1%})\")\n            print(f\"  Medias: {self.medias[clase]}\")\n            print(f\"  Varianzas: {self.varianzas[clase]}\")\n\n        return self\n\n    def _calcular_gaussiana(self, x, media, varianza):\n        \"\"\"\n        Calcula P(x|Œº,œÉ¬≤) usando la distribuci√≥n gaussiana\n\n        F√≥rmula: P(x|Œº,œÉ¬≤) = 1/‚àö(2œÄœÉ¬≤) * exp(-(x-Œº)¬≤/(2œÉ¬≤))\n        \"\"\"\n        coeficiente = 1.0 / np.sqrt(2.0 * np.pi * varianza)\n        exponente = -((x - media) ** 2) / (2.0 * varianza)\n        return coeficiente * np.exp(exponente)\n\n    def predecir_probabilidades(self, X):\n        \"\"\"\n        Calcula P(Y=k|X) para cada clase k\n\n        Usa el teorema de Bayes:\n        P(Y=k|X) ‚àù P(X|Y=k) * P(Y=k)\n        \"\"\"\n        n_muestras = X.shape[0]\n        n_clases = len(self.clases)\n        probabilidades = np.zeros((n_muestras, n_clases))\n\n        for i, x in enumerate(X):\n            for j, clase in enumerate(self.clases):\n                # Calcular P(Y=clase) - prior\n                prob_prior = self.priors[clase]\n\n                # Calcular P(X|Y=clase) - verosimilitud\n                # Producto de probabilidades (asumiendo independencia)\n                verosimilitud = 1.0\n                for k in range(len(x)):\n                    prob_caracteristica = self._calcular_gaussiana(\n                        x[k],\n                        self.medias[clase][k],\n                        self.varianzas[clase][k]\n                    )\n                    verosimilitud *= prob_caracteristica\n\n                # P(Y=clase|X) ‚àù P(X|Y=clase) * P(Y=clase)\n                probabilidades[i, j] = verosimilitud * prob_prior\n\n            # Normalizar para que sumen 1\n            probabilidades[i] = probabilidades[i] / np.sum(probabilidades[i])\n\n        return probabilidades\n\n    def predecir(self, X):\n        \"\"\"\n        Predice la clase con mayor probabilidad posterior\n        \"\"\"\n        probabilidades = self.predecir_probabilidades(X)\n        indices_maximos = np.argmax(probabilidades, axis=1)\n        return self.clases[indices_maximos]\n\n\n# Crear y entrenar nuestro modelo\nprint(\"=\" * 60)\nprint(\"IMPLEMENTACI√ìN DESDE CERO\")\nprint(\"=\" * 60)\n\n# Usar un conjunto peque√±o para demostraci√≥n\nX_demo = X_train[:20]\ny_demo = y_train[:20]\nX_test_demo = X_test[:10]\ny_test_demo = y_test[:10]\n\n# Entrenar nuestro modelo\nmodelo_propio = GaussianNBDesdesCero()\nmodelo_propio.entrenar(X_demo, y_demo)\n\n# Hacer predicciones\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PREDICCIONES\")\nprint(\"=\" * 60)\n\ny_pred_propio = modelo_propio.predecir(X_test_demo)\nprobabilidades = modelo_propio.predecir_probabilidades(X_test_demo)\n\n# Mostrar resultados detallados\nfor i in range(5):  # Mostrar solo las primeras 5\n    print(f\"\\nMuestra {i+1}:\")\n    print(f\"  Caracter√≠sticas: [{X_test_demo[i, 0]:.2f}, {X_test_demo[i, 1]:.2f}]\")\n    print(f\"  Probabilidades: P(Y=0|X)={probabilidades[i, 0]:.3f}, P(Y=1|X)={probabilidades[i, 1]:.3f}\")\n    print(f\"  Predicci√≥n: {y_pred_propio[i]}\")\n    print(f\"  Valor real: {y_test_demo[i]}\")\n    print(f\"  {'‚úì Correcto' if y_pred_propio[i] == y_test_demo[i] else '‚úó Incorrecto'}\")\n\n============================================================\nIMPLEMENTACI√ìN DESDE CERO\n============================================================\nEntrenando con 20 muestras y 2 caracter√≠sticas\nClases encontradas: [0 1]\n\nClase 0: 8 muestras (40.0%)\n  Medias: [ 0.02629741 -1.03784986]\n  Varianzas: [0.69718407 0.29135071]\n\nClase 1: 12 muestras (60.0%)\n  Medias: [0.35445188 0.56734888]\n  Varianzas: [1.40440918 0.69226573]\n\n============================================================\nPREDICCIONES\n============================================================\n\nMuestra 1:\n  Caracter√≠sticas: [-0.43, -0.48]\n  Probabilidades: P(Y=0|X)=0.672, P(Y=1|X)=0.328\n  Predicci√≥n: 0\n  Valor real: 0\n  ‚úì Correcto\n\nMuestra 2:\n  Caracter√≠sticas: [0.85, -0.84]\n  Probabilidades: P(Y=0|X)=0.792, P(Y=1|X)=0.208\n  Predicci√≥n: 0\n  Valor real: 0\n  ‚úì Correcto\n\nMuestra 3:\n  Caracter√≠sticas: [-0.60, -1.15]\n  Probabilidades: P(Y=0|X)=0.926, P(Y=1|X)=0.074\n  Predicci√≥n: 0\n  Valor real: 0\n  ‚úì Correcto\n\nMuestra 4:\n  Caracter√≠sticas: [0.96, 1.52]\n  Probabilidades: P(Y=0|X)=0.000, P(Y=1|X)=1.000\n  Predicci√≥n: 1\n  Valor real: 0\n  ‚úó Incorrecto\n\nMuestra 5:\n  Caracter√≠sticas: [0.72, -0.68]\n  Probabilidades: P(Y=0|X)=0.727, P(Y=1|X)=0.273\n  Predicci√≥n: 0\n  Valor real: 0\n  ‚úì Correcto\n\n\n\n# Comparaci√≥n con scikit-learn\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARACI√ìN CON SCIKIT-LEARN\")\nprint(\"=\" * 60)\n\n# Entrenar modelo de scikit-learn con los mismos datos\ngnb_sklearn_demo = GaussianNB()\ngnb_sklearn_demo.fit(X_demo, y_demo)\ny_pred_sklearn_demo = gnb_sklearn_demo.predict(X_test_demo)\n\n# Comparar resultados\nprint(\"\\nPredicciones:\")\nprint(f\"  Implementaci√≥n propia: {y_pred_propio}\")\nprint(f\"  Scikit-learn:         {y_pred_sklearn_demo}\")\nprint(f\"  Valores reales:       {y_test_demo}\")\n\n# Calcular accuracy\nacc_propio = np.mean(y_pred_propio == y_test_demo)\nacc_sklearn = np.mean(y_pred_sklearn_demo == y_test_demo)\n\nprint(f\"\\nAccuracy:\")\nprint(f\"  Implementaci√≥n propia: {acc_propio:.3f}\")\nprint(f\"  Scikit-learn:         {acc_sklearn:.3f}\")\n\n# Verificar que los par√°metros aprendidos son similares\nprint(\"\\n\" + \"=\" * 60)\nprint(\"VERIFICACI√ìN DE PAR√ÅMETROS APRENDIDOS\")\nprint(\"=\" * 60)\n\nfor clase in [0, 1]:\n    print(f\"\\nClase {clase}:\")\n    print(f\"  Priors:\")\n    print(f\"    Propio: {modelo_propio.priors[clase]:.3f}\")\n    print(f\"    Sklearn: {gnb_sklearn_demo.class_prior_[clase]:.3f}\")\n    print(f\"  Medias (primera caracter√≠stica):\")\n    print(f\"    Propio: {modelo_propio.medias[clase][0]:.3f}\")\n    print(f\"    Sklearn: {gnb_sklearn_demo.theta_[clase, 0]:.3f}\")\n\n\n============================================================\nCOMPARACI√ìN CON SCIKIT-LEARN\n============================================================\n\nPredicciones:\n  Implementaci√≥n propia: [0 0 0 1 0 0 0 0 1 1]\n  Scikit-learn:         [0 0 0 1 0 0 0 0 1 1]\n  Valores reales:       [0 0 0 0 0 0 0 0 0 1]\n\nAccuracy:\n  Implementaci√≥n propia: 0.800\n  Scikit-learn:         0.800\n\n============================================================\nVERIFICACI√ìN DE PAR√ÅMETROS APRENDIDOS\n============================================================\n\nClase 0:\n  Priors:\n    Propio: 0.400\n    Sklearn: 0.400\n  Medias (primera caracter√≠stica):\n    Propio: 0.026\n    Sklearn: 0.026\n\nClase 1:\n  Priors:\n    Propio: 0.600\n    Sklearn: 0.600\n  Medias (primera caracter√≠stica):\n    Propio: 0.354\n    Sklearn: 0.354\n\n\n\n\n\nCu√°ndo Usar Naive Bayes\nNaive Bayes es particularmente efectivo en:\n\nClasificaci√≥n de texto y procesamiento de lenguaje natural\n\nFiltrado de spam\nAn√°lisis de sentimientos\nCategorizaci√≥n de documentos\n\nSistemas de recomendaci√≥n\n\nPredicci√≥n de preferencias basada en caracter√≠sticas\n\nDiagn√≥stico m√©dico\n\nCuando las caracter√≠sticas son s√≠ntomas relativamente independientes\n\nAplicaciones en tiempo real\n\nCuando se necesitan predicciones muy r√°pidas\n\nConjuntos de datos peque√±os\n\nCuando hay pocos ejemplos de entrenamiento por clase\n\n\nEl clasificador Naive Bayes, a pesar de su simplicidad, sigue siendo uno de los algoritmos fundamentales en machine learning, especialmente valioso como baseline y en aplicaciones donde la velocidad y simplicidad son cr√≠ticas.\n\n\n\nRegresi√≥n Log√≠stica\nLa regresi√≥n log√≠stica es uno de los modelos m√°s utilizados para clasificaci√≥n binaria. Modela directamente la probabilidad posterior usando una transformaci√≥n log√≠stica de una combinaci√≥n lineal de las caracter√≠sticas.\n\nModelo\nLa regresi√≥n log√≠stica modela la probabilidad de que \\(Y = 1\\) como:\n\\[P(Y = 1 | \\mathbf{X} = \\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p)}} = \\frac{1}{1 + e^{-\\mathbf{x}^T\\boldsymbol{\\beta}}}\\]\nEsta funci√≥n se conoce como funci√≥n sigmoide o log√≠stica:\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z}\\]\n\n\nTransformaci√≥n Logit\nEl modelo puede reescribirse usando la transformaci√≥n logit (log-odds):\n\\[\\log\\left(\\frac{P(Y = 1 | \\mathbf{x})}{P(Y = 0 | \\mathbf{x})}\\right) = \\log\\left(\\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})}\\right) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\\]\nEsto muestra que el log-odds es una funci√≥n lineal de las caracter√≠sticas.\n\n\nEstimaci√≥n de Par√°metros\nLos par√°metros \\(\\boldsymbol{\\beta}\\) se estiman maximizando la verosimilitud. Para \\(n\\) observaciones:\n\\[L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p(\\mathbf{x}_i)^{y_i} \\cdot (1-p(\\mathbf{x}_i))^{1-y_i}\\]\nTomando el logaritmo:\n\\[\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} [y_i \\log(p(\\mathbf{x}_i)) + (1-y_i) \\log(1-p(\\mathbf{x}_i))]\\]\nEsta es exactamente la negativa de la p√©rdida logar√≠tmica. No existe soluci√≥n anal√≠tica, por lo que se utiliza optimizaci√≥n num√©rica (t√≠picamente Newton-Raphson o gradiente descendente).\n\n\nFrontera de Decisi√≥n\nLa frontera de decisi√≥n en regresi√≥n log√≠stica es lineal en el espacio de caracter√≠sticas:\n\\[\\{\\mathbf{x} : P(Y = 1 | \\mathbf{x}) = 0.5\\} = \\{\\mathbf{x} : \\mathbf{x}^T\\boldsymbol{\\beta} = 0\\}\\]\nEsto define un hiperplano que separa las dos clases.\n\n\nEjemplo en Python\n\n\n\n\n\nRegresi√≥n log√≠stica: datos, probabilidades y frontera de decisi√≥n\n\n\n\n\nIntercepto (Œ≤‚ÇÄ): 0.057\nCoeficientes: Œ≤‚ÇÅ = -0.273, Œ≤‚ÇÇ = 2.214\n\n\n\n\nInterpretaci√≥n de Coeficientes\n\nConceptos Fundamentales: Odds y Log-Odds\nAntes de interpretar los coeficientes, definamos los conceptos clave:\nOdds (momios o chances): La raz√≥n entre la probabilidad de √©xito y la probabilidad de fracaso:\n\\[\\text{Odds} = \\frac{P(Y = 1)}{P(Y = 0)} = \\frac{p}{1-p}\\]\nSi \\(p = 0.75\\), entonces los odds son \\(\\frac{0.75}{0.25} = 3\\), es decir, el √©xito es 3 veces m√°s probable que el fracaso.\nLog-odds (logit): El logaritmo natural de los odds:\n\\[\\text{Log-odds} = \\log\\left(\\frac{p}{1-p}\\right) = \\text{logit}(p)\\]\n\n\nDerivaci√≥n Matem√°tica\nPartiendo del modelo de regresi√≥n log√≠stica:\n\\[P(Y = 1 | \\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\sum_{j=1}^p \\beta_j x_j)}}\\]\nCalculemos el log-odds:\n\\[\\log\\left(\\frac{P(Y = 1 | \\mathbf{x})}{1 - P(Y = 1 | \\mathbf{x})}\\right) = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j\\]\nAhora, consideremos qu√© sucede cuando incrementamos \\(x_k\\) en una unidad (de \\(x_k\\) a \\(x_k + 1\\)):\nLog-odds original: \\[L_0 = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k + ... + \\beta_p x_p\\]\nLog-odds despu√©s del incremento: \\[L_1 = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k (x_k + 1) + ... + \\beta_p x_p\\]\nCambio en log-odds: \\[\\Delta L = L_1 - L_0 = \\beta_k\\]\nPor lo tanto, \\(\\beta_k\\) representa el cambio en log-odds cuando \\(x_k\\) aumenta en una unidad.\n\n\nOdds Ratio\nEl odds ratio compara los odds antes y despu√©s del cambio:\n\\[\\text{Odds ratio} = \\frac{\\text{Odds}_{\\text{nuevo}}}{\\text{Odds}_{\\text{original}}} = \\frac{e^{L_1}}{e^{L_0}} = e^{L_1 - L_0} = e^{\\beta_k}\\]\nEsto significa que \\(e^{\\beta_k}\\) es el factor por el cual se multiplican los odds cuando \\(x_k\\) aumenta en una unidad.\n\n\nEjemplo Pr√°ctico: Clicks en Memes y Edad\nImaginemos un estudio sobre la probabilidad de que una persona haga click en un meme seg√∫n su edad. Nuestro modelo de regresi√≥n log√≠stica es:\n\\[\\log\\left(\\frac{P(\\text{click} = 1)}{P(\\text{click} = 0)}\\right) = 2.5 - 0.08 \\cdot \\text{edad}\\]\nDonde: - \\(\\beta_0 = 2.5\\) (intercepto) - \\(\\beta_{\\text{edad}} = -0.08\\) (coeficiente de edad)\nInterpretaciones:\n\nCoeficiente \\(\\beta_{\\text{edad}} = -0.08\\):\n\nPor cada a√±o adicional de edad, el log-odds de hacer click disminuye en 0.08\nEl signo negativo indica que personas mayores tienen menor probabilidad de hacer click\n\nOdds ratio \\(e^{-0.08} \\approx 0.923\\):\n\nPor cada a√±o adicional de edad, los odds de hacer click se multiplican por 0.923\nEquivalentemente: los odds disminuyen un 7.7% por cada a√±o adicional\n\nEjemplo num√©rico concreto:\n\nPara una persona de 20 a√±os: \\[\\text{Log-odds}_{20} = 2.5 - 0.08(20) = 0.9\\] \\[\\text{Odds}_{20} = e^{0.9} \\approx 2.46\\] \\[P(\\text{click})_{20} = \\frac{2.46}{1 + 2.46} \\approx 0.71\\]\nPara una persona de 30 a√±os: \\[\\text{Log-odds}_{30} = 2.5 - 0.08(30) = 0.1\\] \\[\\text{Odds}_{30} = e^{0.1} \\approx 1.11\\] \\[P(\\text{click})_{30} = \\frac{1.11}{1 + 1.11} \\approx 0.53\\]\nVerificaci√≥n del odds ratio: \\[\\frac{\\text{Odds}_{30}}{\\text{Odds}_{20}} = \\frac{1.11}{2.46} \\approx 0.45 = e^{-0.08 \\times 10} = (e^{-0.08})^{10}\\]\nEsto confirma que en 10 a√±os (de 20 a 30), los odds se multiplican por \\((0.923)^{10} \\approx 0.45\\).\n\n\nResumen de Interpretaciones\n\n\n\n\n\n\n\n\nPar√°metro\nInterpretaci√≥n\nEjemplo (edad y clicks)\n\n\n\n\n\\(\\beta_j &gt; 0\\)\nVariable aumenta log-odds\nLos j√≥venes clickean m√°s\n\n\n\\(\\beta_j &lt; 0\\)\nVariable disminuye log-odds\nLos mayores clickean menos\n\n\n\\(\\beta_j\\)\nCambio en log-odds por unidad\n-0.08: cada a√±o reduce log-odds\n\n\n\\(e^{\\beta_j} &gt; 1\\)\nOdds aumentan\n-\n\n\n\\(e^{\\beta_j} &lt; 1\\)\nOdds disminuyen\n0.923: odds bajan 7.7% por a√±o\n\n\n\\(e^{\\beta_j} = 2\\)\nOdds se duplican\n-\n\n\n\\(e^{\\beta_j} = 0.5\\)\nOdds se reducen a la mitad\n-",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html#m√©tricas-de-evaluaci√≥n-de-modelos-de-clasificaci√≥n",
    "href": "04-clasificacion.html#m√©tricas-de-evaluaci√≥n-de-modelos-de-clasificaci√≥n",
    "title": "Clasificaci√≥n",
    "section": "M√©tricas de Evaluaci√≥n de Modelos de Clasificaci√≥n",
    "text": "M√©tricas de Evaluaci√≥n de Modelos de Clasificaci√≥n\nUna vez entrenado un modelo de clasificaci√≥n, necesitamos evaluar su desempe√±o de manera rigurosa. Mientras que las funciones de p√©rdida (como Brier Score y Log Loss) son √∫tiles durante el entrenamiento, las m√©tricas de evaluaci√≥n nos permiten interpretar el rendimiento del modelo desde diferentes perspectivas y tomar decisiones informadas sobre su uso en producci√≥n.\n\nLa Matriz de Confusi√≥n: Fundamento de las M√©tricas\nLa matriz de confusi√≥n es la herramienta fundamental para entender el comportamiento de un clasificador binario. Para clasificaci√≥n binaria (clase positiva = 1, clase negativa = 0), la matriz tiene la siguiente estructura:\n\n\n\n\n\n\n\n\n\nPredicci√≥n Positiva (1)\nPredicci√≥n Negativa (0)\n\n\n\n\nClase Real Positiva (1)\nVerdaderos Positivos (VP)\nFalsos Negativos (FN)\n\n\nClase Real Negativa (0)\nFalsos Positivos (FP)\nVerdaderos Negativos (VN)\n\n\n\nDonde:\n\nVerdaderos Positivos (VP): Casos positivos correctamente identificados\nVerdaderos Negativos (VN): Casos negativos correctamente identificados\nFalsos Positivos (FP): Casos negativos incorrectamente clasificados como positivos (Error Tipo I)\nFalsos Negativos (FN): Casos positivos incorrectamente clasificados como negativos (Error Tipo II)\n\n\nInterpretaci√≥n en Contexto\nLa importancia relativa de cada tipo de error depende del contexto de aplicaci√≥n:\nEjemplo 1: Detecci√≥n de Spam\n\nFP (Error Tipo I): Email leg√≠timo marcado como spam ‚Üí Usuario pierde email importante\nFN (Error Tipo II): Spam no detectado ‚Üí Usuario recibe spam (menor consecuencia)\nPrioridad: Minimizar FP (alta precisi√≥n)\n\nEjemplo 2: Diagn√≥stico de C√°ncer\n\nFP (Error Tipo I): Falso positivo ‚Üí Paciente sano sometido a pruebas adicionales\nFN (Error Tipo II): Falso negativo ‚Üí Paciente enfermo no recibe tratamiento\nPrioridad: Minimizar FN (alta sensibilidad/recall)\n\nEjemplo 3: Detecci√≥n de Fraude Bancario\n\nFP (Error Tipo I): Transacci√≥n leg√≠tima bloqueada ‚Üí Cliente molesto\nFN (Error Tipo II): Fraude no detectado ‚Üí P√©rdida econ√≥mica\nPrioridad: Balance entre ambos (F1-score)\n\n\n\n\nM√©tricas Derivadas de la Matriz de Confusi√≥n\n\n1. Exactitud (Accuracy)\nLa exactitud es la proporci√≥n de predicciones correctas sobre el total:\n\\[\\text{Accuracy} = \\frac{VP + VN}{VP + VN + FP + FN}\\]\nVentajas:\n\n‚úì Interpretaci√≥n intuitiva\n‚úì M√©trica general del desempe√±o\n\nDesventajas:\n\n‚úó Enga√±osa con clases desbalanceadas\n‚úó No distingue entre tipos de errores\n\n\n\n\n\n\n\nLimitaci√≥n\n\n\n\n\n\nSi el 95% de los emails son leg√≠timos, un clasificador que siempre predice ‚Äúno spam‚Äù tendr√° 95% de accuracy, pero es completamente in√∫til.\n\n\n\n\n\n2. Precisi√≥n (Precision)\nLa precisi√≥n mide la proporci√≥n de predicciones positivas que son realmente positivas:\n\\[\\text{Precision} = \\frac{VP}{VP + FP} = \\frac{VP}{\\text{Total Predicciones Positivas}}\\]\nInterpretaci√≥n: ‚ÄúDe todos los casos que el modelo predijo como positivos, ¬øqu√© proporci√≥n es realmente positiva?‚Äù\nCu√°ndo usar:\n\nCuando los falsos positivos son costosos\nEjemplo: Recomendaci√≥n de productos (no queremos recomendar productos irrelevantes)\n\n\n\n3. Sensibilidad (Recall, Sensitivity, True Positive Rate)\nLa sensibilidad o recall mide la proporci√≥n de casos positivos que fueron correctamente identificados:\n\\[\\text{Recall} = \\frac{VP}{VP + FN} = \\frac{VP}{\\text{Total Casos Positivos Reales}}\\]\nInterpretaci√≥n: ‚ÄúDe todos los casos que son realmente positivos, ¬øqu√© proporci√≥n detect√≥ el modelo?‚Äù\nCu√°ndo usar:\n\nCuando los falsos negativos son cr√≠ticos\nEjemplo: Detecci√≥n de enfermedades graves (no queremos dejar casos sin diagnosticar)\n\n\n\n4. Especificidad (Specificity, True Negative Rate)\nLa especificidad mide la proporci√≥n de casos negativos correctamente identificados:\n\\[\\text{Specificity} = \\frac{VN}{VN + FP} = \\frac{VN}{\\text{Total Casos Negativos Reales}}\\]\nInterpretaci√≥n: ‚ÄúDe todos los casos que son realmente negativos, ¬øqu√© proporci√≥n identific√≥ correctamente el modelo?‚Äù\n\n\n5. F1-Score\nEl F1-Score es la media arm√≥nica de precisi√≥n y recall:\n\\[F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\cdot VP}{2 \\cdot VP + FP + FN}\\]\nPropiedades:\n\nRango: \\([0, 1]\\) (mayor es mejor)\nPenaliza desbalances entre precision y recall\nSi precision = recall, entonces \\(F_1 = \\text{precision} = \\text{recall}\\)\n\nCu√°ndo usar:\n\nCuando se necesita un balance entre precision y recall\nCon clases desbalanceadas\nComo m√©trica √∫nica de comparaci√≥n entre modelos\n\n\n\n6. F-Beta Score\nGeneralizaci√≥n del F1-Score que permite ponderar la importancia relativa de precision y recall:\n\\[F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\beta^2 \\cdot \\text{Precision} + \\text{Recall}}\\]\nDonde \\(\\beta\\) controla el peso relativo:\n\n\\(\\beta &lt; 1\\): M√°s peso a la precisi√≥n (ej: \\(F_{0.5}\\))\n\\(\\beta = 1\\): Peso igual (F1-Score)\n\\(\\beta &gt; 1\\): M√°s peso al recall (ej: \\(F_2\\))\n\n\n\n\nUmbral de Decisi√≥n\nHasta ahora hemos hablado de ‚Äúpredicciones‚Äù como si fueran categ√≥ricas (clase 0 o clase 1), pero es importante entender que la mayor√≠a de los modelos de clasificaci√≥n en realidad producen probabilidades que luego se convierten en predicciones discretas mediante un umbral de decisi√≥n (decision threshold).\n\nDe Probabilidades a Predicciones\nLos modelos probabil√≠sticos (como regresi√≥n log√≠stica, Naive Bayes, redes neuronales) no predicen directamente una clase, sino que estiman:\n\\[\\hat{p} = P(Y = 1 | \\mathbf{x})\\]\nPara convertir esta probabilidad en una predicci√≥n categ√≥rica, se utiliza un umbral de decisi√≥n \\(\\tau\\):\n\\[\\hat{y} = \\begin{cases}\n1 & \\text{si } \\hat{p} \\geq \\tau \\\\\n0 & \\text{si } \\hat{p} &lt; \\tau\n\\end{cases}\\]\n\n\nEl Umbral Est√°ndar: 0.5\nPor defecto, la mayor√≠a de las implementaciones usan \\(\\tau = 0.5\\):\n\nSi \\(P(Y = 1 | \\mathbf{x}) \\geq 0.5\\) ‚Üí Predecir clase positiva (1)\nSi \\(P(Y = 1 | \\mathbf{x}) &lt; 0.5\\) ‚Üí Predecir clase negativa (0)\n\nEsta elecci√≥n parece natural desde una perspectiva bayesiana (seleccionar la clase m√°s probable), pero no siempre es √≥ptima en la pr√°ctica.\n\n\n¬øPor Qu√© Cambiar el Umbral?\nEl umbral de decisi√≥n debe ajustarse seg√∫n el contexto y los costos relativos de los errores:\nEjemplo 1: Detecci√≥n de C√°ncer\n\nCosto de FN (no detectar c√°ncer): Muy alto (riesgo de vida)\nCosto de FP (falsa alarma): Moderado (pruebas adicionales, ansiedad)\nSoluci√≥n: Usar \\(\\tau = 0.3\\) o menor ‚Üí M√°s sensible, captura m√°s casos positivos\n\nEjemplo 2: Recomendaci√≥n de Productos Premium\n\nCosto de FP (recomendar a quien no comprar√°): Alto (recursos desperdiciados)\nCosto de FN (no recomendar a comprador potencial): Moderado\nSoluci√≥n: Usar \\(\\tau = 0.7\\) o mayor ‚Üí M√°s preciso, solo casos muy probables\n\nEjemplo 3: Filtro de Spam\n\nCosto de FP (email leg√≠timo marcado como spam): Alto (p√©rdida de informaci√≥n importante)\nCosto de FN (spam no detectado): Bajo (molestia menor)\nSoluci√≥n: Usar \\(\\tau = 0.6\\) ‚Üí Balance hacia alta precisi√≥n\n\n\n\nImpacto del Umbral en las M√©tricas\nVeamos con un ejemplo concreto c√≥mo el umbral afecta las predicciones:\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n\n# Ejemplo: 10 casos con sus probabilidades predichas y etiquetas reales\nnp.random.seed(42)\nn_ejemplos = 10\n\n# Simular probabilidades y etiquetas reales\ndata_ejemplo = pd.DataFrame({\n    'ID': range(1, n_ejemplos + 1),\n    'Probabilidad': [0.15, 0.32, 0.48, 0.55, 0.62, 0.71, 0.78, 0.85, 0.91, 0.95],\n    'Clase_Real': [0, 0, 0, 1, 1, 0, 1, 1, 1, 1]\n})\n\nprint(\"DATOS DE EJEMPLO\")\nprint(\"=\" * 60)\nprint(data_ejemplo.to_string(index=False))\n\n# Probar diferentes umbrales\numbrales = [0.3, 0.5, 0.7]\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"IMPACTO DEL UMBRAL EN LAS PREDICCIONES\")\nprint(\"=\" * 60)\n\nfor tau in umbrales:\n    # Aplicar umbral\n    predicciones = (data_ejemplo['Probabilidad'] &gt;= tau).astype(int)\n\n    # Calcular m√©tricas\n    cm = confusion_matrix(data_ejemplo['Clase_Real'], predicciones)\n    precision = precision_score(data_ejemplo['Clase_Real'], predicciones, zero_division=0)\n    recall = recall_score(data_ejemplo['Clase_Real'], predicciones)\n    f1 = f1_score(data_ejemplo['Clase_Real'], predicciones, zero_division=0)\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Umbral œÑ = {tau}\")\n    print(f\"{'='*60}\")\n    print(f\"Predicciones: {predicciones.tolist()}\")\n    print(f\"VP={cm[1,1]}, VN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}\")\n    print(f\"Precision: {precision:.3f}\")\n    print(f\"Recall:    {recall:.3f}\")\n    print(f\"F1-Score:  {f1:.3f}\")\n\nDATOS DE EJEMPLO\n============================================================\n ID  Probabilidad  Clase_Real\n  1          0.15           0\n  2          0.32           0\n  3          0.48           0\n  4          0.55           1\n  5          0.62           1\n  6          0.71           0\n  7          0.78           1\n  8          0.85           1\n  9          0.91           1\n 10          0.95           1\n\n============================================================\nIMPACTO DEL UMBRAL EN LAS PREDICCIONES\n============================================================\n\n============================================================\nUmbral œÑ = 0.3\n============================================================\nPredicciones: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nVP=6, VN=1, FP=3, FN=0\nPrecision: 0.667\nRecall:    1.000\nF1-Score:  0.800\n\n============================================================\nUmbral œÑ = 0.5\n============================================================\nPredicciones: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\nVP=6, VN=3, FP=1, FN=0\nPrecision: 0.857\nRecall:    1.000\nF1-Score:  0.923\n\n============================================================\nUmbral œÑ = 0.7\n============================================================\nPredicciones: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\nVP=4, VN=3, FP=1, FN=2\nPrecision: 0.800\nRecall:    0.667\nF1-Score:  0.727\n\n\nComo podemos observar:\n\nUmbral bajo (œÑ = 0.3): M√°s predicciones positivas ‚Üí Mayor recall, menor precision\nUmbral medio (œÑ = 0.5): Caso est√°ndar (balance)\nUmbral alto (œÑ = 0.7): Menos predicciones positivas ‚Üí Mayor precision, menor recall\n\n\n\nSelecci√≥n del Umbral √ìptimo\nLa selecci√≥n del umbral √≥ptimo depende de:\n\nCostos de negocio: Cuantificar el costo relativo de FP vs FN\nM√©tricas objetivo: Optimizar para la m√©trica m√°s relevante (precision, recall, F1, etc.)\nRestricciones operacionales: Capacidad para manejar volumen de casos positivos\nValidaci√≥n emp√≠rica: Usar curvas Precision-Recall o ROC para explorar opciones\n\n\n\n\nTrade-off entre Precisi√≥n y Recall\nAhora que comprendemos el concepto de umbral de decisi√≥n, podemos analizar el trade-off fundamental entre precision y recall:\n\nAumentar el umbral (\\(\\tau \\uparrow\\)) ‚Üí M√°s conservador ‚Üí ‚Üë Precision, ‚Üì Recall\nDisminuir el umbral (\\(\\tau \\downarrow\\)) ‚Üí M√°s liberal ‚Üí ‚Üì Precision, ‚Üë Recall\n\nEste trade-off es inherente a cualquier clasificador probabil√≠stico y no puede eliminarse, solo puede balancearse seg√∫n las necesidades del problema.\n\n\n\n\n\nTrade-off entre Precisi√≥n y Recall seg√∫n el umbral de decisi√≥n\n\n\n\n\nM√©tricas con umbral = 0.5:\n  Precision: 0.789\n  Recall:    0.602\n  F1-Score:  0.683\n\n\n\n\nCurva ROC y AUC\n\nCurva ROC (Receiver Operating Characteristic)\nLa curva ROC visualiza el desempe√±o del clasificador en todos los posibles umbrales de decisi√≥n, graficando:\n\nEje Y: Tasa de Verdaderos Positivos (TPR = Recall = Sensibilidad)\nEje X: Tasa de Falsos Positivos (FPR = 1 - Especificidad)\n\n\\[\\text{TPR} = \\frac{VP}{VP + FN}, \\quad \\text{FPR} = \\frac{FP}{FP + VN}\\]\nPuntos de referencia:\n\nClasificador perfecto: TPR = 1, FPR = 0 (esquina superior izquierda)\nClasificador aleatorio: L√≠nea diagonal (TPR = FPR)\nPeor clasificador: TPR = 0, FPR = 1\n\n\n\nAUC (Area Under the Curve)\nEl AUC es el √°rea bajo la curva ROC:\n\\[\\text{AUC} \\in [0, 1]\\]\nInterpretaci√≥n:\n\nAUC = 1.0: Clasificador perfecto\nAUC = 0.9-1.0: Excelente\nAUC = 0.8-0.9: Muy bueno\nAUC = 0.7-0.8: Bueno\nAUC = 0.6-0.7: Regular\nAUC = 0.5: No mejor que azar\nAUC &lt; 0.5: Peor que azar (predicciones invertidas)\n\nInterpretaci√≥n probabil√≠stica\nEl AUC representa la probabilidad de que el modelo asigne una mayor probabilidad a un ejemplo positivo aleatorio que a un ejemplo negativo aleatorio.\n\n\n\n\n\nCurvas ROC y AUC para comparaci√≥n de modelos\n\n\n\n\n\n\n\nEjemplo Completo: Evaluaci√≥n de un Modelo\n\nfrom sklearn.metrics import (classification_report, confusion_matrix,\n                            accuracy_score, precision_score, recall_score,\n                            f1_score, roc_auc_score)\nimport pandas as pd\nimport seaborn as sns\n\n# Usar el modelo de Regresi√≥n Log√≠stica entrenado anteriormente\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]\n\nprint(\"=\" * 70)\nprint(\"EVALUACI√ìN COMPLETA DEL MODELO DE CLASIFICACI√ìN\")\nprint(\"=\" * 70)\n\n# 1. Matriz de Confusi√≥n\nprint(\"\\n1. MATRIZ DE CONFUSI√ìN\")\nprint(\"-\" * 70)\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm,\n                     columns=['Predicho Negativo (0)', 'Predicho Positivo (1)'],\n                     index=['Real Negativo (0)', 'Real Positivo (1)'])\nprint(cm_df)\n\n# Extraer valores\nVP = cm[1, 1]  # Verdaderos Positivos\nVN = cm[0, 0]  # Verdaderos Negativos\nFP = cm[0, 1]  # Falsos Positivos\nFN = cm[1, 0]  # Falsos Negativos\n\nprint(f\"\\n  VP (Verdaderos Positivos): {VP}\")\nprint(f\"  VN (Verdaderos Negativos): {VN}\")\nprint(f\"  FP (Falsos Positivos):     {FP}\")\nprint(f\"  FN (Falsos Negativos):     {FN}\")\n\n# 2. M√©tricas principales\nprint(\"\\n2. M√âTRICAS DE DESEMPE√ëO\")\nprint(\"-\" * 70)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nspecificity = VN / (VN + FP)\nauc_score = roc_auc_score(y_test, y_proba)\n\nmetricas = {\n    'M√©trica': ['Accuracy', 'Precision', 'Recall (Sensibilidad)',\n                'Specificity', 'F1-Score', 'AUC-ROC'],\n    'Valor': [accuracy, precision, recall, specificity, f1, auc_score],\n    'Interpretaci√≥n': [\n        'Proporci√≥n de predicciones correctas',\n        'De las predicciones positivas, proporci√≥n correcta',\n        'De los casos positivos reales, proporci√≥n detectada',\n        'De los casos negativos reales, proporci√≥n correcta',\n        'Media arm√≥nica de Precision y Recall',\n        '√Årea bajo la curva ROC'\n    ]\n}\n\nmetricas_df = pd.DataFrame(metricas)\nmetricas_df['Valor'] = metricas_df['Valor'].apply(lambda x: f'{x:.3f}')\nprint(metricas_df.to_string(index=False))\n\n# 3. Reporte de clasificaci√≥n completo\nprint(\"\\n3. REPORTE DE CLASIFICACI√ìN DETALLADO\")\nprint(\"-\" * 70)\nprint(classification_report(y_test, y_pred, target_names=['Clase 0', 'Clase 1']))\n\n======================================================================\nEVALUACI√ìN COMPLETA DEL MODELO DE CLASIFICACI√ìN\n======================================================================\n\n1. MATRIZ DE CONFUSI√ìN\n----------------------------------------------------------------------\n                   Predicho Negativo (0)  Predicho Positivo (1)\nReal Negativo (0)                    192                     15\nReal Positivo (1)                     37                     56\n\n  VP (Verdaderos Positivos): 56\n  VN (Verdaderos Negativos): 192\n  FP (Falsos Positivos):     15\n  FN (Falsos Negativos):     37\n\n2. M√âTRICAS DE DESEMPE√ëO\n----------------------------------------------------------------------\n              M√©trica Valor                                      Interpretaci√≥n\n             Accuracy 0.827                Proporci√≥n de predicciones correctas\n            Precision 0.789  De las predicciones positivas, proporci√≥n correcta\nRecall (Sensibilidad) 0.602 De los casos positivos reales, proporci√≥n detectada\n          Specificity 0.928  De los casos negativos reales, proporci√≥n correcta\n             F1-Score 0.683                Media arm√≥nica de Precision y Recall\n              AUC-ROC 0.872                              √Årea bajo la curva ROC\n\n3. REPORTE DE CLASIFICACI√ìN DETALLADO\n----------------------------------------------------------------------\n              precision    recall  f1-score   support\n\n     Clase 0       0.84      0.93      0.88       207\n     Clase 1       0.79      0.60      0.68        93\n\n    accuracy                           0.83       300\n   macro avg       0.81      0.76      0.78       300\nweighted avg       0.82      0.83      0.82       300\n\n\n\n\n# Visualizaci√≥n mejorada de la matriz de confusi√≥n\nplt.figure(figsize=(8, 6))\n\n# Crear matriz de confusi√≥n normalizada\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n# Crear anotaciones personalizadas con conteos y porcentajes\nannot = np.empty_like(cm, dtype=object)\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        annot[i, j] = f'{cm[i, j]}\\n({cm_normalized[i, j]:.1%})'\n\n# Crear heatmap\nsns.heatmap(cm_normalized, annot=annot, fmt='', cmap='Blues',\n            xticklabels=['Predicho: 0', 'Predicho: 1'],\n            yticklabels=['Real: 0', 'Real: 1'],\n            cbar_kws={'label': 'Proporci√≥n'},\n            vmin=0, vmax=1, linewidths=2, linecolor='white')\n\nplt.title(f'Matriz de Confusi√≥n (Accuracy = {accuracy:.3f})', fontsize=14, pad=15)\nplt.ylabel('Clase Real', fontsize=12)\nplt.xlabel('Clase Predicha', fontsize=12)\n\n# A√±adir etiquetas descriptivas\nplt.text(-0.5, 0.5, f'VN\\n{VN}', ha='center', va='center', fontsize=10,\n         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\nplt.text(1.5, 0.5, f'FP\\n{FP}', ha='center', va='center', fontsize=10,\n         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\nplt.text(-0.5, 1.5, f'FN\\n{FN}', ha='center', va='center', fontsize=10,\n         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\nplt.text(1.5, 1.5, f'VP\\n{VP}', ha='center', va='center', fontsize=10,\n         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nVisualizaci√≥n de la Matriz de Confusi√≥n\n\n\n\n\n\n\nSelecci√≥n de M√©tricas seg√∫n el Contexto\nLa elecci√≥n de la m√©trica apropiada depende del problema espec√≠fico:\n\n\n\n\n\n\n\n\nContexto\nM√©trica Recomendada\nRaz√≥n\n\n\n\n\nClases balanceadas\nAccuracy\nProporci√≥n general de aciertos es suficiente\n\n\nClases desbalanceadas\nF1-Score, AUC-ROC\nAccuracy puede ser enga√±osa\n\n\nCosto alto de FP\nPrecision\nMinimizar falsos positivos\n\n\nCosto alto de FN\nRecall\nMinimizar falsos negativos\n\n\nBalance entre FP y FN\nF1-Score\nConsidera ambos errores\n\n\nComparaci√≥n de modelos\nAUC-ROC\nIndependiente del umbral\n\n\nDiagn√≥stico m√©dico\nRecall, AUC-ROC\nNo perder casos positivos\n\n\nFiltro de spam\nPrecision, F1-Score\nNo bloquear emails leg√≠timos\n\n\nDetecci√≥n de fraude\nF1-Score, Recall\nBalance seg√∫n costo relativo\n\n\n\n\n\nM√©tricas para Clasificaci√≥n Multiclase\nPara problemas con \\(K &gt; 2\\) clases, las m√©tricas se generalizan de dos formas:\n\n1. Macro-averaging\nCalcula la m√©trica para cada clase por separado y promedia:\n\\[\\text{Precision}_{\\text{macro}} = \\frac{1}{K} \\sum_{k=1}^{K} \\text{Precision}_k\\]\nVentaja: Trata todas las clases por igual (√∫til si todas las clases son igualmente importantes)\n\n\n2. Weighted-averaging\nPromedio ponderado por el n√∫mero de muestras reales de cada clase:\n\\[\\text{Precision}_{\\text{weighted}} = \\sum_{k=1}^{K} w_k \\cdot \\text{Precision}_k\\]\nDonde \\(w_k = \\frac{n_k}{n}\\) (proporci√≥n de muestras de la clase \\(k\\))\nVentaja: Tiene en cuenta el desbalance de clases\n\n# Ejemplo con dataset Iris (3 clases)\nfrom sklearn.datasets import load_iris\n\n# Cargar datos\niris = load_iris()\nX_iris_full = iris.data\ny_iris_full = iris.target\n\n# Dividir datos\nX_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n    X_iris_full, y_iris_full, test_size=0.3, random_state=42, stratify=y_iris_full\n)\n\n# Entrenar modelo\nmodel_iris = LogisticRegression(max_iter=1000)\nmodel_iris.fit(X_train_iris, y_train_iris)\n\n# Predicciones\ny_pred_iris = model_iris.predict(X_test_iris)\n\nprint(\"=\" * 70)\nprint(\"EVALUACI√ìN MULTICLASE (Dataset Iris - 3 clases)\")\nprint(\"=\" * 70)\n\n# Reporte de clasificaci√≥n con macro y weighted averaging\nprint(\"\\nReporte de Clasificaci√≥n:\")\nprint(classification_report(y_test_iris, y_pred_iris,\n                          target_names=iris.target_names,\n                          digits=3))\n\n# Matriz de confusi√≥n multiclase\nprint(\"\\nMatriz de Confusi√≥n:\")\ncm_iris = confusion_matrix(y_test_iris, y_pred_iris)\ncm_iris_df = pd.DataFrame(cm_iris,\n                          columns=[f'Pred: {name}' for name in iris.target_names],\n                          index=[f'Real: {name}' for name in iris.target_names])\nprint(cm_iris_df)\n\n======================================================================\nEVALUACI√ìN MULTICLASE (Dataset Iris - 3 clases)\n======================================================================\n\nReporte de Clasificaci√≥n:\n              precision    recall  f1-score   support\n\n      setosa      1.000     1.000     1.000        15\n  versicolor      0.875     0.933     0.903        15\n   virginica      0.929     0.867     0.897        15\n\n    accuracy                          0.933        45\n   macro avg      0.935     0.933     0.933        45\nweighted avg      0.935     0.933     0.933        45\n\n\nMatriz de Confusi√≥n:\n                  Pred: setosa  Pred: versicolor  Pred: virginica\nReal: setosa                15                 0                0\nReal: versicolor             0                14                1\nReal: virginica              0                 2               13\n\n\n\n\n\n\n\nMatriz de Confusi√≥n para Clasificaci√≥n Multiclase\n\n\n\n\n\n\n\nConsideraciones Finales\nPrincipios clave para la evaluaci√≥n de modelos de clasificaci√≥n:\n\nNunca conf√≠es solo en accuracy - Especialmente con clases desbalanceadas\nAnaliza la matriz de confusi√≥n - Comprende qu√© tipos de errores comete tu modelo\nSelecciona m√©tricas seg√∫n el contexto - Considera el costo relativo de FP vs FN\nUsa m√∫ltiples m√©tricas - Una sola m√©trica raramente cuenta toda la historia\nEval√∫a en datos independientes - Usa conjuntos de validaci√≥n/prueba no vistos\nConsidera el umbral de decisi√≥n - El umbral 0.5 no siempre es √≥ptimo\nVisualiza el desempe√±o - Curvas ROC y Precision-Recall proveen informaci√≥n valiosa\n\nLa evaluaci√≥n rigurosa del desempe√±o es fundamental para construir modelos de clasificaci√≥n confiables y tomar decisiones informadas sobre su uso en aplicaciones del mundo real.\n\n\n\n\n\n\nüí¨ Discusi√≥n en Parejas (15 minutos)\n\n\n\nEscenario de An√°lisis:\nTu equipo ha desarrollado un modelo de clasificaci√≥n para una aplicaci√≥n real. Ahora deben presentar los resultados al equipo de negocio y justificar la elecci√≥n de m√©tricas.\nInstrucciones:\n\nFormen parejas y lean el siguiente escenario asignado por el instructor (o elijan uno):\nEscenario A: Sistema de Detecci√≥n de Fraude con Tarjetas de Cr√©dito\n\nEl modelo predice si una transacci√≥n es fraudulenta (clase positiva) o leg√≠tima (clase negativa)\nSolo el 0.5% de las transacciones son fraudulentas (alta desbalance)\nCostos: Bloquear una transacci√≥n leg√≠tima = $5 de molestia al cliente; No detectar un fraude = $150 de p√©rdida\n\nEscenario B: Sistema de Recomendaci√≥n de Contenido Premium\n\nEl modelo predice si un usuario comprar√° contenido premium (clase positiva) o no (clase negativa)\n20% de los usuarios compran contenido premium\nCostos: Mostrar anuncio a usuario no interesado = $0.50 de recursos desperdiciados; No mostrar anuncio a comprador potencial = $15 de venta perdida\n\nEscenario C: Sistema de Diagn√≥stico M√©dico de Enfermedad Rara\n\nEl modelo predice si un paciente tiene una enfermedad rara (clase positiva) o est√° sano (clase negativa)\nSolo el 2% de los pacientes tienen la enfermedad\nCostos: Falso positivo = $500 en pruebas adicionales + ansiedad; Falso negativo = enfermedad no tratada (alto riesgo)\n\nDiscutan en pareja (10 minutos):\n\nAn√°lisis de costos:\n\n¬øQu√© tipo de error es m√°s costoso: FP o FN?\n¬øPor qu√© el accuracy no es una buena m√©trica para este problema?\n\nSelecci√≥n de m√©tricas:\n\n¬øQu√© m√©trica(s) deber√≠an reportar al equipo de negocio?\n¬øUsar√≠an el umbral est√°ndar de 0.5 o lo ajustar√≠an? ¬øHacia d√≥nde y por qu√©?\n\nInterpretaci√≥n de resultados:\n\nSi el modelo tiene Precision = 0.85 y Recall = 0.60, ¬øes aceptable para este caso?\n¬øQu√© cambios har√≠an para mejorar el modelo seg√∫n las prioridades del negocio?\n\n\nPreparen (2 minutos):\n\nUna conclusi√≥n de 1-2 frases sobre la m√©trica m√°s importante para su escenario\nUn argumento breve de por qu√© eligieron esa m√©trica\n\nCompartan con otra pareja o con la clase (3 minutos):\n\nPresenten su razonamiento\nEscuchen el an√°lisis de otro escenario\n\n\nPreguntas gu√≠a para profundizar:\n\n¬øC√≥mo cambiar√≠a su respuesta si los costos fueran diferentes?\n¬øQu√© informaci√≥n adicional necesitar√≠an del negocio para tomar una mejor decisi√≥n?\n¬øC√≥mo comunicar√≠an las limitaciones del modelo a stakeholders no t√©cnicos?",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  },
  {
    "objectID": "05-arboles.html",
    "href": "05-arboles.html",
    "title": "√Årboles de Decisi√≥n",
    "section": "",
    "text": "Introducci√≥n\nLos √°rboles de decisi√≥n son uno de los m√©todos m√°s intuitivos y ampliamente utilizados en el aprendizaje supervisado. A diferencia de los m√©todos lineales como la regresi√≥n log√≠stica, los √°rboles pueden capturar relaciones no lineales complejas e interacciones entre variables de forma natural, produciendo modelos que son f√°ciles de interpretar y visualizar.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>√Årboles de Decisi√≥n</span>"
    ]
  },
  {
    "objectID": "05-arboles.html#introducci√≥n",
    "href": "05-arboles.html#introducci√≥n",
    "title": "√Årboles de Decisi√≥n",
    "section": "",
    "text": "Motivaci√≥n: Limitaciones de los M√©todos Lineales\nConsideremos un problema donde queremos predecir si un cliente comprar√° un producto bas√°ndonos en su edad y su ingreso. Los m√©todos lineales (como regresi√≥n log√≠stica) asumir√≠an que existe una frontera de decisi√≥n lineal:\n\\[\\beta_0 + \\beta_1 \\cdot \\text{edad} + \\beta_2 \\cdot \\text{ingreso} = 0\\]\nSin embargo, la realidad puede ser m√°s compleja: tal vez los clientes j√≥venes con ingresos altos compran, los clientes mayores con cualquier ingreso compran, pero los clientes j√≥venes con ingresos bajos no compran. Esta regla no es lineal y involucra interacciones entre variables.\nLos √°rboles de decisi√≥n resuelven este problema al particionar el espacio de caracter√≠sticas en regiones rectangulares, donde cada regi√≥n tiene su propia predicci√≥n.\n\n\nEstructura de un √Årbol de Decisi√≥n\nUn √°rbol de decisi√≥n es una estructura jer√°rquica compuesta por:\n\nNodo ra√≠z (root node): Contiene todos los datos de entrenamiento\nNodos internos (internal nodes): Representan decisiones basadas en caracter√≠sticas\nRamas (branches): Representan el resultado de una decisi√≥n\nNodos hoja o terminales (leaf nodes): Contienen las predicciones finales\n\nCada nodo interno realiza una pregunta binaria sobre una caracter√≠stica:\n\n‚Äú¬øEdad ‚â§ 30?‚Äù\n‚Äú¬øIngreso &gt; $50,000?‚Äù\n‚Äú¬øCategor√≠a = A o B?‚Äù\n\n\n\nEjemplo Visual Simple\n                    [Edad ‚â§ 30?]\n                    /           \\\n                  S√≠             No\n                 /                 \\\n        [Ingreso ‚â§ 40K?]        Compra = S√≠\n           /         \\\n         S√≠          No\n        /             \\\n   Compra = No    Compra = S√≠\nEste √°rbol representa las siguientes reglas:\n\nSi edad &gt; 30 ‚Üí Compra = S√≠\nSi edad ‚â§ 30 y ingreso &gt; 40K ‚Üí Compra = S√≠\nSi edad ‚â§ 30 y ingreso ‚â§ 40K ‚Üí Compra = No",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>√Årboles de Decisi√≥n</span>"
    ]
  },
  {
    "objectID": "05-arboles.html#construcci√≥n-de-√°rboles-de-decisi√≥n",
    "href": "05-arboles.html#construcci√≥n-de-√°rboles-de-decisi√≥n",
    "title": "√Årboles de Decisi√≥n",
    "section": "Construcci√≥n de √Årboles de Decisi√≥n",
    "text": "Construcci√≥n de √Årboles de Decisi√≥n\n\nParticionamiento Recursivo del Espacio\nLos √°rboles de decisi√≥n construyen su estructura mediante particionamiento recursivo binario (recursive binary splitting). Este proceso:\n\nComienza con todos los datos en el nodo ra√≠z\nEncuentra la mejor divisi√≥n (variable y punto de corte)\nDivide los datos en dos nodos hijos\nRepite el proceso recursivamente para cada nodo hijo\nSe detiene cuando se cumple un criterio de parada\n\nMatem√°ticamente, el espacio de caracter√≠sticas \\(\\mathbb{R}^p\\) se divide en \\(M\\) regiones disjuntas \\(R_1, R_2, ..., R_M\\) tales que:\n\\[\\bigcup_{m=1}^{M} R_m = \\mathbb{R}^p, \\quad R_i \\cap R_j = \\emptyset \\text{ para } i \\neq j\\]\nCada regi√≥n \\(R_m\\) es un hiperrect√°ngulo paralelo a los ejes de coordenadas.\n\n\nCriterios de Impureza\nPara decidir c√≥mo dividir un nodo, necesitamos medir la impureza o heterogeneidad de un nodo. Un nodo es ‚Äúpuro‚Äù si contiene mayormente ejemplos de una sola clase.\n\n1. √çndice de Gini\nEl √≠ndice de Gini mide la probabilidad de clasificar incorrectamente un elemento elegido aleatoriamente si se etiqueta aleatoriamente seg√∫n la distribuci√≥n de clases del nodo:\n\\[I_G(t) = \\sum_{k=1}^{K} p_k(t) \\cdot (1 - p_k(t)) = \\sum_{k=1}^{K} p_k(t) - \\sum_{k=1}^{K} p_k(t)^2 = 1 - \\sum_{k=1}^{K} p_k(t)^2\\]\nDonde:\n\n\\(K\\) es el n√∫mero de clases\n\\(p_k(t)\\) es la proporci√≥n de ejemplos de la clase \\(k\\) en el nodo \\(t\\)\n\nPropiedades del √≠ndice de Gini:\n\nM√≠nimo (\\(I_G = 0\\)): Nodo puro (una sola clase)\n\nEjemplo: Si \\(p_1 = 1, p_2 = 0\\) ‚Üí \\(I_G = 1 - (1^2 + 0^2) = 0\\)\n\nM√°ximo (cuando las clases est√°n balanceadas):\n\nPara 2 clases con \\(p_1 = p_2 = 0.5\\) ‚Üí \\(I_G = 1 - (0.5^2 + 0.5^2) = 0.5\\)\nPara \\(K\\) clases con \\(p_k = 1/K\\) ‚Üí \\(I_G = 1 - K(1/K)^2 = (K-1)/K\\)\n\n\n\n\n2. Entrop√≠a\nLa entrop√≠a mide el desorden o incertidumbre en un nodo, basada en la teor√≠a de la informaci√≥n:\n\\[H(t) = -\\sum_{k=1}^{K} p_k(t) \\log_2(p_k(t))\\]\nPor convenci√≥n, \\(0 \\log(0) = 0\\).\nPropiedades de la entrop√≠a:\n\nM√≠nimo (\\(H = 0\\)): Nodo puro (certidumbre completa)\nM√°ximo (\\(H = \\log_2(K)\\)): Clases uniformemente distribuidas (m√°xima incertidumbre)\n\nPara 2 clases: \\(H_{\\max} = 1\\) bit\nPara 4 clases: \\(H_{\\max} = 2\\) bits\n\n\nGanancia de Informaci√≥n (Information Gain):\nLa ganancia de informaci√≥n mide la reducci√≥n en entrop√≠a al realizar una divisi√≥n:\n\\[IG = H(t_{\\text{padre}}) - \\sum_{i \\in \\{\\text{izq, der}\\}} \\frac{n_i}{n} H(t_i)\\]\nDonde \\(n_i\\) es el n√∫mero de ejemplos en el nodo hijo \\(i\\) y \\(n\\) es el total en el nodo padre.\n\n\n3. Error de Clasificaci√≥n\nEl error de clasificaci√≥n es la tasa de ejemplos que no pertenecen a la clase mayoritaria:\n\\[E(t) = 1 - \\max_k p_k(t)\\]\nEste criterio es menos sensible a cambios en la distribuci√≥n de clases y se usa menos en la pr√°ctica.\n\n\n\nComparaci√≥n Visual de Criterios de Impureza\n\n\n\n\n\nComparaci√≥n de criterios de impureza para clasificaci√≥n binaria\n\n\n\n\nValores de impureza en puntos clave:\n============================================================\nProporci√≥n p    Gini         Entrop√≠a     Error       \n------------------------------------------------------------\n0.0             0.0000       0.0000       0.0000      \n0.1             0.1800       0.4690       0.1000      \n0.3             0.4200       0.8813       0.3000      \n0.5             0.5000       1.0000       0.5000      \n0.7             0.4200       0.8813       0.3000      \n0.9             0.1800       0.4690       0.1000      \n1.0             0.0000       0.0000       0.0000      \n\n\nObservaciones:\n\nGini y Entrop√≠a son muy similares en comportamiento y suelen dar resultados comparables\nError de clasificaci√≥n es menos sensible a cambios en las probabilidades\nEn la pr√°ctica, Gini es m√°s com√∫n por ser m√°s eficiente computacionalmente\nTodas alcanzan su m√°ximo cuando las clases est√°n balanceadas (\\(p = 0.5\\))\n\n\n\nAlgoritmo de Construcci√≥n CART\nEl algoritmo CART (Classification And Regression Trees) es el m√©todo m√°s com√∫n para construir √°rboles de decisi√≥n:\nAlgoritmo: Construcci√≥n Greedy de √Årbol de Decisi√≥n\nfunci√≥n CONSTRUIR_ARBOL(datos, profundidad_actual, max_profundidad):\n    // Criterios de parada\n    si profundidad_actual &gt;= max_profundidad O\n       nodo es puro O\n       n√∫mero de muestras &lt; min_muestras:\n        crear nodo hoja con predicci√≥n mayoritaria\n        retornar\n\n    // Encontrar mejor divisi√≥n\n    mejor_ganancia = -infinito\n\n    para cada caracter√≠stica j en {1, ..., p}:\n        para cada posible punto de corte c:\n            dividir datos en: {x_j ‚â§ c} y {x_j &gt; c}\n            calcular impureza ponderada de los nodos hijos\n            calcular ganancia = impureza_padre - impureza_hijos\n\n            si ganancia &gt; mejor_ganancia:\n                mejor_ganancia = ganancia\n                mejor_caracter√≠stica = j\n                mejor_corte = c\n\n    // Crear divisi√≥n\n    crear nodo interno con pregunta: \"x[mejor_caracter√≠stica] ‚â§ mejor_corte?\"\n    datos_izq = datos donde x[mejor_caracter√≠stica] ‚â§ mejor_corte\n    datos_der = datos donde x[mejor_caracter√≠stica] &gt; mejor_corte\n\n    // Recursi√≥n\n    hijo_izquierdo = CONSTRUIR_ARBOL(datos_izq, profundidad_actual + 1, max_profundidad)\n    hijo_derecho = CONSTRUIR_ARBOL(datos_der, profundidad_actual + 1, max_profundidad)\n\n    retornar nodo_actual\nCaracter√≠sticas clave del algoritmo:\n\nGreedy (Voraz): En cada paso, elige la mejor divisi√≥n local sin considerar divisiones futuras\nTop-down: Construye desde la ra√≠z hacia las hojas\nRecursivo: Aplica el mismo proceso a cada sub√°rbol\nBinario: Cada divisi√≥n genera exactamente dos nodos hijos\n\n\n\nEjemplo: Construcci√≥n Paso a Paso\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n\n# Generar datos sint√©ticos simples (2D para visualizaci√≥n)\nnp.random.seed(42)\nX, y = make_classification(\n    n_samples=200,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    flip_y=0.1,\n    class_sep=1.5,\n    random_state=42\n)\n\n# Crear DataFrame para mejor visualizaci√≥n\ndf = pd.DataFrame(X, columns=['X1', 'X2'])\ndf['Clase'] = y\n\nprint(\"Datos de ejemplo:\")\nprint(\"=\" * 60)\nprint(df.head(10))\nprint(f\"\\nTotal de muestras: {len(df)}\")\nprint(f\"Clases: {df['Clase'].value_counts().to_dict()}\")\n\nDatos de ejemplo:\n============================================================\n         X1        X2  Clase\n0  1.122201 -3.621909      0\n1  2.055968  3.471449      1\n2  1.626547 -0.708767      0\n3  2.238265  2.357568      1\n4  1.010960  2.377681      1\n5  0.095620  2.794548      1\n6  0.700506  1.005135      1\n7  1.873085  2.558868      1\n8  1.076216  1.470596      1\n9  2.176681  0.741384      1\n\nTotal de muestras: 200\nClases: {1: 101, 0: 99}\n\n\n\nfrom sklearn.tree import plot_tree\n\n# Entrenar √°rboles con diferentes profundidades\nprofundidades = [1, 2, 3, 5]\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.ravel()\n\nfor idx, depth in enumerate(profundidades):\n    # Entrenar √°rbol\n    tree = DecisionTreeClassifier(\n        max_depth=depth,\n        criterion='gini',\n        random_state=42\n    )\n    tree.fit(X, y)\n\n    # Visualizar √°rbol\n    plot_tree(\n        tree,\n        ax=axes[idx],\n        feature_names=['X1', 'X2'],\n        class_names=['Clase 0', 'Clase 1'],\n        filled=True,\n        rounded=True,\n        fontsize=9\n    )\n\n    # Calcular accuracy en entrenamiento\n    train_accuracy = tree.score(X, y)\n    axes[idx].set_title(\n        f'Profundidad = {depth} | Accuracy = {train_accuracy:.3f}',\n        fontsize=12,\n        pad=10\n    )\n\nplt.tight_layout()\nplt.show()\n\n# Mostrar informaci√≥n detallada del √°rbol m√°s complejo\nprint(\"\\n\" + \"=\" * 60)\nprint(\"INFORMACI√ìN DEL √ÅRBOL (Profundidad = 5)\")\nprint(\"=\" * 60)\ntree_detailed = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_detailed.fit(X, y)\nprint(f\"N√∫mero de nodos: {tree_detailed.tree_.node_count}\")\nprint(f\"N√∫mero de hojas: {tree_detailed.get_n_leaves()}\")\nprint(f\"Profundidad real: {tree_detailed.get_depth()}\")\nprint(f\"Accuracy en entrenamiento: {tree_detailed.score(X, y):.3f}\")\n\n\n\n\nComparaci√≥n de √°rboles con diferentes profundidades\n\n\n\n\n\n============================================================\nINFORMACI√ìN DEL √ÅRBOL (Profundidad = 5)\n============================================================\nN√∫mero de nodos: 41\nN√∫mero de hojas: 21\nProfundidad real: 5\nAccuracy en entrenamiento: 0.950\n\n\n\n\n\n\n\nFronteras de decisi√≥n para diferentes profundidades de √°rbol\n\n\n\n\nObservaciones importantes:\n\nProfundidad = 1 (stump): Una sola divisi√≥n, frontera muy simple\nProfundidad = 2-3: Capturas las principales regiones de decisi√≥n\nProfundidad = 5: Frontera muy compleja, posible sobreajuste\nLas fronteras son siempre paralelas a los ejes (particiones rectangulares)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>√Årboles de Decisi√≥n</span>"
    ]
  },
  {
    "objectID": "05-arboles.html#sobreajuste-y-control-de-complejidad",
    "href": "05-arboles.html#sobreajuste-y-control-de-complejidad",
    "title": "√Årboles de Decisi√≥n",
    "section": "Sobreajuste y Control de Complejidad",
    "text": "Sobreajuste y Control de Complejidad\n\nEl Problema del Sobreajuste\nLos √°rboles de decisi√≥n tienen una tendencia natural al sobreajuste (overfitting). Sin restricciones, un √°rbol puede crecer hasta que cada nodo hoja contenga un solo ejemplo, logrando 100% de accuracy en entrenamiento pero generalizando muy mal.\nCausas del sobreajuste:\n\nAlta varianza: Peque√±os cambios en los datos pueden producir √°rboles muy diferentes\nFalta de regularizaci√≥n inherente: Sin restricciones, el √°rbol memoriza los datos\nCaptura de ruido: El √°rbol aprende patrones espec√≠ficos del conjunto de entrenamiento\n\n\n\nEstrategias de Control de Complejidad\n\n1. Pre-Poda (Pre-Pruning)\nLa pre-poda detiene el crecimiento del √°rbol durante su construcci√≥n mediante criterios:\nHiperpar√°metros comunes:\n\nmax_depth: Profundidad m√°xima del √°rbol\n\nValores t√≠picos: 3-10\nMenor ‚Üí M√°s sesgo, menos varianza\n\nmin_samples_split: M√≠nimo de muestras para dividir un nodo\n\nValores t√≠picos: 2-20\nMayor ‚Üí √Årbol m√°s peque√±o\n\nmin_samples_leaf: M√≠nimo de muestras en una hoja\n\nValores t√≠picos: 1-10\nMayor ‚Üí Hojas m√°s confiables\n\nmax_features: N√∫mero m√°ximo de caracter√≠sticas a considerar por divisi√≥n\n\n'sqrt': ‚àöp caracter√≠sticas (usado en Random Forest)\n'log2': log‚ÇÇ(p) caracter√≠sticas\nNone: Todas las caracter√≠sticas\n\nmax_leaf_nodes: N√∫mero m√°ximo de nodos hoja\n\nControla directamente el tama√±o del √°rbol\n\n\n\n\n2. Post-Poda (Post-Pruning)\nLa post-poda construye un √°rbol completo y luego lo reduce eliminando nodos que no aportan suficiente mejora.\nCost-Complexity Pruning (Poda por Costo-Complejidad):\nDefine una funci√≥n de costo que balancea error y complejidad:\n\\[C_\\alpha(T) = \\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} L(y_i, \\hat{y}_m) + \\alpha |T|\\]\nDonde:\n\n\\(|T|\\) es el n√∫mero de nodos hoja\n\\(\\alpha \\geq 0\\) es el par√°metro de complejidad\n\\(L\\) es la funci√≥n de p√©rdida\n\\(\\hat{y}_m\\) es la predicci√≥n en el nodo hoja \\(m\\)\n\nEfecto de \\(\\alpha\\):\n\n\\(\\alpha = 0\\): √Årbol completo (sin poda)\n\\(\\alpha\\) grande: √Årbol muy peque√±o (mayor regularizaci√≥n)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Entrenar √°rbol completo\ntree_full = DecisionTreeClassifier(random_state=42)\ntree_full.fit(X_train, y_train)\n\n# Obtener camino de cost-complexity pruning\npath = tree_full.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas = path.ccp_alphas\nimpurities = path.impurities\n\nprint(\"Cost-Complexity Pruning Path:\")\nprint(\"=\" * 60)\nprint(f\"N√∫mero de valores de alpha: {len(ccp_alphas)}\")\nprint(f\"Rango de alpha: [{ccp_alphas[0]:.6f}, {ccp_alphas[-1]:.6f}]\")\n\n# Entrenar √°rboles para diferentes valores de alpha\ntrain_scores = []\ntest_scores = []\nn_leaves = []\ndepths = []\n\nfor alpha in ccp_alphas:\n    tree = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n    tree.fit(X_train, y_train)\n    train_scores.append(tree.score(X_train, y_train))\n    test_scores.append(tree.score(X_test, y_test))\n    n_leaves.append(tree.get_n_leaves())\n    depths.append(tree.get_depth())\n\n# Visualizaci√≥n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Panel 1: Accuracy vs Alpha\naxes[0].plot(ccp_alphas, train_scores, label='Entrenamiento',\n             marker='o', linewidth=2)\naxes[0].plot(ccp_alphas, test_scores, label='Prueba',\n             marker='s', linewidth=2)\naxes[0].set_xlabel('Alpha (ccp_alpha)', fontsize=11)\naxes[0].set_ylabel('Accuracy', fontsize=11)\naxes[0].set_title('Accuracy vs Alpha', fontsize=12)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Encontrar mejor alpha\nbest_idx = np.argmax(test_scores)\nbest_alpha = ccp_alphas[best_idx]\naxes[0].axvline(x=best_alpha, color='red', linestyle='--',\n                label=f'Mejor Œ± = {best_alpha:.4f}')\n\n# Panel 2: N√∫mero de hojas vs Alpha\naxes[1].plot(ccp_alphas, n_leaves, marker='o', linewidth=2, color='green')\naxes[1].set_xlabel('Alpha (ccp_alpha)', fontsize=11)\naxes[1].set_ylabel('N√∫mero de Hojas', fontsize=11)\naxes[1].set_title('Complejidad del √Årbol vs Alpha', fontsize=12)\naxes[1].grid(True, alpha=0.3)\naxes[1].axvline(x=best_alpha, color='red', linestyle='--')\n\n# Panel 3: Profundidad vs Alpha\naxes[2].plot(ccp_alphas, depths, marker='o', linewidth=2, color='purple')\naxes[2].set_xlabel('Alpha (ccp_alpha)', fontsize=11)\naxes[2].set_ylabel('Profundidad del √Årbol', fontsize=11)\naxes[2].set_title('Profundidad vs Alpha', fontsize=12)\naxes[2].grid(True, alpha=0.3)\naxes[2].axvline(x=best_alpha, color='red', linestyle='--')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n{'='*60}\")\nprint(\"COMPARACI√ìN: √Årbol sin poda vs √Årbol podado\")\nprint(\"=\"*60)\nprint(f\"\\n√Årbol sin poda (Œ± = 0):\")\nprint(f\"  Hojas: {n_leaves[0]}\")\nprint(f\"  Profundidad: {depths[0]}\")\nprint(f\"  Accuracy entrenamiento: {train_scores[0]:.3f}\")\nprint(f\"  Accuracy prueba: {test_scores[0]:.3f}\")\n\nprint(f\"\\n√Årbol podado √≥ptimo (Œ± = {best_alpha:.4f}):\")\nprint(f\"  Hojas: {n_leaves[best_idx]}\")\nprint(f\"  Profundidad: {depths[best_idx]}\")\nprint(f\"  Accuracy entrenamiento: {train_scores[best_idx]:.3f}\")\nprint(f\"  Accuracy prueba: {test_scores[best_idx]:.3f}\")\n\nCost-Complexity Pruning Path:\n============================================================\nN√∫mero de valores de alpha: 13\nRango de alpha: [0.000000, 0.309700]\n\n\n\n\n\nEfecto de la poda en el desempe√±o del √°rbol\n\n\n\n\n\n============================================================\nCOMPARACI√ìN: √Årbol sin poda vs √Årbol podado\n============================================================\n\n√Årbol sin poda (Œ± = 0):\n  Hojas: 24\n  Profundidad: 10\n  Accuracy entrenamiento: 1.000\n  Accuracy prueba: 0.850\n\n√Årbol podado √≥ptimo (Œ± = 0.0129):\n  Hojas: 4\n  Profundidad: 3\n  Accuracy entrenamiento: 0.907\n  Accuracy prueba: 0.883",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>√Årboles de Decisi√≥n</span>"
    ]
  },
  {
    "objectID": "05-arboles.html#interpretabilidad-y-an√°lisis",
    "href": "05-arboles.html#interpretabilidad-y-an√°lisis",
    "title": "√Årboles de Decisi√≥n",
    "section": "Interpretabilidad y An√°lisis",
    "text": "Interpretabilidad y An√°lisis\n\nImportancia de Variables\nUna de las grandes ventajas de los √°rboles es que podemos medir la importancia de cada variable bas√°ndonos en cu√°nto reduce la impureza:\n\\[\\text{Importancia}(X_j) = \\sum_{t: \\text{usa } X_j} \\frac{n_t}{n} \\cdot \\Delta I(t)\\]\nDonde: - \\(n_t\\) es el n√∫mero de muestras en el nodo \\(t\\) - \\(n\\) es el n√∫mero total de muestras - \\(\\Delta I(t)\\) es la reducci√≥n en impureza por la divisi√≥n en el nodo \\(t\\)\n\n# Entrenar √°rbol en dataset con m√°s caracter√≠sticas\nfrom sklearn.datasets import make_classification\n\n# Generar datos con 10 caracter√≠sticas\nX_multi, y_multi = make_classification(\n    n_samples=500,\n    n_features=10,\n    n_informative=6,\n    n_redundant=2,\n    n_repeated=0,\n    random_state=42\n)\n\n# Nombres de caracter√≠sticas\nfeature_names = [f'X{i+1}' for i in range(10)]\n\n# Entrenar √°rbol\ntree_multi = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_multi.fit(X_multi, y_multi)\n\n# Obtener importancias\nimportances = tree_multi.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Visualizaci√≥n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Panel 1: Gr√°fico de barras\naxes[0].barh(range(10), importances[indices], color='steelblue', alpha=0.7)\naxes[0].set_yticks(range(10))\naxes[0].set_yticklabels([feature_names[i] for i in indices])\naxes[0].set_xlabel('Importancia', fontsize=11)\naxes[0].set_title('Importancia de Variables (Reducci√≥n de Impureza)', fontsize=12)\naxes[0].grid(True, alpha=0.3, axis='x')\n\n# A√±adir valores\nfor i, (idx, imp) in enumerate(zip(indices, importances[indices])):\n    axes[0].text(imp + 0.005, i, f'{imp:.3f}', va='center', fontsize=9)\n\n# Panel 2: Importancia acumulada\ncumsum_importance = np.cumsum(importances[indices])\naxes[1].plot(range(1, 11), cumsum_importance, marker='o', linewidth=2.5,\n             markersize=8, color='darkgreen')\naxes[1].fill_between(range(1, 11), cumsum_importance, alpha=0.3, color='green')\naxes[1].axhline(y=0.8, color='red', linestyle='--', linewidth=1.5,\n                label='80% de importancia')\naxes[1].axhline(y=0.95, color='orange', linestyle='--', linewidth=1.5,\n                label='95% de importancia')\naxes[1].set_xlabel('N√∫mero de Variables', fontsize=11)\naxes[1].set_ylabel('Importancia Acumulada', fontsize=11)\naxes[1].set_title('Importancia Acumulada de Variables', fontsize=12)\naxes[1].set_xticks(range(1, 11))\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Imprimir tabla de importancias\nprint(\"\\nTabla de Importancias:\")\nprint(\"=\" * 60)\nprint(f\"{'Variable':&lt;12} {'Importancia':&lt;15} {'Importancia Acum.':&lt;20}\")\nprint(\"-\" * 60)\ncumsum = 0\nfor idx in indices:\n    cumsum += importances[idx]\n    print(f\"{feature_names[idx]:&lt;12} {importances[idx]:&lt;15.4f} {cumsum:&lt;20.4f}\")\n\n\n\n\nImportancia de variables en √°rbol de decisi√≥n\n\n\n\n\n\nTabla de Importancias:\n============================================================\nVariable     Importancia     Importancia Acum.   \n------------------------------------------------------------\nX1           0.3599          0.3599              \nX6           0.1732          0.5331              \nX8           0.1130          0.6462              \nX2           0.0921          0.7383              \nX5           0.0771          0.8154              \nX10          0.0768          0.8921              \nX9           0.0514          0.9436              \nX4           0.0385          0.9821              \nX7           0.0134          0.9955              \nX3           0.0045          1.0000              \n\n\n\n\nExtracci√≥n de Reglas\nLos √°rboles pueden convertirse en reglas IF-THEN interpretables:\n\nfrom sklearn.tree import export_text\n\n# Entrenar √°rbol simple para mejor interpretabilidad\ntree_simple = DecisionTreeClassifier(max_depth=3, min_samples_leaf=10, random_state=42)\ntree_simple.fit(X[:, :2], y)\n\n# Exportar reglas como texto\ntree_rules = export_text(tree_simple, feature_names=['X1', 'X2'])\n\nprint(\"REGLAS DE DECISI√ìN DEL √ÅRBOL:\")\nprint(\"=\" * 60)\nprint(tree_rules)\n\n# Funci√≥n para extraer rutas de decisi√≥n\ndef get_decision_path(tree, feature_names, sample):\n    \"\"\"Extrae la ruta de decisi√≥n para una muestra\"\"\"\n    node = 0\n    path = []\n\n    while tree.tree_.feature[node] != -2:  # -2 indica nodo hoja\n        feature_idx = tree.tree_.feature[node]\n        threshold = tree.tree_.threshold[node]\n\n        if sample[feature_idx] &lt;= threshold:\n            direction = \"&lt;=\"\n            node = tree.tree_.children_left[node]\n        else:\n            direction = \"&gt;\"\n            node = tree.tree_.children_right[node]\n\n        path.append(f\"{feature_names[feature_idx]} {direction} {threshold:.3f}\")\n\n    # Obtener predicci√≥n\n    class_probs = tree.tree_.value[node][0]\n    predicted_class = np.argmax(class_probs)\n\n    return path, predicted_class, class_probs\n\n# Ejemplo: explicar predicci√≥n para algunas muestras\nprint(\"\\n\" + \"=\" * 60)\nprint(\"EXPLICACI√ìN DE PREDICCIONES\")\nprint(\"=\" * 60)\n\nfor i in range(3):\n    sample = X[i, :2]\n    path, pred_class, probs = get_decision_path(tree_simple, ['X1', 'X2'], sample)\n\n    print(f\"\\nMuestra {i+1}: X1={sample[0]:.3f}, X2={sample[1]:.3f}\")\n    print(f\"Clase real: {y[i]}\")\n    print(f\"Predicci√≥n: {pred_class}\")\n    print(f\"Probabilidades: Clase 0 = {probs[0]:.3f}, Clase 1 = {probs[1]:.3f}\")\n    print(\"Ruta de decisi√≥n:\")\n    for step in path:\n        print(f\"  ‚Üí {step}\")\n\nREGLAS DE DECISI√ìN DEL √ÅRBOL:\n============================================================\n|--- X2 &lt;= 0.31\n|   |--- X1 &lt;= 1.07\n|   |   |--- X2 &lt;= -0.96\n|   |   |   |--- class: 0\n|   |   |--- X2 &gt;  -0.96\n|   |   |   |--- class: 0\n|   |--- X1 &gt;  1.07\n|   |   |--- X2 &lt;= -2.17\n|   |   |   |--- class: 0\n|   |   |--- X2 &gt;  -2.17\n|   |   |   |--- class: 0\n|--- X2 &gt;  0.31\n|   |--- X1 &lt;= 1.07\n|   |   |--- X2 &lt;= 1.20\n|   |   |   |--- class: 1\n|   |   |--- X2 &gt;  1.20\n|   |   |   |--- class: 1\n|   |--- X1 &gt;  1.07\n|   |   |--- X2 &lt;= 0.82\n|   |   |   |--- class: 1\n|   |   |--- X2 &gt;  0.82\n|   |   |   |--- class: 1\n\n\n============================================================\nEXPLICACI√ìN DE PREDICCIONES\n============================================================\n\nMuestra 1: X1=1.122, X2=-3.622\nClase real: 0\nPredicci√≥n: 0\nProbabilidades: Clase 0 = 0.800, Clase 1 = 0.200\nRuta de decisi√≥n:\n  ‚Üí X2 &lt;= 0.315\n  ‚Üí X1 &gt; 1.072\n  ‚Üí X2 &lt;= -2.166\n\nMuestra 2: X1=2.056, X2=3.471\nClase real: 1\nPredicci√≥n: 1\nProbabilidades: Clase 0 = 0.018, Clase 1 = 0.982\nRuta de decisi√≥n:\n  ‚Üí X2 &gt; 0.315\n  ‚Üí X1 &gt; 1.066\n  ‚Üí X2 &gt; 0.821\n\nMuestra 3: X1=1.627, X2=-0.709\nClase real: 0\nPredicci√≥n: 0\nProbabilidades: Clase 0 = 0.961, Clase 1 = 0.039\nRuta de decisi√≥n:\n  ‚Üí X2 &lt;= 0.315\n  ‚Üí X1 &gt; 1.072\n  ‚Üí X2 &gt; -2.166",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>√Årboles de Decisi√≥n</span>"
    ]
  },
  {
    "objectID": "05-arboles.html#ventajas-y-desventajas",
    "href": "05-arboles.html#ventajas-y-desventajas",
    "title": "√Årboles de Decisi√≥n",
    "section": "Ventajas y Desventajas",
    "text": "Ventajas y Desventajas\n\nVentajas de los √Årboles de Decisi√≥n\n\nInterpretabilidad: F√°ciles de entender y explicar, incluso para no expertos\n\nSe pueden visualizar completamente\nGeneran reglas IF-THEN interpretables\n\nManejo de variables mixtas: Pueden manejar caracter√≠sticas num√©ricas y categ√≥ricas sin preprocesamiento\nNo requieren normalizaci√≥n: Las decisiones son invariantes a transformaciones mon√≥tonas\nCapturan interacciones autom√°ticamente: Detectan interacciones sin especificarlas expl√≠citamente\nRobustos a outliers: Las divisiones son basadas en rankings, no en valores absolutos\nSelecci√≥n impl√≠cita de caracter√≠sticas: Variables irrelevantes no se usan en las divisiones\n\n\n\nDesventajas de los √Årboles de Decisi√≥n\n\nAlta varianza: Peque√±os cambios en datos ‚Üí √°rboles muy diferentes\n\nSoluci√≥n: M√©todos ensemble (Random Forest, Gradient Boosting)\n\nDificultad con relaciones lineales: Necesitan muchas divisiones para aproximar funciones lineales\nFronteras de decisi√≥n restrictivas: Solo particiones rectangulares paralelas a los ejes\nSesgo hacia variables con muchos valores: Tienden a seleccionar variables con m√°s opciones de corte\nInestabilidad: Peque√±as variaciones pueden cambiar completamente la estructura\nSobreajuste natural: Sin restricciones, memorizan los datos de entrenamiento\n\n\n\nComparaci√≥n Visual: √Årbol vs Regresi√≥n Log√≠stica\n\n\n\n\n\nComparaci√≥n de fronteras de decisi√≥n: √Årbol vs Regresi√≥n Log√≠stica\n\n\n\n\nObservaciones:\n============================================================\n- Regresi√≥n log√≠stica captura mejor la relaci√≥n lineal subyacente\n- √Årbol de decisi√≥n crea fronteras rectangulares que aproximan la l√≠nea\n- Para relaciones lineales, la regresi√≥n log√≠stica es m√°s eficiente\n- Para relaciones no lineales, los √°rboles son m√°s flexibles",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>√Årboles de Decisi√≥n</span>"
    ]
  },
  {
    "objectID": "05-arboles.html#aplicaci√≥n-pr√°ctica-dataset-real",
    "href": "05-arboles.html#aplicaci√≥n-pr√°ctica-dataset-real",
    "title": "√Årboles de Decisi√≥n",
    "section": "Aplicaci√≥n Pr√°ctica: Dataset Real",
    "text": "Aplicaci√≥n Pr√°ctica: Dataset Real\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nimport pandas as pd\n\n# Cargar dataset\ncancer = load_breast_cancer()\nX_cancer = cancer.data\ny_cancer = cancer.target\n\nprint(\"DATASET: Wisconsin Breast Cancer\")\nprint(\"=\" * 60)\nprint(f\"N√∫mero de muestras: {X_cancer.shape[0]}\")\nprint(f\"N√∫mero de caracter√≠sticas: {X_cancer.shape[1]}\")\nprint(f\"Clases: {cancer.target_names}\")\nprint(f\"Distribuci√≥n: {np.bincount(y_cancer)}\")\n\n# Dividir datos\nX_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer\n)\n\n# 1. √Årbol sin regularizaci√≥n\ntree_unreg = DecisionTreeClassifier(random_state=42)\ntree_unreg.fit(X_train_c, y_train_c)\n\nprint(\"\\n1. √ÅRBOL SIN REGULARIZACI√ìN\")\nprint(\"-\" * 60)\nprint(f\"Profundidad: {tree_unreg.get_depth()}\")\nprint(f\"N√∫mero de hojas: {tree_unreg.get_n_leaves()}\")\nprint(f\"Accuracy entrenamiento: {tree_unreg.score(X_train_c, y_train_c):.3f}\")\nprint(f\"Accuracy prueba: {tree_unreg.score(X_test_c, y_test_c):.3f}\")\n\n# 2. B√∫squeda de hiperpar√°metros √≥ptimos\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [2, 5, 10, 20],\n    'min_samples_leaf': [1, 2, 5, 10],\n    'criterion': ['gini', 'entropy']\n}\n\nprint(\"\\n2. B√öSQUEDA DE HIPERPAR√ÅMETROS (Grid Search)\")\nprint(\"-\" * 60)\nprint(\"Evaluando combinaciones de hiperpar√°metros con CV...\")\n\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\ngrid_search.fit(X_train_c, y_train_c)\n\nprint(f\"Mejor combinaci√≥n de par√°metros:\")\nfor param, value in grid_search.best_params_.items():\n    print(f\"  {param}: {value}\")\n\n# 3. Evaluar mejor modelo\nbest_tree = grid_search.best_estimator_\n\nprint(\"\\n3. MEJOR √ÅRBOL (despu√©s de optimizaci√≥n)\")\nprint(\"-\" * 60)\nprint(f\"Profundidad: {best_tree.get_depth()}\")\nprint(f\"N√∫mero de hojas: {best_tree.get_n_leaves()}\")\nprint(f\"Accuracy entrenamiento: {best_tree.score(X_train_c, y_train_c):.3f}\")\nprint(f\"Accuracy prueba: {best_tree.score(X_test_c, y_test_c):.3f}\")\n\n# 4. Validaci√≥n cruzada\ncv_scores = cross_val_score(best_tree, X_train_c, y_train_c, cv=5)\nprint(f\"\\nValidaci√≥n cruzada (5-fold):\")\nprint(f\"  Scores: {cv_scores}\")\nprint(f\"  Media: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n\nDATASET: Wisconsin Breast Cancer\n============================================================\nN√∫mero de muestras: 569\nN√∫mero de caracter√≠sticas: 30\nClases: ['malignant' 'benign']\nDistribuci√≥n: [212 357]\n\n1. √ÅRBOL SIN REGULARIZACI√ìN\n------------------------------------------------------------\nProfundidad: 6\nN√∫mero de hojas: 16\nAccuracy entrenamiento: 1.000\nAccuracy prueba: 0.918\n\n2. B√öSQUEDA DE HIPERPAR√ÅMETROS (Grid Search)\n------------------------------------------------------------\nEvaluando combinaciones de hiperpar√°metros con CV...\nMejor combinaci√≥n de par√°metros:\n  criterion: gini\n  max_depth: 3\n  min_samples_leaf: 2\n  min_samples_split: 2\n\n3. MEJOR √ÅRBOL (despu√©s de optimizaci√≥n)\n------------------------------------------------------------\nProfundidad: 3\nN√∫mero de hojas: 7\nAccuracy entrenamiento: 0.980\nAccuracy prueba: 0.924\n\nValidaci√≥n cruzada (5-fold):\n  Scores: [0.9        0.95       0.9        0.97468354 1.        ]\n  Media: 0.945 (+/- 0.040)\n\n\n\n# Importancia de caracter√≠sticas\nimportances_cancer = best_tree.feature_importances_\nindices_cancer = np.argsort(importances_cancer)[::-1][:10]  # Top 10\n\nplt.figure(figsize=(10, 6))\nplt.barh(range(10), importances_cancer[indices_cancer], color='coral', alpha=0.7)\nplt.yticks(range(10), [cancer.feature_names[i] for i in indices_cancer])\nplt.xlabel('Importancia (Reducci√≥n de Impureza)', fontsize=12)\nplt.title('Top 10 Caracter√≠sticas M√°s Importantes', fontsize=13)\nplt.gca().invert_yaxis()\nplt.grid(True, alpha=0.3, axis='x')\n\n# A√±adir valores\nfor i, imp in enumerate(importances_cancer[indices_cancer]):\n    plt.text(imp + 0.005, i, f'{imp:.3f}', va='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nTop 10 caracter√≠sticas m√°s importantes para clasificar c√°ncer de mama",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>√Årboles de Decisi√≥n</span>"
    ]
  },
  {
    "objectID": "05-arboles.html#conclusiones-y-mejores-pr√°cticas",
    "href": "05-arboles.html#conclusiones-y-mejores-pr√°cticas",
    "title": "√Årboles de Decisi√≥n",
    "section": "Conclusiones y Mejores Pr√°cticas",
    "text": "Conclusiones y Mejores Pr√°cticas\n\nRecomendaciones para Usar √Årboles de Decisi√≥n\n\nComienza simple: Empieza con √°rboles poco profundos (max_depth=3-5)\nUsa validaci√≥n cruzada: Para seleccionar hiperpar√°metros √≥ptimos\nConsidera la interpretabilidad: Si necesitas explicar decisiones, mant√©n √°rboles peque√±os\nCombina con ensemble: Para producci√≥n, considera Random Forest o Gradient Boosting\nAnaliza importancia de variables: Para entender qu√© caracter√≠sticas son relevantes\nVisualiza el √°rbol: Ayuda a detectar problemas y entender el modelo\nCompara con baselines: √Årbol vs regresi√≥n log√≠stica en datos lineales\n\n\n\nCu√°ndo Usar √Årboles de Decisi√≥n\nUsar √°rboles cuando: - Necesitas interpretabilidad - Tienes interacciones complejas entre variables - Variables num√©ricas y categ√≥ricas mezcladas - Outliers en los datos - Recursos computacionales limitados (√°rboles son r√°pidos)\nEvitar √°rboles individuales cuando: - Datos con relaciones predominantemente lineales - Necesitas el mejor desempe√±o predictivo (usar ensemble) - Tienes muy pocos datos (alta varianza) - Variables con muchas categor√≠as (sesgo en selecci√≥n)\n\n\nPr√≥ximos Pasos: M√©todos Ensemble\nLos √°rboles individuales tienen limitaciones, pero combin√°ndolos podemos crear modelos extremadamente poderosos:\n\nBagging: Reduce varianza promediando m√∫ltiples √°rboles\nRandom Forest: Bagging + aleatorizaci√≥n de caracter√≠sticas\nGradient Boosting: Construye √°rboles secuencialmente para corregir errores\nXGBoost, LightGBM, CatBoost: Implementaciones optimizadas de boosting\n\nEstos m√©todos ensemble est√°n entre los algoritmos m√°s efectivos en machine learning y ser√°n tema de cap√≠tulos futuros.\n\nReferencias clave:\n\nBreiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and regression trees. CRC press.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning (2nd ed.). Springer.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning. Springer.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>√Årboles de Decisi√≥n</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and\nJonathan Taylor. 2023. An Introduction to\nStatistical Learning: With Applications in Python. Springer\nTexts in Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Referencias"
    ]
  },
  {
    "objectID": "introduccion.html",
    "href": "introduccion.html",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "",
    "text": "1. Librer√≠as Necesarias\nEste notebook cubre los fundamentos de Python necesarios para miner√≠a de datos, incluyendo:\nPrimero importamos todas las librer√≠as que usaremos en este notebook:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#fundamentos-de-numpy",
    "href": "introduccion.html#fundamentos-de-numpy",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "2. Fundamentos de NumPy",
    "text": "2. Fundamentos de NumPy\nNumPy es la librer√≠a fundamental para computaci√≥n cient√≠fica en Python. Proporciona arrays multidimensionales eficientes.\n\n2.1 Creaci√≥n de Arrays\n\n# Creamos un array de 100 n√∫meros igualmente espaciados entre 0 y 10\nX = np.linspace(0, 10, 100)\nprint(\"Primeros 5 elementos:\", X[:5])\nprint(\"Forma del array:\", X.shape)\nprint(\"√öltimos 10 elementos:\", X[90:])\n\nPrimeros 5 elementos: [0.        0.1010101 0.2020202 0.3030303 0.4040404]\nForma del array: (100,)\n√öltimos 10 elementos: [ 9.09090909  9.19191919  9.29292929  9.39393939  9.49494949  9.5959596\n  9.6969697   9.7979798   9.8989899  10.        ]\n\n\n\n\n2.2 Operaciones con Arrays y Generaci√≥n de Datos\n\n# Acceso a elementos individuales\nprint(\"Elemento en posici√≥n 1:\", X[1])\n\n# Generaci√≥n de datos sint√©ticos para regresi√≥n lineal\n# y = 0.5*x + 10 + ruido_gaussiano\ny_linear = 0.5 * X + 10 + np.random.normal(loc=0, scale=1.5, size=100)\nprint(\"Primeros 5 valores de y:\", y_linear[:5])\n\nElemento en posici√≥n 1: 0.10101010101010101\nPrimeros 5 valores de y: [10.947363    7.72186528 10.63466357 10.19993526 11.22329185]",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#fundamentos-de-pandas",
    "href": "introduccion.html#fundamentos-de-pandas",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "3. Fundamentos de Pandas",
    "text": "3. Fundamentos de Pandas\nPandas es la librer√≠a principal para manipulaci√≥n y an√°lisis de datos estructurados en Python.\n\n3.1 Creaci√≥n de DataFrames\n\n# Creamos un DataFrame con nuestros datos\ndf = pd.DataFrame({\"x\": X, \"y\": y_linear})\nprint(\"Primeras 5 filas del DataFrame:\")\nprint(df.head())\nprint(\"\\nInformaci√≥n del DataFrame:\")\nprint(df.info())\n\nPrimeras 5 filas del DataFrame:\n         x          y\n0  0.00000  10.947363\n1  0.10101   7.721865\n2  0.20202  10.634664\n3  0.30303  10.199935\n4  0.40404  11.223292\n\nInformaci√≥n del DataFrame:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   x       100 non-null    float64\n 1   y       100 non-null    float64\ndtypes: float64(2)\nmemory usage: 1.7 KB\nNone\n\n\n\n\n3.2 Manipulaci√≥n de DataFrames\n\n# Filtrado de datos\nprint(\"Filas donde y &gt; 12:\")\ndf_filtrado = df.query(\"y &gt; 12\")\nprint(df_filtrado)\n\n# Estad√≠sticas descriptivas\nprint(\"\\nCuantiles de la variable y:\")\nprint(df.y.quantile(q=[0.25, 0.5, 0.75, 0.99]))\n\nFilas donde y &gt; 12:\n            x          y\n10   1.010101  12.755558\n17   1.717172  16.315991\n28   2.828283  12.890289\n31   3.131313  12.095512\n33   3.333333  12.186190\n35   3.535354  16.519562\n38   3.838384  14.936178\n39   3.939394  12.445576\n43   4.343434  13.407873\n44   4.444444  14.740559\n46   4.646465  14.085751\n47   4.747475  13.406937\n48   4.848485  12.711032\n49   4.949495  14.302195\n51   5.151515  13.419018\n53   5.353535  14.202517\n54   5.454545  13.460248\n55   5.555556  13.570844\n56   5.656566  15.336198\n58   5.858586  13.248156\n59   5.959596  12.120376\n60   6.060606  13.536254\n61   6.161616  13.877174\n62   6.262626  12.643431\n64   6.464646  13.144207\n65   6.565657  16.558753\n66   6.666667  14.174915\n67   6.767677  13.188301\n68   6.868687  15.327148\n69   6.969697  14.229089\n70   7.070707  14.671629\n71   7.171717  15.430842\n72   7.272727  14.078094\n74   7.474747  14.913045\n75   7.575758  16.341642\n76   7.676768  13.882098\n77   7.777778  14.801524\n78   7.878788  16.116168\n79   7.979798  14.168691\n80   8.080808  12.395476\n81   8.181818  12.295980\n82   8.282828  15.132112\n83   8.383838  12.813598\n84   8.484848  14.631908\n86   8.686869  14.303784\n87   8.787879  13.279560\n88   8.888889  12.344803\n89   8.989899  12.711722\n91   9.191919  16.917684\n92   9.292929  14.830744\n93   9.393939  15.292800\n95   9.595960  12.746624\n96   9.696970  16.000313\n97   9.797980  15.209167\n98   9.898990  14.825473\n99  10.000000  13.841673\n\nCuantiles de la variable y:\n0.25    11.154739\n0.50    12.420526\n0.75    14.181815\n0.99    16.562342\nName: y, dtype: float64\n\n\n\n# Renombramiento de columnas\nprint(\"Renombrando columnas...\")\ndf_renamed = df.rename(columns={\"x\": \"variable_independiente\", \"y\": \"variable_dependiente\"})\nprint(df_renamed.head())\n\n# Volvemos a los nombres originales para el resto del notebook\ndf = df.rename(columns={\"x\": \"x\", \"y\": \"y\"})\n\nRenombrando columnas...\n   variable_independiente  variable_dependiente\n0                 0.00000             10.947363\n1                 0.10101              7.721865\n2                 0.20202             10.634664\n3                 0.30303             10.199935\n4                 0.40404             11.223292",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#iteraci√≥n-en-python",
    "href": "introduccion.html#iteraci√≥n-en-python",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "4. Iteraci√≥n en Python",
    "text": "4. Iteraci√≥n en Python\n\n4.1 Iteraci√≥n sobre Diccionarios\n\n# Ejemplo de diccionario con diferentes tipos de datos\ndiccionario_ejemplo = {\n    \"entero\": 42,\n    \"array_numpy\": np.array([1, 2, 3, 4]),\n    \"cadena\": \"miner√≠a de datos\",\n    \"flotante\": 3.14159\n}\n\n# Iteraci√≥n sobre diccionarios\nprint(\"Iterando sobre el diccionario:\")\nfor clave, valor in diccionario_ejemplo.items():\n    print(f\"Clave: {clave} | Valor: {valor} | Tipo: {type(valor).__name__}\")\n\nIterando sobre el diccionario:\nClave: entero | Valor: 42 | Tipo: int\nClave: array_numpy | Valor: [1 2 3 4] | Tipo: ndarray\nClave: cadena | Valor: miner√≠a de datos | Tipo: str\nClave: flotante | Valor: 3.14159 | Tipo: float\n\n\n\n\n4.2 Iteraci√≥n sobre Listas\n\n# Ejemplo con lista de n√∫meros\nnumeros = [1, 4, 9, 16, 25]\nprint(\"Iterando sobre lista de n√∫meros:\")\nfor i, numero in enumerate(numeros):\n    print(f\"Posici√≥n {i}: {numero}, ra√≠z cuadrada: {np.sqrt(numero):.2f}\")\n    \n# Ejemplo con diferentes valores de k para KNN\nvalores_k = [1, 5, 10, 20, 50]\nprint(\"\\nIterando sobre valores de k:\")\nfor k in valores_k:\n    print(f\"Valor de k: {k}\")\n\nIterando sobre lista de n√∫meros:\nPosici√≥n 0: 1, ra√≠z cuadrada: 1.00\nPosici√≥n 1: 4, ra√≠z cuadrada: 2.00\nPosici√≥n 2: 9, ra√≠z cuadrada: 3.00\nPosici√≥n 3: 16, ra√≠z cuadrada: 4.00\nPosici√≥n 4: 25, ra√≠z cuadrada: 5.00\n\nIterando sobre valores de k:\nValor de k: 1\nValor de k: 5\nValor de k: 10\nValor de k: 20\nValor de k: 50",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#visualizaci√≥n-con-matplotlib-y-seaborn",
    "href": "introduccion.html#visualizaci√≥n-con-matplotlib-y-seaborn",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "5. Visualizaci√≥n con Matplotlib y Seaborn",
    "text": "5. Visualizaci√≥n con Matplotlib y Seaborn\n\n5.1 Gr√°fica de Dispersi√≥n B√°sica\n\n# Gr√°fica de dispersi√≥n b√°sica\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x=\"x\", y=\"y\")\nplt.title(\"Relaci√≥n entre x y y\")\nplt.xlabel(\"Variable independiente (x)\")\nplt.ylabel(\"Variable dependiente (y)\")\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#funciones-con-type-hints",
    "href": "introduccion.html#funciones-con-type-hints",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "6. Funciones con Type Hints",
    "text": "6. Funciones con Type Hints\n\n6.1 Funci√≥n para Generar Datos\n\ndef generar_datos_lineales(\n    n: int = 100,\n    slope: float = 0.5,\n    intercept: float = 1.0,\n    noise_scale: float = 1.5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Genera datos sint√©ticos para regresi√≥n lineal.\n    \n    Par√°metros:\n    -----------\n    n : int\n        N√∫mero de puntos de datos a generar\n    slope : float\n        Pendiente de la relaci√≥n lineal\n    intercept : float\n        Intercepto de la relaci√≥n lineal\n    noise_scale : float\n        Desviaci√≥n est√°ndar del ruido gaussiano\n    \n    Retorna:\n    --------\n    pd.DataFrame\n        DataFrame con columnas 'x' y 'y'\n    \"\"\"\n    X = np.linspace(0, 10, n)\n    y = slope * X + intercept + np.random.normal(scale=noise_scale, size=n)\n    return pd.DataFrame({\"x\": X, \"y\": y})\n\n# Ejemplo de uso\ndf_nuevo = generar_datos_lineales(n=50, slope=2.0, intercept=5.0)\nprint(\"Datos generados:\")\nprint(df_nuevo.head())\n\nDatos generados:\n          x         y\n0  0.000000  6.962633\n1  0.204082  5.261446\n2  0.408163  6.349024\n3  0.612245  6.767730\n4  0.816327  5.768821\n\n\n\n\n6.2 Funci√≥n para Visualizaci√≥n\n\ndef crear_grafica_dispersion(\n    data_frame: pd.DataFrame,\n    x_col: str = \"x\",\n    y_col: str = \"y\",\n    titulo: str = \"Gr√°fica de Dispersi√≥n\",\n    nombre_eje_x: str = \"X\",\n    nombre_eje_y: str = \"Y\"\n) -&gt; None:\n    \"\"\"\n    Crea una gr√°fica de dispersi√≥n con formato personalizado.\n    \n    Par√°metros:\n    -----------\n    data_frame : pd.DataFrame\n        DataFrame que contiene los datos\n    x_col : str\n        Nombre de la columna para el eje x\n    y_col : str\n        Nombre de la columna para el eje y\n    titulo : str\n        T√≠tulo de la gr√°fica\n    nombre_eje_x : str\n        Etiqueta del eje x\n    nombre_eje_y : str\n        Etiqueta del eje y\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.scatterplot(data=data_frame, x=x_col, y=y_col)\n    plt.title(titulo)\n    plt.xlabel(nombre_eje_x)\n    plt.ylabel(nombre_eje_y)\n    plt.show()\n\n# Ejemplo de uso\ncrear_grafica_dispersion(\n    df_nuevo, \n    titulo=\"Datos Sint√©ticos Generados\",\n    nombre_eje_x=\"Variable Independiente\",\n    nombre_eje_y=\"Variable Dependiente\"\n)",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#regresi√≥n-lineal",
    "href": "introduccion.html#regresi√≥n-lineal",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "7. Regresi√≥n Lineal",
    "text": "7. Regresi√≥n Lineal\n\n7.1 Implementaci√≥n y Ajuste del Modelo\n\n# Generamos datos para trabajar\ndf = generar_datos_lineales(n=1000, slope=1.0, intercept=2.0, noise_scale=1.5)\n\n# 1. Crear el modelo de regresi√≥n lineal\nmodelo_lr = LinearRegression()\n\n# 2. Ajustar el modelo (entrenamiento)\n# Nota: sklearn necesita X como matriz (2D) y y como vector (1D)\nX_features = df[[\"x\"]]  # Matriz 2D\ny_target = df[\"y\"]     # Vector 1D\n\nmodelo_lr.fit(X_features, y_target)\n\nprint(f\"Coeficiente (pendiente): {modelo_lr.coef_[0]:.3f}\")\nprint(f\"Intercepto: {modelo_lr.intercept_:.3f}\")\n\nCoeficiente (pendiente): 1.006\nIntercepto: 1.935\n\n\n\n\n7.2 Predicciones y Evaluaci√≥n\n\n# 3. Hacer predicciones\ny_pred_lr = modelo_lr.predict(X_features)\n\n# 4. Evaluar el modelo\nmse_lr = mean_squared_error(y_target, y_pred_lr)\nprint(f\"Error Cuadr√°tico Medio (MSE): {mse_lr:.3f}\")\n\n# Agregar predicciones al DataFrame para visualizaci√≥n\ndf[\"y_pred_lr\"] = y_pred_lr\n\nError Cuadr√°tico Medio (MSE): 2.369\n\n\n\n\n7.3 Visualizaci√≥n del Modelo\n\n# Visualizaci√≥n de datos originales y l√≠nea de regresi√≥n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"x\", y=\"y\", alpha=0.6, label=\"Datos Originales\")\nsns.lineplot(data=df, x=\"x\", y=\"y_pred_lr\", color=\"red\", linewidth=2, label=\"Regresi√≥n Lineal\")\nplt.title(\"Regresi√≥n Lineal: Datos vs Predicciones\")\nplt.xlabel(\"Variable Independiente (x)\")\nplt.ylabel(\"Variable Dependiente (y)\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#k-nearest-neighbors-knn",
    "href": "introduccion.html#k-nearest-neighbors-knn",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "8. K-Nearest Neighbors (KNN)",
    "text": "8. K-Nearest Neighbors (KNN)\n\n8.1 Implementaci√≥n B√°sica de KNN\n\n# 1. Crear modelo KNN con k=10\nmodelo_knn = KNeighborsRegressor(n_neighbors=10)\n\n# 2. Ajustar el modelo\nmodelo_knn.fit(X_features, y_target)\n\n# 3. Hacer predicciones\ny_pred_knn = modelo_knn.predict(X_features)\n\n# 4. Evaluar el modelo\nmse_knn = mean_squared_error(y_target, y_pred_knn)\nprint(f\"MSE KNN (k=10): {mse_knn:.3f}\")\nprint(f\"MSE Regresi√≥n Lineal: {mse_lr:.3f}\")\n\n# Agregar predicciones al DataFrame\ndf[\"y_pred_knn10\"] = y_pred_knn\n\nMSE KNN (k=10): 2.055\nMSE Regresi√≥n Lineal: 2.369\n\n\n\n\n8.2 Funci√≥n para Evaluar Diferentes Valores de K\n\ndef evaluar_knn_diferentes_k(\n    X: pd.DataFrame, \n    y: pd.Series, \n    valores_k: list\n) -&gt; dict:\n    \"\"\"\n    Eval√∫a el rendimiento de KNN para diferentes valores de k.\n    \n    Par√°metros:\n    -----------\n    X : pd.DataFrame\n        Variables independientes\n    y : pd.Series\n        Variable dependiente\n    valores_k : list\n        Lista de valores de k a evaluar\n    \n    Retorna:\n    --------\n    dict\n        Diccionario con k como clave y MSE como valor\n    \"\"\"\n    resultados = {}\n    \n    for k in valores_k:\n        # Crear y ajustar modelo\n        knn = KNeighborsRegressor(n_neighbors=k)\n        knn.fit(X, y)\n        \n        # Predicciones y evaluaci√≥n\n        y_pred = knn.predict(X)\n        mse = mean_squared_error(y, y_pred)\n        \n        resultados[k] = mse\n        print(f\"k={k}: MSE={mse:.3f}\")\n    \n    return resultados\n\n# Evaluar diferentes valores de k\nvalores_k = [1, 5, 10, 20, 50, 100]\nprint(\"Evaluando diferentes valores de k:\")\nresultados_k = evaluar_knn_diferentes_k(X_features, y_target, valores_k)\n\nEvaluando diferentes valores de k:\nk=1: MSE=0.000\nk=5: MSE=1.837\nk=10: MSE=2.055\nk=20: MSE=2.206\nk=50: MSE=2.335\nk=100: MSE=2.367\n\n\n\n\n8.3 Visualizaci√≥n de Diferentes Valores de K\n\n# Crear DataFrame para comparar diferentes valores de k\ndf_comparacion = df[[\"x\", \"y\"]].copy()\n\n# Agregar predicciones para diferentes valores de k\nfor k in [1, 10, 100]:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_features, y_target)\n    df_comparacion[f\"y_pred_k{k}\"] = knn.predict(X_features)\n\n# Visualizaci√≥n comparativa\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, k in enumerate([1, 10, 100]):\n    ax = axes[i]\n    ax.scatter(df_comparacion[\"x\"], df_comparacion[\"y\"], alpha=0.6, label=\"Datos\")\n    ax.plot(df_comparacion[\"x\"], df_comparacion[f\"y_pred_k{k}\"], 'r-', linewidth=2, label=f\"KNN k={k}\")\n    ax.set_title(f\"KNN con k={k}\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#selecci√≥n-del-valor-√≥ptimo-de-k",
    "href": "introduccion.html#selecci√≥n-del-valor-√≥ptimo-de-k",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "9. Selecci√≥n del Valor √ìptimo de K",
    "text": "9. Selecci√≥n del Valor √ìptimo de K\n\n9.1 Evaluaci√≥n Sistem√°tica de K\n\n# Generar un dataset m√°s grande para la evaluaci√≥n\ndf_grande = generar_datos_lineales(n=1000, slope=2.0, intercept=3.0, noise_scale=2.0)\nX_grande = df_grande[[\"x\"]]\ny_grande = df_grande[\"y\"]\n\n# Rango de valores k a evaluar\nmax_k = min(200, len(df_grande) // 5)  # k m√°ximo razonable\nvalores_k_rango = np.arange(1, max_k, 10)\n\nprint(f\"Evaluando k desde 1 hasta {max_k-1} (cada 10 valores)\")\nprint(f\"Total de valores a evaluar: {len(valores_k_rango)}\")\n\nEvaluando k desde 1 hasta 199 (cada 10 valores)\nTotal de valores a evaluar: 20\n\n\n\n# Evaluaci√≥n de todos los valores de k\nerrores_k = []\nfor k in valores_k_rango:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_grande, y_grande)\n    y_pred = knn.predict(X_grande)\n    mse = mean_squared_error(y_grande, y_pred)\n    errores_k.append(mse)\n\n# Encontrar el k √≥ptimo\nk_optimo = valores_k_rango[np.argmin(errores_k)]\nerror_minimo = min(errores_k)\n\nprint(f\"Valor √≥ptimo de k: {k_optimo}\")\nprint(f\"MSE m√≠nimo: {error_minimo:.3f}\")\n\nValor √≥ptimo de k: 1\nMSE m√≠nimo: 0.000\n\n\n\n\n9.2 Visualizaci√≥n de la Curva de Error vs K\n\n# Gr√°fica de MSE vs k\nplt.figure(figsize=(10, 6))\nplt.plot(valores_k_rango, errores_k, 'b-', linewidth=2, marker='o', markersize=4)\nplt.axvline(x=k_optimo, color='r', linestyle='--', \n            label=f'k √≥ptimo = {k_optimo}\\nMSE = {error_minimo:.3f}')\nplt.xlabel('Valor de k')\nplt.ylabel('Error Cuadr√°tico Medio (MSE)')\nplt.title('Selecci√≥n del Valor √ìptimo de k en KNN')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#estimaci√≥n-del-error-real",
    "href": "introduccion.html#estimaci√≥n-del-error-real",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "10. Estimaci√≥n del Error Real",
    "text": "10. Estimaci√≥n del Error Real\n\n10.1 Divisi√≥n Train-Test\n\n# Generar dataset para validaci√≥n\nnp.random.seed(42)  # Para reproducibilidad\ndf_validacion = generar_datos_lineales(n=2000, slope=1.5, intercept=2.5, noise_scale=2.0)\n\n# Divisi√≥n train-test\nX_val = df_validacion[[\"x\"]]\ny_val = df_validacion[\"y\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_val, y_val, test_size=0.3, random_state=42\n)\n\nprint(f\"Datos de entrenamiento: {X_train.shape[0]}\")\nprint(f\"Datos de prueba: {X_test.shape[0]}\")\n\nDatos de entrenamiento: 1400\nDatos de prueba: 600\n\n\n\n\n10.2 Funci√≥n para Evaluaci√≥n Train-Test\n\ndef evaluar_modelo_train_test(\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n    k: int\n) -&gt; tuple:\n    \"\"\"\n    Eval√∫a KNN usando divisi√≥n train-test.\n    \n    Retorna:\n    --------\n    tuple\n        (error_entrenamiento, error_prueba)\n    \"\"\"\n    # Entrenar el modelo\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    # Predicciones\n    y_train_pred = knn.predict(X_train)\n    y_test_pred = knn.predict(X_test)\n    \n    # Errores\n    error_train = mean_squared_error(y_train, y_train_pred)\n    error_test = mean_squared_error(y_test, y_test_pred)\n    \n    return error_train, error_test\n\n# Evaluar diferentes valores de k\nvalores_k_test = [1, 5, 10, 20, 50, 100]\nerrores_train = []\nerrores_test = []\n\nprint(\"Evaluaci√≥n con divisi√≥n train-test:\")\nprint(\"-\" * 40)\nfor k in valores_k_test:\n    error_train, error_test = evaluar_modelo_train_test(X_train, X_test, y_train, y_test, k)\n    errores_train.append(error_train)\n    errores_test.append(error_test)\n    print(f\"k={k:2d} | Train MSE: {error_train:.3f} | Test MSE: {error_test:.3f}\")\n\nEvaluaci√≥n con divisi√≥n train-test:\n----------------------------------------\nk= 1 | Train MSE: 0.000 | Test MSE: 7.600\nk= 5 | Train MSE: 3.092 | Test MSE: 5.007\nk=10 | Train MSE: 3.436 | Test MSE: 4.425\nk=20 | Train MSE: 3.641 | Test MSE: 4.214\nk=50 | Train MSE: 3.820 | Test MSE: 4.016\nk=100 | Train MSE: 3.874 | Test MSE: 3.975\n\n\n\n\n10.3 Visualizaci√≥n de Sesgo vs Varianza\n\n# Gr√°fica de error de entrenamiento vs error de prueba\nplt.figure(figsize=(10, 6))\nplt.plot(valores_k_test, errores_train, 'b-o', linewidth=2, label='Error de Entrenamiento')\nplt.plot(valores_k_test, errores_test, 'r-o', linewidth=2, label='Error de Prueba')\nplt.xlabel('Valor de k')\nplt.ylabel('Error Cuadr√°tico Medio (MSE)')\nplt.title('Curva de Validaci√≥n: Error de Entrenamiento vs Error de Prueba')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')  # Escala logar√≠tmica para mejor visualizaci√≥n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.4 Comparaci√≥n Final: Regresi√≥n Lineal vs KNN √ìptimo\n\n# Encontrar el k √≥ptimo basado en error de prueba\nk_optimo_test = valores_k_test[np.argmin(errores_test)]\nprint(f\"Valor √≥ptimo de k (basado en error de prueba): {k_optimo_test}\")\n\n# Evaluar regresi√≥n lineal\nlr_final = LinearRegression()\nlr_final.fit(X_train, y_train)\ny_test_pred_lr = lr_final.predict(X_test)\nerror_test_lr = mean_squared_error(y_test, y_test_pred_lr)\n\n# Evaluar KNN √≥ptimo\nknn_final = KNeighborsRegressor(n_neighbors=k_optimo_test)\nknn_final.fit(X_train, y_train)\ny_test_pred_knn = knn_final.predict(X_test)\nerror_test_knn = mean_squared_error(y_test, y_test_pred_knn)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"COMPARACI√ìN FINAL - ERROR DE PRUEBA (ESTIMACI√ìN REAL)\")\nprint(\"=\"*50)\nprint(f\"Regresi√≥n Lineal:        {error_test_lr:.3f}\")\nprint(f\"KNN (k={k_optimo_test}):             {error_test_knn:.3f}\")\nprint(\"=\"*50)\n\nif error_test_knn &lt; error_test_lr:\n    mejora = ((error_test_lr - error_test_knn) / error_test_lr) * 100\n    print(f\"KNN es mejor por {mejora:.1f}%\")\nelse:\n    mejora = ((error_test_knn - error_test_lr) / error_test_knn) * 100\n    print(f\"Regresi√≥n Lineal es mejor por {mejora:.1f}%\")\n\nValor √≥ptimo de k (basado en error de prueba): 100\n\n==================================================\nCOMPARACI√ìN FINAL - ERROR DE PRUEBA (ESTIMACI√ìN REAL)\n==================================================\nRegresi√≥n Lineal:        3.887\nKNN (k=100):             3.975\n==================================================\nRegresi√≥n Lineal es mejor por 2.2%",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#resumen",
    "href": "introduccion.html#resumen",
    "title": "Introducci√≥n a Python para Miner√≠a de Datos",
    "section": "Resumen",
    "text": "Resumen\nEn este notebook hemos cubierto:\n\nNumPy: Creaci√≥n y manipulaci√≥n de arrays para datos num√©ricos\nPandas: Manipulaci√≥n de datos estructurados con DataFrames\nIteraci√≥n: T√©cnicas para iterar sobre diccionarios y listas\nVisualizaci√≥n: Creaci√≥n de gr√°ficas informativas con matplotlib/seaborn\nType Hints: Definici√≥n de funciones bien documentadas y tipadas\nRegresi√≥n Lineal: Implementaci√≥n, ajuste y evaluaci√≥n\nKNN: Implementaci√≥n y comparaci√≥n de diferentes valores de k\nSelecci√≥n de Hiperpar√°metros: T√©cnicas para encontrar el k √≥ptimo\nValidaci√≥n: Estimaci√≥n del error real usando divisi√≥n train-test\n\n\nConceptos Clave Aprendidos:\n\nSesgo vs Varianza: KNN con k peque√±o tiene alta varianza, k grande tiene alto sesgo\nValidaci√≥n: El error de entrenamiento subestima el error real\nSelecci√≥n de Modelos: Comparar diferentes algoritmos usando datos de prueba\nType Hints: Mejoran la legibilidad y mantenibilidad del c√≥digo",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Introducci√≥n a Python para Miner√≠a de Datos</span>"
    ]
  },
  {
    "objectID": "regresion_lineal.html",
    "href": "regresion_lineal.html",
    "title": "Descenso en gradiente con regresi√≥n lineal",
    "section": "",
    "text": "Implementaci√≥n multivariada\ndef generar_datos_lineales_multivariados(n_muestras: int, n_caracteristicas: int, ruido: float = 0.5) -&gt; tuple[pd.DataFrame, np.ndarray, float]:\n    \"\"\"\n    Genera un conjunto de datos sint√©tico para regresi√≥n lineal multivariada.\n\n    Args:\n        n_muestras (int): El n√∫mero de puntos de datos a generar (filas).\n        n_caracteristicas (int): El n√∫mero de variables independientes (caracter√≠sticas).\n        ruido (float): La desviaci√≥n est√°ndar del ruido gaussiano a a√±adir.\n                       Controla la dispersi√≥n de los puntos.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray, float]:\n            - Un DataFrame de pandas con las caracter√≠sticas (x1, x2, ...) y la variable objetivo (y).\n            - El array de coeficientes (pesos) reales que se usaron para generar los datos.\n            - El intercepto (sesgo) real que se us√≥.\n    \"\"\"\n    # 1. Generar las caracter√≠sticas (X) con valores aleatorios entre 0 y 10\n    X = 10 * np.random.rand(n_muestras, n_caracteristicas)\n\n    # 2. Generar coeficientes e intercepto reales aleatorios\n    # Estos son los \"verdaderos\" par√°metros que un modelo intentar√≠a encontrar.\n    coeficientes_reales = np.random.randn(n_caracteristicas) * 2\n    intercepto_real = np.random.randn() * 5\n\n    # 3. Generar el ruido gaussiano\n    # El ruido simula la variabilidad aleatoria en los datos del mundo real.\n    ruido_gaussiano = np.random.randn(n_muestras) * ruido\n\n    # 4. Calcular la variable objetivo (y) usando la ecuaci√≥n lineal\n    # y = (X ‚Ä¢ coeficientes) + intercepto + ruido\n    y = np.dot(X, coeficientes_reales) + intercepto_real + ruido_gaussiano\n\n    # 5. Formatear la salida en un DataFrame de pandas\n    nombres_columnas = [f'x{i+1}' for i in range(n_caracteristicas)]\n    datos = pd.DataFrame(X, columns=nombres_columnas)\n    datos['y'] = y\n\n    return datos, coeficientes_reales, intercepto_real\nnum_muestras = 200\nnum_caracteristicas = 10 \nnivel_ruido = 1.5\n\n# Generar los datos\ndatos_generados, coeficientes, intercepto = generar_datos_lineales_multivariados(\n    n_muestras=num_muestras,\n    n_caracteristicas=num_caracteristicas,\n    ruido=nivel_ruido\n)\ndef descenso_gradiente_multivariado(datos: pd.DataFrame, learning_rate: float, iteraciones: int) -&gt; tuple[np.ndarray, float, list]:\n    \"\"\"\n    Realiza el descenso de gradiente para una regresi√≥n lineal multivariada.\n\n    Args:\n        datos (pd.DataFrame): DataFrame que contiene las caracter√≠sticas y la variable objetivo 'y'.\n        learning_rate (float): La tasa de aprendizaje.\n        iteraciones (int): El n√∫mero de iteraciones para ejecutar el algoritmo.\n\n    Returns:\n        tuple[np.ndarray, float, list]:\n            - El array de coeficientes (pesos) optimizados.\n            - El intercepto (sesgo) optimizado.\n            - Una lista con el historial del Error Cuadr√°tico Medio (MSE) en cada iteraci√≥n.\n    \"\"\"\n    # 1. Preparar los datos\n    X = datos.drop('y', axis=1).values  # Matriz de caracter√≠sticas\n    y = datos['y'].values              # Vector de la variable objetivo\n    n_muestras, n_caracteristicas = X.shape\n\n    # 2. Inicializar par√°metros\n    coeficientes = np.zeros(n_caracteristicas)\n    intercepto = 0.0\n    historial_error = []\n\n    # 3. Iterar para optimizar los par√°metros\n    for i in range(iteraciones):\n        # Calcular las predicciones (producto punto de X y coeficientes)\n        # Ecuaci√≥n: y_pred = (X ‚Ä¢ coeficientes) + intercepto\n        y_pred = np.dot(X, coeficientes) + intercepto\n\n        # Calcular el Error Cuadr√°tico Medio (MSE) y guardarlo\n        error = np.mean((y - y_pred) ** 2)\n        historial_error.append(error)\n\n        # Calcular los gradientes (derivadas parciales)\n        # El gradiente es la direcci√≥n de m√°ximo ascenso del error.\n        # Lo calculamos de forma vectorizada para eficiencia.\n        D_coeficientes = -(2/n_muestras) * np.dot(X.T, (y - y_pred))\n        D_intercepto = -(2/n_muestras) * np.sum(y - y_pred)\n\n        # 4. Actualizar los par√°metros (moverse en direcci√≥n opuesta al gradiente)\n        coeficientes = coeficientes - learning_rate * D_coeficientes\n        intercepto = intercepto - learning_rate * D_intercepto\n\n    return coeficientes, intercepto, historial_error\nnum_muestras = 200\nnum_caracteristicas = 3\nnivel_ruido = 1.5\ndatos_generados, coef_reales, int_real = generar_datos_lineales_multivariados(\n    n_muestras=num_muestras,\n    n_caracteristicas=num_caracteristicas,\n    ruido=nivel_ruido\n)\n\nX_original = datos_generados.drop('y', axis=1)\ny = datos_generados['y']\n\nscaler = StandardScaler()\nX_escalado = scaler.fit_transform(X_original)\n\ndatos_escalados = pd.DataFrame(X_escalado, columns=X_original.columns)\ndatos_escalados['y'] = y.values\n\nlearning_rate = 0.01\niteraciones = 1000\ncoef_aprendidos, int_aprendido, error_hist = descenso_gradiente_multivariado(\n    datos=datos_escalados,\n    learning_rate=learning_rate,\n    iteraciones=iteraciones\n)\n\nmodelo_sklearn = LinearRegression()\nmodelo_sklearn.fit(X_escalado, y)\ncoef_sklearn = modelo_sklearn.coef_\nint_sklearn = modelo_sklearn.intercept_\nprint(\"--- Comparaci√≥n de Par√°metros (con datos escalados) ---\")\nprint(f\"Intercepto GD: {int_aprendido:.4f}  |  Intercepto Sklearn: {int_sklearn:.4f}\")\nfor i in range(num_caracteristicas):\n    print(f\"Coef. x{i+1} GD: {coef_aprendidos[i]:.4f} |  Coef. x{i+1} Sklearn: {coef_sklearn[i]:.4f}\")\n\n--- Comparaci√≥n de Par√°metros (con datos escalados) ---\nIntercepto GD: -16.0492  |  Intercepto Sklearn: -16.0492\nCoef. x1 GD: -6.1331 |  Coef. x1 Sklearn: -6.1331\nCoef. x2 GD: -2.4005 |  Coef. x2 Sklearn: -2.4005\nCoef. x3 GD: -2.0273 |  Coef. x3 Sklearn: -2.0273",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Descenso en gradiente con regresi√≥n lineal</span>"
    ]
  }
]