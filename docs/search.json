[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Minería de Datos",
    "section": "",
    "text": "Temario\n\nIntroducción al aprendizaje de máquina\nPrincipios de aprendizaje supervisado\nRegresión lineal\nMétodos de remuestreo y validación cruzada\nPrincipios de Regularización\nProblemas de clasificación, métricas y evaluación\nÁrboles, bosques aleatorios y boosting\nRedes neuronales\nMétodos no supervisados\n\n\nEvaluación\n\nTarea (20%)\nExamen parcial (20%)\nProyecto final (30%):\n\nEntrega (75%)\nExposición (25%)\n\n\nExamen final (30%)\n\nExistirá una parte extra a los alumnos que contribuyan al aprendizaje de sus compañeros:\n\nContribuciones al repositorio: añadiendo redacción más entendible, añadiendo ejemplos particulares a sus carreras, etc.\nActividad en el canal de Slack: contestando dudas de sus compañeros, iniciando discusiones para resolver problemas.\n\n\n\nProfesor\nNombre: Saúl Caballero Ramírez\nCorreo: saul.caballero.ramirez@gmail.com\nCorreo alternativo: saul@nixtla.io\nEl canal más rápido y efectivo será el siguiente canal de Slack. La idea de este canal es que puedan comunicarse entre ustedes para ayudarse a aprender y si necesitan de mi ayuda intentaré contestar en un periodo corto de tiempo. Cualquier comportamiento inadecuado dentro de este foro será penalizado por las reglas de convivencia del ITAM.\n\n\nReferencias principales\n\nAn Introduction to Statistical Learning, James et al. (2023)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, y Jonathan Taylor. 2023. An Introduction to Statistical Learning: With Applications in Python. Springer Texts en Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Temario"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html",
    "href": "00-requerimientos-computacion.html",
    "title": "Requerimientos computacionales",
    "section": "",
    "text": "Git\nEste capítulo proporciona una guía completa para instalar y configurar las herramientas computacionales necesarias para el curso de Minería de Datos. Cubriremos la instalación y uso básico de Git, GitHub, y Conda en sistemas Windows y macOS.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#git",
    "href": "00-requerimientos-computacion.html#git",
    "title": "Requerimientos computacionales",
    "section": "",
    "text": "¿Qué es Git?\nGit es un sistema de control de versiones distribuido que permite rastrear cambios en archivos y coordinar el trabajo entre múltiples personas. Es esencial para el desarrollo de software y la gestión de proyectos de análisis de datos.\nCaracterísticas principales: - Control de versiones: mantiene un historial completo de cambios - Trabajo colaborativo: permite que múltiples personas trabajen en el mismo proyecto - Ramificación (branching): facilita el desarrollo de características en paralelo - Respaldo distribuido: cada copia del repositorio es un respaldo completo\n\n\nInstalación de Git\n\nWindows\n\nDescargar Git:\n\nVisita https://git-scm.com/download/win\nDescarga la versión más reciente para Windows\n\nInstalación:\n\nEjecuta el archivo descargado\nAcepta las opciones por defecto (recomendado para principiantes)\nImportante: asegúrate de seleccionar “Git Bash Here” durante la instalación\n\nVerificar instalación:\ngit --version\n\n\n\nmacOS\nOpción 1: Usando Homebrew (recomendado)\n# Instalar Homebrew si no lo tienes\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Instalar Git\nbrew install git\nOpción 2: Descarga directa 1. Visita https://git-scm.com/download/mac 2. Descarga e instala el paquete\nOpción 3: Xcode Command Line Tools\nxcode-select --install\n\n\n\nConfiguración inicial de Git\nDespués de instalar Git, configura tu identidad:\ngit config --global user.name \"Tu Nombre\"\ngit config --global user.email \"tu.email@ejemplo.com\"\n\n\nComandos básicos de Git\n\nInicializar un repositorio\n# Crear un nuevo repositorio\ngit init\n\n# Clonar un repositorio existente\ngit clone https://github.com/usuario/repositorio.git\n\n\nOperaciones básicas\n# Ver el estado del repositorio\ngit status\n\n# Añadir archivos al área de staging\ngit add archivo.py\ngit add .  # Añadir todos los archivos\n\n# Crear un commit\ngit commit -m \"Mensaje descriptivo del cambio\"\n\n# Ver el historial de commits\ngit log --oneline\n\n\nTrabajar con repositorios remotos\n# Ver repositorios remotos configurados\ngit remote -v\n\n# Añadir un repositorio remoto\ngit remote add origin https://github.com/usuario/repositorio.git\n\n# Enviar cambios al repositorio remoto\ngit push origin main\n\n# Obtener cambios del repositorio remoto\ngit pull origin main",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#github",
    "href": "00-requerimientos-computacion.html#github",
    "title": "Requerimientos computacionales",
    "section": "GitHub",
    "text": "GitHub\n\n¿Qué es GitHub?\nGitHub es una plataforma de desarrollo colaborativo basada en Git que permite: - Hospedar repositorios de código - Colaborar en proyectos - Realizar seguimiento de issues y bugs - Automatizar workflows - Crear documentación con GitHub Pages\n\n\nCreación de cuenta en GitHub\n\nVisita https://github.com\nHaz clic en “Sign up”\nCompleta el formulario de registro\nVerifica tu email\nConfigura tu perfil\n\n\n\nConectar Git local con GitHub\n\nAutenticación con Token Personal\n\nCrear un Personal Access Token:\n\nVe a GitHub → Settings → Developer settings → Personal access tokens → Tokens (classic)\nGenerate new token (classic)\nSelecciona los permisos necesarios: repo, workflow\nGuarda el token en un lugar seguro\n\nUsar el token:\n# Al hacer push por primera vez, usa tu username y el token como password\ngit push origin main\n\n\n\nAutenticación con SSH (recomendado para uso avanzado)\n\nGenerar clave SSH:\nssh-keygen -t ed25519 -C \"tu.email@ejemplo.com\"\nAñadir la clave a GitHub:\n# Copiar la clave pública\ncat ~/.ssh/id_ed25519.pub\n\nVe a GitHub → Settings → SSH and GPG keys → New SSH key\nPega la clave pública\n\nProbar la conexión:\nssh -T git@github.com",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#pull-requests",
    "href": "00-requerimientos-computacion.html#pull-requests",
    "title": "Requerimientos computacionales",
    "section": "Pull Requests",
    "text": "Pull Requests\n\n¿Qué son los Pull Requests?\nLos Pull Requests (PRs) son una funcionalidad de GitHub que permite: - Proponer cambios a un repositorio - Revisar código antes de integrarlo - Discutir modificaciones - Mantener un historial de cambios\n\n\nFlujo de trabajo con Pull Requests\n\n1. Crear una nueva rama\n# Crear y cambiar a una nueva rama\ngit checkout -b nueva-caracteristica\n\n# O usando el comando más moderno\ngit switch -c nueva-caracteristica\n\n\n2. Realizar cambios y commits\n# Hacer cambios en tu código\n# ...\n\n# Añadir y commitear cambios\ngit add .\ngit commit -m \"Implementa nueva característica\"\n\n\n3. Subir la rama a GitHub\ngit push origin nueva-caracteristica\n\n\n4. Crear el Pull Request\n\nVe a tu repositorio en GitHub\nHaz clic en “Compare & pull request”\nAñade un título y descripción clara\nSelecciona los revisores si es necesario\nHaz clic en “Create pull request”\n\n\n\n5. Proceso de revisión\n\nLos revisores pueden añadir comentarios\nPuedes hacer cambios adicionales con nuevos commits\nUna vez aprobado, el PR puede ser fusionado\n\n\n\n6. Fusionar y limpiar\n# Después de que se fusione el PR, actualiza tu rama principal\ngit checkout main\ngit pull origin main\n\n# Elimina la rama local\ngit branch -d nueva-caracteristica",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#conda",
    "href": "00-requerimientos-computacion.html#conda",
    "title": "Requerimientos computacionales",
    "section": "Conda",
    "text": "Conda\n\n¿Qué es Conda?\nConda es un gestor de paquetes y entornos de código abierto que: - Instala, ejecuta y actualiza paquetes y sus dependencias - Crea, guarda, carga y cambia entre entornos - Funciona con cualquier lenguaje (Python, R, Ruby, Lua, Scala, Java, etc.) - Es multiplataforma (Windows, macOS, Linux)\n\n\nInstalación de Conda\n\nOpción 1: Miniconda (recomendado)\nWindows: 1. Descargar desde https://docs.conda.io/en/latest/miniconda.html 2. Ejecutar el instalador 3. Seguir las instrucciones por defecto 4. Reiniciar la terminal\nmacOS:\n# Descargar e instalar con curl\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\n# Para Apple Silicon (M1/M2)\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nbash Miniconda3-latest-MacOSX-arm64.sh\n\n\nOpción 2: Anaconda (instalación completa)\n\nDescargar desde https://www.anaconda.com/products/distribution\nSeguir las instrucciones de instalación para tu sistema operativo\n\n\n\n\nVerificar la instalación\nconda --version\nconda info",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#gestión-de-entornos-con-conda",
    "href": "00-requerimientos-computacion.html#gestión-de-entornos-con-conda",
    "title": "Requerimientos computacionales",
    "section": "Gestión de Entornos con Conda",
    "text": "Gestión de Entornos con Conda\n\n¿Por qué usar entornos?\nLos entornos virtuales permiten: - Aislar dependencias entre proyectos - Usar diferentes versiones de Python/paquetes - Evitar conflictos entre bibliotecas - Reproducir entornos de trabajo\n\n\nComandos básicos para entornos\n\nCrear entornos\n# Crear un entorno con Python específico\nconda create -n mi_entorno python=3.10\n\n# Crear un entorno con paquetes específicos\nconda create -n ciencia_datos python=3.10 numpy pandas matplotlib\n\n# Crear entorno desde un archivo\nconda env create -f environment.yml\n\n\nActivar y desactivar entornos\n# Activar un entorno\nconda activate mi_entorno\n\n# Desactivar el entorno actual\nconda deactivate\n\n# Listar entornos disponibles\nconda env list\n\n\nGestionar paquetes en entornos\n# Instalar paquetes\nconda install numpy pandas scikit-learn\n\n# Instalar desde conda-forge (recomendado)\nconda install -c conda-forge seaborn\n\n# Instalar múltiples paquetes\nconda install jupyter matplotlib seaborn\n\n# Listar paquetes instalados\nconda list\n\n# Actualizar paquetes\nconda update numpy\nconda update --all\n\n\nEliminar entornos\n# Eliminar un entorno\nconda env remove -n mi_entorno\n\n\n\nArchivo environment.yml\nPara reproducir entornos, crea un archivo environment.yml:\nname: mineria_datos\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - jupyter\n  - numpy\n  - pandas\n  - matplotlib\n  - seaborn\n  - scikit-learn\n  - pip\n  - pip:\n    - some-pip-package\nCrear el entorno desde el archivo:\nconda env create -f environment.yml\n\n\nMejores prácticas\n\nUn entorno por proyecto: Crea un entorno específico para cada proyecto\nUsa conda-forge: Preferir el canal conda-forge para paquetes actualizados\nExporta tus entornos: Mantén archivos environment.yml para reproducibilidad\nActualiza regularmente: Mantén tus entornos actualizados\nDocumenta dependencias: Incluye versiones específicas cuando sea crítico\n\n\n\nSolución de problemas comunes\n\nProblema: Conda no se reconoce en la terminal\nSolución: Reinicia la terminal o añade conda al PATH:\n# Windows (en Command Prompt como administrador)\nconda init cmd.exe\n\n# macOS/Linux\nconda init bash\n\n\nProblema: Conflictos de paquetes\nSolución: Usa mamba (más rápido) o especifica canales:\n# Instalar mamba\nconda install mamba -n base -c conda-forge\n\n# Usar mamba en lugar de conda\nmamba install numpy pandas\n\n\nProblema: Entorno no se activa\nSolución: Verifica la instalación y configuración:\nconda info --envs\nconda config --show",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#configuración-de-conda-en-diferentes-entornos",
    "href": "00-requerimientos-computacion.html#configuración-de-conda-en-diferentes-entornos",
    "title": "Requerimientos computacionales",
    "section": "Configuración de Conda en diferentes entornos",
    "text": "Configuración de Conda en diferentes entornos\n\nConda en Windows con Git Bash\nGit Bash en Windows puede requerir configuración adicional para funcionar correctamente con Conda:\n\n1. Configuración inicial en Git Bash\nDespués de instalar Miniconda en Windows, es posible que conda no esté disponible en Git Bash por defecto:\n# Verificar si conda está disponible\nconda --version\nSi no funciona, necesitas inicializar conda para Git Bash:\n# Navega al directorio de instalación de Miniconda (ajusta la ruta según tu instalación)\ncd /c/Users/TuUsuario/miniconda3/Scripts\n\n# Inicializar conda para bash\n./conda.exe init bash\n\n\n2. Alternativa: Añadir conda al PATH manualmente\nSi la inicialización no funciona, añade conda manualmente al PATH en Git Bash:\n# Añadir estas líneas a tu archivo ~/.bashrc\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3:$PATH\"' &gt;&gt; ~/.bashrc\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3/Scripts:$PATH\"' &gt;&gt; ~/.bashrc\n\n# Recargar el archivo bashrc\nsource ~/.bashrc\n\n\n3. Verificar la configuración\n# Reiniciar Git Bash y verificar\nconda --version\nconda info\n\n\n4. Trabajar con entornos en Git Bash\n# Crear y activar entorno\nconda create -n mineria_datos python=3.10\nconda activate mineria_datos\n\n# Si aparece un error de activación, usa:\nsource activate mineria_datos\n\n\n\nTroubleshooting específico para Windows\n\nProblema: “conda: command not found” en Git Bash\nSoluciones:\n\nReinstalar Miniconda con “Add to PATH” marcado\nUsar el Anaconda Prompt (más confiable en Windows)\nConfigurar manualmente el PATH:\n\n# En ~/.bashrc, añade:\nalias conda='/c/Users/TuUsuario/miniconda3/Scripts/conda.exe'\nalias python='/c/Users/TuUsuario/miniconda3/python.exe'\n\n\nProblema: Activación de entorno no funciona\n# En lugar de conda activate, usa:\nsource /c/Users/TuUsuario/miniconda3/etc/profile.d/conda.sh\nconda activate mineria_datos",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#visual-studio-code",
    "href": "00-requerimientos-computacion.html#visual-studio-code",
    "title": "Requerimientos computacionales",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\n¿Por qué usar VS Code para ciencia de datos?\nVisual Studio Code es un editor de código ligero pero potente que ofrece: - Soporte excelente para Python y Jupyter notebooks - Integración nativa con conda y entornos virtuales - Extensiones específicas para ciencia de datos - Terminal integrado - Control de versiones Git integrado - IntelliSense y debugging avanzado\n\n\nInstalación de Visual Studio Code\n\nWindows y macOS\n\nDescargar desde https://code.visualstudio.com/\nEjecutar el instalador siguiendo las instrucciones por defecto\nReiniciar el sistema si es necesario\n\n\n\n\nExtensiones esenciales para ciencia de datos\nInstala estas extensiones desde el marketplace de VS Code (Ctrl/Cmd + Shift + X):\n\nExtensiones obligatorias:\n- Python (Microsoft)\n- Jupyter (Microsoft)\n- Python Debugger (Microsoft)\n\n\nExtensiones recomendadas:\n- GitLens — Git supercharged\n- Pylance (análisis avanzado de Python)\n- autoDocstring - Python Docstring Generator\n- Python Indent\n- Bracket Pair Colorizer\n- Data Wrangler (Microsoft)\n\n\n\nConfiguración de Python y Conda en VS Code\n\n1. Seleccionar el intérprete de Python\n\nAbre VS Code\nPresiona Ctrl+Shift+P (Windows) o Cmd+Shift+P (macOS)\nEscribe “Python: Select Interpreter”\nSelecciona el intérprete del entorno mineria_datos\n\nLa ruta debería ser similar a: - Windows: C:\\Users\\TuUsuario\\miniconda3\\envs\\mineria_datos\\python.exe - macOS: /Users/TuUsuario/miniconda3/envs/mineria_datos/bin/python\n\n\n2. Verificar la configuración\nCrea un archivo de prueba test.py:\nimport sys\nprint(f\"Python path: {sys.executable}\")\nprint(f\"Python version: {sys.version}\")\n\n# Verificar paquetes\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nprint(\"¡Todos los paquetes importados correctamente!\")\n\n\n3. Configurar terminal integrado\nEn VS Code, abre el terminal integrado (Ctrl+`` o View → Terminal) y configura conda:\nWindows:\n# Si usas Git Bash en VS Code\nconda activate mineria_datos\nmacOS:\nconda activate mineria_datos\n\n\n\nTrabajar con Jupyter Notebooks en VS Code\n\n1. Crear un nuevo notebook\n\nCtrl+Shift+P → “Jupyter: Create New Jupyter Notebook”\nO crear un archivo con extensión .ipynb\n\n\n\n2. Seleccionar kernel\n\nEn la esquina superior derecha del notebook, haz clic en “Select Kernel”\nSelecciona “Python Environments”\nEscoge el entorno mineria_datos\n\n\n\n3. Verificar configuración del notebook\n# Primera celda del notebook\nimport sys\nprint(f\"Ejecutándose en: {sys.executable}\")\n\n# Importar bibliotecas del curso\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nprint(\"Notebook configurado correctamente para el curso!\")\n\n\n\nConfiguración avanzada de VS Code\n\nConfiguración del workspace\nCrea un archivo .vscode/settings.json en tu directorio del proyecto:\n{\n    \"python.defaultInterpreterPath\": \"./mineria_datos/bin/python\",\n    \"python.terminal.activateEnvironment\": true,\n    \"jupyter.defaultKernel\": \"mineria_datos\",\n    \"files.associations\": {\n        \"*.qmd\": \"markdown\"\n    },\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"editor.formatOnSave\": true\n}\n\n\nAtajos de teclado útiles\n\nCtrl+Shift+P: Command Palette\nCtrl+``: Toggle Terminal\nShift+Enter: Ejecutar celda de notebook\nCtrl+Enter: Ejecutar celda sin avanzar\nF5: Iniciar debugging\nCtrl+Shift+G: Control de versiones Git\n\n\n\n\nIntegración con Git en VS Code\n\n1. Clonar repositorio\n\nCtrl+Shift+P → “Git: Clone”\nPega la URL del repositorio\nSelecciona la carpeta destino\n\n\n\n2. Operaciones Git básicas\n\nSource Control panel (Ctrl+Shift+G): Ver cambios\nStage changes: Hacer clic en el “+” junto a los archivos\nCommit: Escribir mensaje y presionar Ctrl+Enter\nPush/Pull: Usar los botones en la barra de estado\n\n\n\n3. Crear branches\n\nClic en el nombre de la branch en la barra de estado\n“Create new branch”\nEscribir el nombre de la nueva branch",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "00-requerimientos-computacion.html#configuración-completa-del-entorno-para-el-curso",
    "href": "00-requerimientos-computacion.html#configuración-completa-del-entorno-para-el-curso",
    "title": "Requerimientos computacionales",
    "section": "Configuración completa del entorno para el curso",
    "text": "Configuración completa del entorno para el curso\n\nPasos de configuración paso a paso\n\n1. Instalar las herramientas base\n\nGit (siguiendo las instrucciones de instalación según tu sistema operativo)\nConda (Miniconda recomendado)\nVisual Studio Code (opcional pero recomendado)\n\n\n\n2. Crear y configurar el entorno de conda\nEn cualquier terminal (Command Prompt, Git Bash, Terminal de macOS):\n# Crear el entorno del curso\nconda create -n mineria_datos python=3.10\n\n# Activar el entorno\nconda activate mineria_datos\nSi tienes problemas con conda activate en Windows Git Bash:\n# Alternativa para Windows Git Bash\nsource activate mineria_datos\n\n\n3. Instalar paquetes necesarios\nOpción recomendada (con mamba para mayor velocidad):\n# Instalar mamba primero\nconda install mamba -c conda-forge\n\n# Instalar todos los paquetes del curso\nmamba install jupyter numpy pandas matplotlib seaborn scikit-learn nbclient ipykernel pyyaml plotly -y\nOpción alternativa (solo con conda):\nconda install jupyter numpy pandas matplotlib seaborn scikit-learn nbclient ipykernel pyyaml plotly -y\n\n\n4. Configurar Jupyter para el entorno\n# Registrar el entorno como kernel de Jupyter\npython -m ipykernel install --user --name mineria_datos --display-name \"Python (Minería de Datos)\"\n\n# Verificar que el kernel se registró correctamente\njupyter kernelspec list\n\n\n5. Verificar la instalación\n# Probar que todos los paquetes se importan correctamente\npython -c \"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport plotly\nprint('✅ ¡Todos los paquetes instalados correctamente!')\nprint(f'Python version: {pd.__version__}')\nprint(f'Pandas version: {pd.__version__}')\nprint(f'NumPy version: {np.__version__}')\nprint(f'Scikit-learn version: {sklearn.__version__}')\n\"\n\n\n\nConfiguración específica por entorno\n\nPara usuarios de Windows Git Bash\nSi planeas usar Git Bash como tu terminal principal:\n# Añadir conda al PATH permanentemente\necho 'export PATH=\"/c/Users/TuUsuario/miniconda3/Scripts:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Verificar configuración\nconda info\n\n\nPara usuarios de Visual Studio Code\n\nInstalar VS Code y extensiones:\n\nPython (Microsoft)\nJupyter (Microsoft)\nPython Debugger (Microsoft)\n\nConfigurar el intérprete:\n\nCtrl+Shift+P → “Python: Select Interpreter”\nSeleccionar el intérprete del entorno mineria_datos\n\nCrear archivo de configuración del workspace:\n\nCrea .vscode/settings.json en tu directorio de proyecto:\n{\n    \"python.defaultInterpreterPath\": \"~/miniconda3/envs/mineria_datos/bin/python\",\n    \"python.terminal.activateEnvironment\": true,\n    \"jupyter.defaultKernel\": \"mineria_datos\",\n    \"files.associations\": {\n        \"*.qmd\": \"markdown\"\n    }\n}\n\n\nPara usuarios de Jupyter Lab (opcional)\nSi prefieres usar JupyterLab en lugar de notebooks en VS Code:\n# Instalar JupyterLab\nconda install jupyterlab\n\n# Iniciar JupyterLab\njupyter lab\n\n\n\nClonar el repositorio del curso\nUna vez configurado tu entorno, clona el repositorio del curso:\n# Clonar el repositorio (sustituye por la URL real del curso)\ngit clone [URL_DEL_REPOSITORIO_DEL_CURSO]\n\n# Navegar al directorio\ncd nombre-del-repositorio\n\n# Activar el entorno\nconda activate mineria_datos\n\n# Si hay un archivo environment.yml, úsalo para instalar dependencias adicionales\nconda env update -f environment.yml\n\n\nFlujo de trabajo recomendado\n\nIniciar sesión de trabajo:\n# Activar entorno\nconda activate mineria_datos\n\n# Navegar al directorio del proyecto\ncd ruta/al/proyecto\n\n# Abrir VS Code (si lo usas)\ncode .\nPara cada tarea/proyecto:\n\nCrear una nueva branch en Git\nTrabajar en tus notebooks/scripts\nHacer commits regularmente\nCrear Pull Request cuando esté listo\n\nMantener el entorno actualizado:\n# Actualizar paquetes periódicamente\nconda update --all\n\n\n\nVerificación final\nEjecuta este script para verificar que todo está configurado correctamente:\n# test_setup.py\nimport sys\nimport subprocess\n\ndef test_environment():\n    print(\"🔍 Verificando configuración del entorno...\")\n    \n    # Verificar Python\n    print(f\"✅ Python: {sys.version}\")\n    print(f\"✅ Ejecutable: {sys.executable}\")\n    \n    # Verificar paquetes críticos\n    required_packages = [\n        'pandas', 'numpy', 'matplotlib', 'seaborn', \n        'sklearn', 'jupyter', 'plotly'\n    ]\n    \n    for package in required_packages:\n        try:\n            __import__(package)\n            print(f\"✅ {package}: disponible\")\n        except ImportError:\n            print(f\"❌ {package}: NO disponible\")\n    \n    # Verificar conda\n    try:\n        result = subprocess.run(['conda', '--version'], \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"✅ Conda: {result.stdout.strip()}\")\n        else:\n            print(\"❌ Conda: no disponible\")\n    except FileNotFoundError:\n        print(\"❌ Conda: no encontrado en PATH\")\n    \n    # Verificar git\n    try:\n        result = subprocess.run(['git', '--version'], \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"✅ Git: {result.stdout.strip()}\")\n        else:\n            print(\"❌ Git: no disponible\")\n    except FileNotFoundError:\n        print(\"❌ Git: no encontrado en PATH\")\n    \n    print(\"\\n🎉 ¡Verificación completa!\")\n\nif __name__ == \"__main__\":\n    test_environment()\nGuarda este script como test_setup.py y ejecútalo:\npython test_setup.py\n¡Con esta configuración completa, estarás listo para trabajar eficientemente en todos los proyectos del curso de Minería de Datos!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Requerimientos computacionales</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "Introducción",
    "section": "",
    "text": "¿Qué es aprendizaje de máquina?\nMétodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe también aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisión afecta directa e inmediatamente al entorno.\nLas tareas más apropiadas para este enfoque, en general, son aquellas en donde:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#qué-es-aprendizaje-de-máquina",
    "href": "01-introduccion.html#qué-es-aprendizaje-de-máquina",
    "title": "Introducción",
    "section": "",
    "text": "Existe una cantidad considerable de datos relevantes para aprender a ejecutar la tarea.\nEl costo por errores al ejecutar la tarea es relativamente bajo (al menos comparado con alternativas).\nLa tarea se repite de manera más o menos homogénea una cantidad grande de veces.\n\n\nEjemplos de tareas de aprendizaje:\n\nPredecir si un cliente de tarjeta de crédito va a caer en impago en los próximos doce meses.\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica.\nDividir a los clientes de Netflix según sus gustos.\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\nReconocer un tipos de documentos (identificación, comprobante de domicilio, comprobante de ingresos) para acelerar el proceso de evaluación de crédito.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender más del problema que nos interesa: estas soluciones forman parte de un ciclo de análisis de datos donde podemos aprender de una forma más concentrada cuáles son características y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc). Las razones para no tomar un enfoque de reglas construidas “a mano”:\n\nCuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¿qué búsquedas www se enfocan en dar direcciones como resultados? ¿cómo filtrar comentarios no aceptables en foros?\nFinalmente, notamos que en estos problemas nuestro interés principal no es entender qué variables influyen en otras (en el proceso natural o de negocio). Sin más teoría o diseño de datos, los métodos que utilizaremos explotan patrones en los datos que no necesariamente explican cómo funcionan los sistemas de interés.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "Introducción",
    "section": "Aprendizaje supervisado y no supervisado",
    "text": "Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión\nProblemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-principios.html",
    "href": "02-principios.html",
    "title": "Principios de aprendizaje supervisado",
    "section": "",
    "text": "Definición de aprendizaje supervisado\nSupongamos que observamos una variable cuantitativa \\(Y \\in \\mathbb{R}\\) y tenemos \\(p\\) variables predictoras, \\(X_1, X_2, ..., X_p\\), las cuales denotaremos como \\(X = (X_1, X_2, ..., X_p)\\). Supongamos que existe alguna reluación entre ellas y se puede expresar de la siguiente forma:\n\\[ Y = f(x) + \\epsilon\\]\nLa tarea del aprendizaje supervisado es aprender la función \\(f\\). Existen dos razones por las cuales estimar \\(f\\): predicción e inferencia.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#definición-de-aprendizaje-supervisado",
    "href": "02-principios.html#definición-de-aprendizaje-supervisado",
    "title": "Principios de aprendizaje supervisado",
    "section": "",
    "text": "Función \\(f\\): función desconocida que relaciona a \\(X\\) con \\(Y\\). Representa la información sistémica que \\(X\\) aporta a \\(Y\\).\nError \\(\\epsilon\\): representa qué tan equivocados estamos con respecto al verdadero valor de \\(Y\\).\n\n\n\nPredicción\nEn muchas ocasiones existen un conjunto de variables \\(X\\) que están listas para aprovecharse, sin embargo, puede que no se pueda obtener la variable \\(Y\\) de manera inmediata. En este sentido, podemos predecir la variable \\(Y\\) siguiendo la ecuación:\n\\[\\hat{Y} = \\hat{f}(X)\\]\ndonde \\(\\hat{f}\\) representa nuestro estimador de \\(f\\) y \\(\\hat{Y}\\) es nuestra predicción de \\(Y\\). En este sentido \\(\\hat{f}\\) es una caja negra en el sentido en el que no nos preocupa cuál es la función, sino que provee predicciones precisas para \\(Y\\).\nLa precisión de \\(\\hat{Y}\\) depende de dos cantidades:\n\nError reducible: En general, \\(\\hat{f}\\) no será un estimador perfecto de \\(f\\) y esto introducirá un error el cuál puede reducirse. Ejemplos: Introducir una estructura lineal cuándo el problema tiene estructura cuadrática, falta de variables explicativas, exceso de variables que no contribuyen a la predicción.\nError ireducible: La variable \\(Y\\) es una función también de \\(\\epsilon\\) y por definición nuestra predicción tendra un error inherente. Ejemplos: Predecir que comerán mañana, determinar si lloverá o no, determinar cuándo ocurrirá un temblor, ¿quién ganará una elección?.\n\n\\[ \\begin{align*}\n\\mathbb{E}[(Y-\\hat{Y})^2] &= \\mathbb{E}[(f(X) + \\epsilon -\\hat{f}(x))^2]\\\\\n&= \\underset{Reducible}{\\underbrace{\\mathbb{E}[(f(X) - \\hat{f}(x))^2]}} + \\underset{Irreducible}{\\underbrace{\\text{Var}(\\epsilon)}}\n\\end{align*}\\]\nEl objetivo del curso se enfoca en técnicas para estimar \\(f\\) con el objectivo de minimizar el error reducible. Es importante tener en cuenta que el error irreducible siempre nos pondrá una cota en la predicción de \\(Y\\).\n\n\nInferencia\nExisten problemas en donde nos interesa más entender la relación intrinseca que existe entre \\(Y\\) y \\(X\\). En esta situación nuestro objetivo no es hacer predicción, entonces \\(\\hat{f}\\) ya no puede ser tratada como una caja negra. En este tipo de enfoque se contestan preguntas cómo:\n\n¿Cuáles son los predictores que se asocian con la variable \\(Y\\)?: Muchas veces solo un subconjunto de los datos \\(X\\) son los que realmente están relacionados con \\(Y\\).\n¿Cuál es la relación entre \\(Y\\) y \\(X_i\\)?\n¿La relación entre \\(Y\\) y \\(X_i\\) es lineal o más compleja?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#cómo-estimar-f",
    "href": "02-principios.html#cómo-estimar-f",
    "title": "Principios de aprendizaje supervisado",
    "section": "¿Cómo estimar \\(f\\)?",
    "text": "¿Cómo estimar \\(f\\)?\nAsumiremos que tenemos \\(n\\) datos diferentes estas observaciones serán llamadas conjunto de entrenamiento. \\(x_{ij}\\) representa el valor del predictor \\(j\\) para la observación \\(i\\), donde \\(i=1,2,...,n\\) y \\(j=1,2,...,p\\). \\(y_i\\) representa la variable respuesta de la observación \\(i\\). Entonces nuestro conjunto de entrenamiento consiste en:\n\\[{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}\\]\ndonde \\(x_i=(x_{i1}, x_{i2}, ..., x_{ip})^T\\).\nNuestro objetivo es aplicar un método de aprendizaje en el conjunto de datos para poder estimar una función desconocida de \\(f\\). Nos encantaría encontrar una función \\(\\hat{f}\\) de forma tal que \\(Y\\simeq \\hat{f}(X)\\) para cualquier observación \\((X, Y)\\). Muchos de estos enfoque se pueden caracterizar como métodos paramétricos o no paramétricos.\n\nMétodos paramétricos\nLos métodos paramétricos involucran un enfoque de dos pasos:\n\nHacemos un supuesto de la forma función de \\(f\\). Por ejemplo, la más sencilla es que \\(f\\) es linear en \\(\\beta\\):\n\n\\[ f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]\nUna vez haciendo haciendo el supuesto de linealidad el problema de estimar \\(f\\) es simplificado ya que en lugar de explorar el espacio funcional uno solo necesita estimar \\(p+1\\) coeficientes \\(\\beta_0, ..., \\beta_p\\).\n\nNecesitamos un proceso que utilice los datos de entrenamiento para ajustar u entrenar el modelo. El enfoque más sencillo es el método de mínimos cuadrados ordinarios (OLS):\n\n\\[\\underset{\\beta_0, \\beta_1, ..., \\beta_p}{min} \\sum_{i=1}^{N}(y_i - (\\beta_0 + \\beta_1 x_{i1} +\\beta_2 x_{i2} + ... + \\beta_p X_p))^2\\]\nEl enfoque basado en modelado se refiere a los modelos paramétricos; reduce el problema de estimar \\(f\\) a estimar un conjunto de parámetros. La desventaja potencial es que el modelo podría no ser igual a la verdadera \\(f\\) y tendremos malas estimaciones del valor de \\(y\\).\n\n\nMétodos no paramétricos",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html",
    "href": "03-regresion_lineal.html",
    "title": "Regresión lineal",
    "section": "",
    "text": "Regresión Lineal Simple\nComenzaremos con el caso más sencillo: predecir una variable de resultado Y a partir de una única variable predictora X.\nEl modelo matemático que queremos ajustar es una línea recta:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nDonde:\nNuestro objetivo 🎯 es encontrar los mejores valores posibles para los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) usando los datos que tenemos.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresión-lineal-simple",
    "href": "03-regresion_lineal.html#regresión-lineal-simple",
    "title": "Regresión lineal",
    "section": "",
    "text": "\\(Y\\): La variable dependiente (lo que queremos predecir).\n\\(X\\): La variable independiente (nuestro predictor).\n\\(\\beta_0\\): El intercepto (el valor de \\(Y\\) cuando \\(X=0\\)).\n\\(\\beta_1\\): La pendiente (cuánto cambia \\(Y\\) por cada unidad que aumenta \\(X\\)).\n\\(\\epsilon\\): El término de error (la parte de \\(Y\\) que nuestro modelo no puede explicar).\n\n\n\n¿Cómo estimamos los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\)?\n“Mejor” para nosotros significa encontrar la línea que minimice la distancia vertical entre cada punto de dato y la propia línea. Específicamente, minimizamos la Suma de los Errores al Cuadrado (SEC o Sum of Squared Errors, SSE).\nLa función de costo (o pérdida) que queremos minimizar es:\n\\[J(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\\]\nTenemos dos métodos principales para encontrar los \\(\\beta\\) que minimizan esta función:\n\nMétodo 1: Las Ecuaciones Normales (La solución analítica 🧠)\nEste método utiliza cálculo para encontrar el mínimo exacto de la función de costo. Para ello, tomamos las derivadas parciales de \\(J\\) con respecto a \\(\\beta_0\\) y \\(\\beta_1\\), las igualamos a cero y resolvemos para los coeficientes.\n\n\n\n\n\n\nDerivada parcial con respecto a \\(\\beta_0\\):\n\n\n\n\n\n\\[\\frac{\\partial J}{\\partial \\beta_0} = \\sum_{i=1}^{n} -2(y_i - \\beta_0 - \\beta_1 x_i) = 0\\] \\[\\sum y_i - n\\beta_0 - \\beta_1 \\sum x_i = 0\\] \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\n\n\n\n\n\n\n\n\n\nDerivada parcial con respecto a \\(\\beta_1\\)\n\n\n\n\n\n\\[\\frac{\\partial J}{\\partial \\beta_1} = \\sum_{i=1}^{n} -2x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0\\] Sustituyendo \\(\\beta_0\\) de la primera ecuación y resolviendo, llegamos a: \\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\n\n\n\nEstas fórmulas nos dan los valores óptimos y exactos de los coeficientes directamente a partir de los datos.\n\n\nMétodo 2: Descenso en Gradiente (La solución iterativa ⚙️)\nEste es un método computacional que nos “acerca” progresivamente a la solución. Es especialmente útil cuando tenemos una cantidad masiva de datos y calcular la solución analítica es muy costoso.\nLa intuición: Imagina que estás en una montaña (la función de costo) y quieres llegar al valle (el costo mínimo). El Descenso en Gradiente te dice que mires a tu alrededor y des un paso en la dirección más inclinada hacia abajo. Repites esto hasta llegar al fondo.\nEl algoritmo funciona así:\n\nInicializa los coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) con valores aleatorios (o en ceros).\nCalcula el gradiente de la función de costo. El gradiente es un vector que apunta en la dirección del máximo ascenso. Nosotros iremos en la dirección opuesta.\n\n\\(\\frac{\\partial J}{\\partial \\beta_0} = -2 \\sum (y_i - (\\beta_0 + \\beta_1 x_i))\\)\n\\(\\frac{\\partial J}{\\partial \\beta_1} = -2 \\sum x_i(y_i - (\\beta_0 + \\beta_1 x_i))\\)\n\nActualiza los coeficientes usando una tasa de aprendizaje (\\(\\alpha\\)), que controla el tamaño del paso que damos.\n\n\\(\\beta_0 := \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0}\\)\n\\(\\beta_1 := \\beta_1 - \\alpha \\frac{\\partial J}{\\partial \\beta_1}\\)\n\nRepite los pasos 2 y 3 durante un número determinado de iteraciones o hasta que el cambio en el costo sea muy pequeño (convergencia).\n\n\n\n\n\n\n\nExplicacion visual",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#cuáles-son-los-supuestos-de-la-regresión",
    "href": "03-regresion_lineal.html#cuáles-son-los-supuestos-de-la-regresión",
    "title": "Regresión lineal",
    "section": "¿Cuáles son los supuestos de la regresión? 🧐",
    "text": "¿Cuáles son los supuestos de la regresión? 🧐\nPara que nuestro modelo sea confiable (es decir, para que los coeficientes y las predicciones tengan sentido), debemos cumplir con ciertos supuestos.\n\nLinealidad: La relación entre \\(\\beta\\) y \\(Y\\) debe ser lineal.\n\n¿Para qué sirve? Si la relación no es lineal, nuestro modelo de línea recta será intrínsecamente incorrecto.\n\nIndependencia de los errores: Los errores (residuos) no deben estar correlacionados entre sí.\n\n¿Para qué sirve? Es crucial para datos de series temporales. Si los errores están correlacionados, la información de un error nos da pistas sobre el siguiente, lo cual viola la idea de que cada observación es independiente.\n\nHomocedasticidad (Varianza constante de los errores): La varianza de los errores debe ser constante para todos los niveles de \\(X\\).\n\n¿Para qué sirve? Si la varianza cambia (heterocedasticidad), nuestras predicciones serán mejores para algunas partes de los datos que para otras, y los intervalos de confianza para los coeficientes serán poco fiables. Visualmente, en un gráfico de residuos vs. valores predichos, no queremos ver una forma de cono o embudo.\n\nNormalidad de los errores: Los errores deben seguir una distribución normal con media cero.\n\n¿Para qué sirve? Este supuesto es fundamental para poder realizar pruebas de hipótesis sobre los coeficientes (como los p-values) y construir intervalos de confianza. Podemos verificarlo con un histograma de los residuos o un gráfico Q-Q.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#cómo-evaluar-la-precisión-del-modelo",
    "href": "03-regresion_lineal.html#cómo-evaluar-la-precisión-del-modelo",
    "title": "Regresión lineal",
    "section": "¿Cómo evaluar la precisión del modelo? 📈",
    "text": "¿Cómo evaluar la precisión del modelo? 📈\nUna vez que hemos ajustado el modelo, ¿cómo sabemos si es bueno?\n\nCoeficiente de Determinación (\\(R^2\\))\nEl \\(R^2\\) mide la proporción de la varianza total en la variable dependiente (\\(Y\\)) que es explicada por nuestro modelo.\n\\[R^2 = 1 - \\frac{\\text{Suma de Errores al Cuadrado (SEC)}}{\\text{Suma Total de Cuadrados (STC)}} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\\]\n\n\\(R^2\\) varía entre 0 y 1 (o 0% y 100%).\nUn \\(R^2\\) de 0.85 significa que el 85% de la variabilidad en \\(Y\\) puede ser explicada por \\(X\\).\nUn \\(R^2\\) más alto generalmente indica un mejor ajuste del modelo.\n\n\n\np-values (Valores p)\nEl p-value nos ayuda a determinar si nuestra variable predictora \\(X\\) es estadísticamente significativa. Responde a la pregunta: ¿Es probable que la relación que observamos entre \\(X\\) y \\(Y\\) haya ocurrido por puro azar?\n\nHipótesis Nula (\\(H_0\\)): No hay relación entre \\(X\\) y \\(Y\\) (es decir, \\(\\beta_1 = 0\\)).\nHipótesis Alternativa (\\(H_a\\)): Sí hay una relación entre \\(X\\) y \\(Y\\) (es decir, \\(\\beta_1 \\neq 0\\)).\n\nUn p-value pequeño (típicamente &lt; 0.05) nos da evidencia para rechazar la hipótesis nula. Esto sugiere que nuestra variable \\(X\\) es un predictor útil para \\(Y\\).",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#métricas-de-error-de-predicción",
    "href": "03-regresion_lineal.html#métricas-de-error-de-predicción",
    "title": "Regresión lineal",
    "section": "Métricas de Error de Predicción",
    "text": "Métricas de Error de Predicción\nAdemás del \\(R^2\\), existen múltiples métricas para evaluar qué tan bien predice nuestro modelo. Cada una tiene sus ventajas y casos de uso específicos:\n\nError Cuadrático Medio (MSE)\nEl MSE mide el promedio de los errores al cuadrado:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\n\nVentajas: Penaliza fuertemente errores grandes, diferenciable (útil para optimización)\nDesventajas: Sensible a valores atípicos, difícil de interpretar (unidades al cuadrado)\nCuándo usar: Cuando errores grandes son especialmente costosos\n\n\n\nRaíz del Error Cuadrático Medio (RMSE)\nEl RMSE es la raíz cuadrada del MSE:\n\\[RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]\n\nVentajas: Mismas unidades que la variable objetivo, interpretable\nDesventajas: Aún sensible a valores atípicos\nInterpretación: “En promedio, nuestras predicciones se desvían X unidades del valor real”\n\n\n\nError Absoluto Medio (MAE)\nEl MAE mide el promedio de los errores absolutos:\n\\[MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\]\n\nVentajas: Robusto a valores atípicos, fácil de interpretar\nDesventajas: No diferenciable en cero, trata todos los errores por igual\nCuándo usar: Cuando hay valores atípicos o todos los errores tienen igual importancia\n\n\n\nError Porcentual Absoluto Medio (MAPE)\nEl MAPE expresa el error como porcentaje del valor real:\n\\[MAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\\]\n\nVentajas: Interpretable (% de error), adimensional, útil para comparar modelos en diferentes escalas\nDesventajas: Indefinido cuando \\(y_i = 0\\), asimétrico (penaliza más las sobreestimaciones)\nInterpretación: “Nuestras predicciones se desvían en promedio X% del valor real”\nCuándo usar: Para comparar precisión entre diferentes productos, regiones, o escalas\n\n\n\nError Porcentual Absoluto Medio Simétrico (SMAPE)\nEl SMAPE es una versión simétrica del MAPE:\n\\[SMAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}\\]\n\nVentajas: Simétrico, acotado entre 0% y 200%\nDesventajas: Puede ser contraintuitivo, no tan estándar como MAPE\nCuándo usar: Cuando queremos evitar el sesgo del MAPE hacia sobreestimaciones\n\n\n\nError Logarítmico Cuadrático Medio (MSLE)\nEl MSLE usa transformación logarítmica:\n\\[MSLE = \\frac{1}{n} \\sum_{i=1}^{n} (\\log(1 + y_i) - \\log(1 + \\hat{y}_i))^2\\]\n\nVentajas: Penaliza más las subestimaciones que las sobreestimaciones\nDesventajas: Solo para valores positivos, menos interpretable\nCuándo usar: Cuando subestimar es más costoso que sobreestimar (ej: demanda de inventario)\n\n\n\n\\(R^2\\) Ajustado\nEl \\(R^2\\) ajustado penaliza por el número de variables en el modelo:\n\\[R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\nDonde \\(p\\) es el número de predictores.\n\nVentajas: No aumenta automáticamente al añadir variables\nCuándo usar: Para comparar modelos con diferente número de variables\nInterpretación: Similar a \\(R^2\\) pero más conservador\n\n\n¿Cuál métrica elegir?\nLa elección de métrica depende del contexto del problema:\n\n\n\n\n\n\n\n\nMétrica\nMejor para\nEvitar cuando\n\n\n\n\nRMSE\nErrores grandes son costosos\nHay muchos valores atípicos\n\n\nMAE\nErrores tienen igual importancia\nNecesitas diferenciabilidad\n\n\nMAPE\nComparar diferentes escalas\nHay valores cercanos a cero\n\n\nSMAPE\nComparar con simetría\nInterpretación debe ser simple\n\n\nR²\nExplicar variabilidad\nSolo importa precisión de predicción\n\n\n\n\n\n\n\n\n\nRecomendación práctica\n\n\n\nUsa múltiples métricas para evaluar tu modelo. Una combinación típica sería: - RMSE para precisión general - MAPE para interpretabilidad de negocio\n- R² para explicación de variabilidad",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresión-lineal-múltiple",
    "href": "03-regresion_lineal.html#regresión-lineal-múltiple",
    "title": "Regresión lineal",
    "section": "Regresión Lineal Múltiple",
    "text": "Regresión Lineal Múltiple\nAhora, ¿qué pasa si tenemos múltiples predictores (\\(X_1, X_2, ..., X_p\\))? El modelo se expande:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\\]\nLa intuición es la misma, pero en lugar de ajustar una línea, estamos ajustando un hiperplano en un espacio multidimensional.\nPara manejar esto de forma elegante, usamos notación matricial:\n\\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\nDonde: - \\(\\mathbf{y}\\) es el vector de observaciones. - \\(\\mathbf{X}\\) es la matriz de diseño (con una primera columna de unos para el intercepto). - \\(\\boldsymbol{\\beta}\\) es el vector de coeficientes. - \\(\\boldsymbol{\\epsilon}\\) es el vector de errores.\nLa función de costo en forma matricial es: \\[J(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#transformaciones-comunes-en-modelos-lineales",
    "href": "03-regresion_lineal.html#transformaciones-comunes-en-modelos-lineales",
    "title": "Regresión lineal",
    "section": "Transformaciones Comunes en Modelos Lineales",
    "text": "Transformaciones Comunes en Modelos Lineales\nA veces, la relación entre X e Y no es estrictamente lineal. Las transformaciones logarítmicas nos permiten modelar relaciones no lineales y, además, ofrecen interpretaciones muy útiles en términos de cambios porcentuales.\n\nModelo Log-Nivel (Transformación en Y)\nEste modelo se usa cuando el efecto de X sobre Y no es absoluto, sino porcentual. Por ejemplo, cómo un año más de educación afecta el porcentaje de aumento salarial.\n\nEcuación: \\(\\ln(Y) = \\beta_0 + \\beta_1 X + \\epsilon\\)\nInterpretación: Un incremento de una unidad en \\(X\\) está asociado con un cambio de \\((100 \\cdot \\beta_1)\\%\\) en \\(Y\\).\n\n\n\n\n\n\n\nExplicación Matemática de la Aproximación\n\n\n\n\n\nLa clave está en la propiedad del logaritmo y el cálculo. La derivada de \\(\\ln(Y)\\) con respecto a \\(X\\) es \\(\\beta_1\\): \\[\\frac{d(\\ln(Y))}{dX} = \\beta_1\\] Sabemos que \\(d(\\ln(Y)) = \\frac{dY}{Y}\\). Por tanto: \\[\\frac{dY/Y}{dX} = \\beta_1\\] Para cambios pequeños (o discretos, \\(\\Delta\\)), podemos aproximar los diferenciales: \\[\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X}\\] Si consideramos un cambio unitario en X, \\(\\Delta X = 1\\), entonces: \\[\\beta_1 \\approx \\frac{\\Delta Y}{Y}\\] Esto significa que \\(\\beta_1\\) es la aproximación del cambio porcentual en \\(Y\\) ante un cambio de una unidad en \\(X\\).\n\n\n\n\n\nModelo Nivel-Log (Transformación en X)\nEste modelo es útil cuando el efecto de X sobre Y se reduce a medida que X aumenta (rendimientos decrecientes). Por ejemplo, el efecto de añadir presupuesto de marketing sobre las ventas.\n\nEcuación: \\(Y = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\)\nInterpretación: Un incremento del 1% en \\(X\\) está asociado con un cambio de \\((\\beta_1 / 100)\\) unidades en \\(Y\\).\n\n\n\n\n\n\n\nExplicación Matemática de la Aproximación\n\n\n\n\n\nTomamos la derivada de \\(Y\\) con respecto a \\(\\ln(X)\\): \\[\\frac{dY}{d(\\ln(X))} = \\beta_1\\] Usando la regla de la cadena, sabemos que \\(d(\\ln(X)) = \\frac{dX}{X}\\). Sustituyendo: \\[\\frac{dY}{dX/X} = \\beta_1 \\implies dY = \\beta_1 \\frac{dX}{X}\\] Para cambios discretos, aproximamos: \\[\\Delta Y \\approx \\beta_1 \\frac{\\Delta X}{X}\\] Si consideramos un cambio del 1% en X, entonces \\(\\frac{\\Delta X}{X} = 0.01\\). La ecuación se convierte en: \\[\\Delta Y \\approx \\beta_1 (0.01) = \\frac{\\beta_1}{100}\\] Esto significa que un cambio del 1% en \\(X\\) provoca un cambio de \\(\\beta_1/100\\) unidades en \\(Y\\).\n\n\n\n\n\nModelo Log-Log (Transformación en X e Y)\nEste modelo es muy común en economía y modela la elasticidad constante entre dos variables.\n\nEcuación: \\(\\ln(Y) = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\)\nInterpretación: Un incremento del 1% en \\(X\\) está asociado con un cambio del \\(\\beta_1\\%\\) en \\(Y\\).\n\n\n\n\n\n\n\nExplicación Matemática de la Aproximación\n\n\n\n\n\nEste caso combina los dos anteriores. \\(\\beta_1\\) es la derivada de \\(\\ln(Y)\\) con respecto a \\(\\ln(X)\\), que es la definición de elasticidad. \\[\\beta_1 = \\frac{d(\\ln(Y))}{d(\\ln(X))}\\] Usando las propiedades del cálculo que vimos antes: \\[\\beta_1 = \\frac{dY/Y}{dX/X}\\] Aproximando para cambios discretos: \\[\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X / X}\\] Esta es la definición de elasticidad: el cambio porcentual en \\(Y\\) dividido por el cambio porcentual en \\(X\\). Por lo tanto, si \\(X\\) cambia en un 1% (\\(\\Delta X / X = 0.01\\)), el cambio porcentual en \\(Y\\) (\\(\\Delta Y / Y\\)) será aproximadamente \\(\\beta_1 \\times 0.01\\), es decir, un \\(\\beta_1\\%\\).",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "03-regresion_lineal.html#regresión-regularizada-penalizada",
    "href": "03-regresion_lineal.html#regresión-regularizada-penalizada",
    "title": "Regresión lineal",
    "section": "Regresión Regularizada (Penalizada) 🎯",
    "text": "Regresión Regularizada (Penalizada) 🎯\nHasta ahora hemos visto la regresión lineal clásica, pero ¿qué pasa cuando tenemos muchas variables o cuando nuestro modelo sufre de sobreajuste? Aquí es donde entran las técnicas de regularización.\n\n¿Por qué necesitamos regularización?\nLa regresión lineal ordinaria (OLS) puede presentar varios problemas:\n\nSobreajuste: Cuando tenemos muchas variables relativas al número de observaciones\nMulticolinealidad: Variables predictoras altamente correlacionadas\nInestabilidad: Pequeños cambios en los datos causan grandes cambios en los coeficientes\nInterpretabilidad: Demasiadas variables hacen difícil entender el modelo\n\nLa regularización añade una penalización a la función de costo para controlar la complejidad del modelo.\n\n\n\nRidge Regression (Regresión Ridge) 🏔️\nLa regresión Ridge añade una penalización L2 (suma de cuadrados) a los coeficientes:\n\\[J_{Ridge}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\\]\nDonde: - \\(\\lambda &gt; 0\\) es el parámetro de regularización - \\(\\sum_{j=1}^{p} \\beta_j^2\\) es la penalización L2\n\nCaracterísticas de Ridge:\n✅ Ventajas: - Reduce el sobreajuste - Maneja bien la multicolinealidad - Siempre tiene solución única - Estabiliza los coeficientes\n❌ Desventajas: - NO elimina variables (coeficientes nunca son exactamente cero) - Dificulta la interpretabilidad - Requiere estandarizar las variables\n\n\nSolución Analítica:\n\\[\\hat{\\boldsymbol{\\beta}}_{Ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nEl término \\(\\lambda\\mathbf{I}\\) hace que la matriz sea invertible incluso con multicolinealidad.\n\n\n¿Cómo elegir λ?\n\nλ = 0: Regresión ordinaria (sin penalización)\nλ → ∞: Todos los coeficientes → 0\nλ óptimo: Se encuentra usando validación cruzada\n\n\n\n\n\nLasso Regression (Least Absolute Shrinkage and Selection Operator) ✂️\nLa regresión Lasso usa penalización L1 (suma de valores absolutos):\n\\[J_{Lasso}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\\]\n\nCaracterísticas de Lasso:\n✅ Ventajas: - Selección automática de variables (coeficientes = 0) - Modelos más interpretables y simples - Útil cuando muchas variables son irrelevantes\n❌ Desventajas: - Puede ser inestable con grupos de variables correlacionadas - Selecciona arbitrariamente entre variables correlacionadas - No tiene solución analítica cerrada\n\n\nLa “Magia” de L1: ¿Por qué produce ceros exactos?\nLa penalización L1 crea una región factible con esquinas puntiagudas. La solución óptima tiende a ocurrir en estas esquinas, donde algunos coeficientes son exactamente cero.\n\n\n\n\n\n\nIntuición Geométrica\n\n\n\n\n\nImagina que estás minimizando una función bajo la restricción de que \\(|\\beta_1| + |\\beta_2| \\leq t\\). Esta restricción forma un diamante en 2D. La función objetivo forma elipses. La solución está donde la elipse más pequeña toca el diamante, y esto frecuentemente ocurre en los vértices (donde \\(\\beta_1 = 0\\) o \\(\\beta_2 = 0\\)).\n\n\n\n\n\n\n\nElastic Net: Lo Mejor de Ambos Mundos 🕸️\nElastic Net combina las penalizaciones L1 y L2:\n\\[J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\\]\nO equivalentemente, con un parámetro de mezcla \\(\\alpha\\):\n\\[J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\left[ \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right]\\]\nDonde: - \\(\\alpha \\in [0,1]\\) controla la mezcla entre L1 y L2 - \\(\\alpha = 0\\): Pure Ridge - \\(\\alpha = 1\\): Pure Lasso - \\(\\alpha = 0.5\\): Igual peso a ambas penalizaciones\n\nCaracterísticas de Elastic Net:\n✅ Ventajas: - Selección de variables como Lasso - Estabilidad como Ridge - Maneja bien grupos de variables correlacionadas - Más flexible que Ridge o Lasso por separado\n❌ Desventajas: - Dos hiperparámetros para ajustar (\\(\\lambda\\) y \\(\\alpha\\)) - Más complejo computacionalmente\n\n\n\n\nComparación Visual: Ridge vs Lasso vs Elastic Net\n\n\n\n\n\n\n\n\n\nAspecto\nRidge\nLasso\nElastic Net\n\n\n\n\nPenalización\nL2: \\(\\sum \\beta_j^2\\)\nL1: \\(\\sum |\\beta_j|\\)\nL1 + L2 combinadas\n\n\nSelección de variables\n❌ No\n✅ Sí\n✅ Sí\n\n\nCoeficientes exactamente cero\n❌ No\n✅ Sí\n✅ Sí\n\n\nManejo de multicolinealidad\n✅ Excelente\n⚠️ Problemático\n✅ Muy bueno\n\n\nEstabilidad\n✅ Alta\n⚠️ Media\n✅ Alta\n\n\nInterpretabilidad\n⚠️ Media\n✅ Alta\n✅ Alta\n\n\nCuando usar\nTodas las variables importan\nPocas variables importantes\nSituaciones mixtas\n\n\n\n\n\n¿Cuándo usar cada método?\n\nUsa Ridge cuando:\n\nCrees que todas las variables contribuyen al modelo\nTienes multicolinealidad severa\nQuieres estabilizar coeficientes sin eliminar variables\nEl número de observaciones es pequeño relativo a variables\n\n\n\nUsa Lasso cuando:\n\nCrees que pocas variables son realmente importantes\nQuieres un modelo simple e interpretable\nNecesitas selección automática de variables\nTienes muchas variables irrelevantes\n\n\n\nUsa Elastic Net cuando:\n\nNo estás seguro de cuántas variables son importantes\nTienes grupos de variables correlacionadas\nQuieres balancear selección y estabilidad\nEs tu primera opción cuando no conoces la estructura de los datos\n\n\n\n\n\nValidación de Modelos y Selección de Hiperparámetros\n\n¿Por qué necesitamos dividir nuestros datos?\nCuando construimos modelos de machine learning, enfrentamos un dilema fundamental: ¿cómo sabemos si nuestro modelo funcionará bien con datos nuevos?\n\nEl Problema del Sobreajuste\nImagina que estás preparándote para un examen. Si solo estudias las preguntas exactas que aparecerán en el examen, podrías obtener una calificación perfecta. Pero si las preguntas cambian ligeramente, tu rendimiento se desplomaría. Esto es sobreajuste: el modelo memoriza los datos de entrenamiento pero no generaliza.\n\n\n\nDivisión Típica de Datos: Entrenamiento/Validación/Prueba\nLa estrategia estándar es dividir nuestros datos en tres conjuntos:\n📊 Dataset Completo (100%)\n├── 🏋️ Entrenamiento (60%) - Para ajustar coeficientes\n├── 🎯 Validación (20%)     - Para seleccionar hiperparámetros  \n└── 🧪 Prueba (20%)         - Para evaluación final\n\nConjunto de Entrenamiento (60%)\n\nPropósito: Ajustar los coeficientes \\(\\beta\\) del modelo\nAnalogía: Los ejercicios que haces para aprender\n\n\n\nConjunto de Validación (20%)\n\nPropósito: Comparar diferentes hiperparámetros (como \\(\\lambda\\) en Ridge/Lasso)\nAnalogía: Exámenes de práctica para decidir qué estrategia de estudio funciona mejor\n\n\n\nConjunto de Prueba (20%)\n\nPropósito: Evaluación final y honesta del modelo\nAnalogía: El examen final real\n⚠️ Regla de Oro: ¡Solo se usa UNA vez al final!\n\n\n\n\n¿Qué pasa si tenemos pocos datos?\nCuando nuestro dataset es pequeño (&lt; 1000 observaciones), dividir en tres partes puede ser problemático:\n❌ Problemas con datasets pequeños: - Conjunto de entrenamiento muy pequeño → modelo pobre - Conjunto de validación pequeño → selección inestable de hiperparámetros - Conjunto de prueba pequeño → evaluación poco confiable\nSolución: ¡Validación Cruzada!\n\n\n\nValidación Cruzada (Cross-Validation)\nLa validación cruzada es una técnica que maximiza el uso de nuestros datos limitados. En lugar de usar una sola división, usamos múltiples divisiones.\n\nValidación Cruzada k-fold\nEl método más común es k-fold cross-validation:\n\nDividir el dataset en \\(k\\) “pliegues” (folds) de igual tamaño\nRepetir \\(k\\) veces:\n\nUsar \\(k-1\\) pliegues para entrenamiento\nUsar 1 pliegue para validación\n\nPromediar los resultados de las \\(k\\) evaluaciones\n\n\n\n\n\n\nVisualización de 5-Fold Cross Validation mostrando cómo se dividen los datos en cada iteración\n\n\n\n\n\n\nVentajas de la Validación Cruzada\n✅ Maximiza el uso de datos: Cada observación se usa tanto para entrenamiento como validación\n✅ Estimación más robusta: Promedia múltiples evaluaciones independientes\n✅ Reduce la varianza: Menos dependiente de una división particular\n✅ Detecta inestabilidad: Si los resultados varían mucho entre folds, el modelo es inestable\n\n\n\nValidación Cruzada para Selección de Hiperparámetros\nEn regresión regularizada, usamos CV para encontrar el mejor \\(\\lambda\\):\n\n\n🎯 SELECCIÓN DE HIPERPARÁMETROS CON VALIDACIÓN CRUZADA\n============================================================\nPara cada valor de λ:\n  1. Aplicar 5-fold CV\n  2. Calcular error promedio\n  3. Seleccionar λ con menor error\n\n\n\n\n\nCurva de validación mostrando cómo seleccionar el hiperparámetro óptimo λ usando validación cruzada\n\n\n\n\n\n📈 Resultado: λ óptimo = 0.1274\n📉 Error de CV mínimo = 0.4776\n\n\n\n\nProceso Completo de Validación\nEl flujo completo para modelos regularizados es:\n1. 📊 Dividir datos originales\n   └── 80% para desarrollo (entrenamiento + validación)\n   └── 20% para prueba final (¡NO TOCAR hasta el final!)\n\n2. 🔄 En el conjunto de desarrollo:\n   └── Para cada λ candidato:\n       ├── Aplicar k-fold CV\n       ├── Calcular error promedio\n       └── Guardar resultado\n\n3. 🎯 Seleccionar λ con menor error de CV\n\n4. 🏗️ Entrenar modelo final con λ óptimo en TODO el conjunto de desarrollo\n\n5. 🧪 Evaluación final en conjunto de prueba\n\n\nVariantes de Validación Cruzada\n\nLeave-One-Out CV (LOOCV)\n\nk = n (número de observaciones)\nVentaja: Máximo uso de datos para entrenamiento\nDesventaja: Computacionalmente costoso, alta varianza\n\n\n\nStratified CV\n\nPara problemas de clasificación\nMantiene la proporción de clases en cada fold\n\n\n\nTime Series CV\n\nPara datos temporales\nRespeta el orden temporal (no mezcla futuro con pasado)",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html",
    "href": "violaciones_supuestos_regresion.html",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "",
    "text": "1. Violación del Supuesto de Linealidad\nEste notebook demuestra gráficamente cómo pueden violarse los cuatro supuestos principales de la regresión lineal:\nEl supuesto de linealidad requiere que la relación entre X e Y sea lineal. Cuando esto se viola, un modelo lineal será inadecuado.\ndef crear_datos_no_lineales(n=200, tipo='cuadratico', noise=1.0, seed=42):\n    \"\"\"Genera datos con relaciones no lineales\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(-3, 3, n)\n    \n    if tipo == 'cuadratico':\n        y = 2 * x**2 + 1 + np.random.normal(0, noise, n)\n        titulo = \"Relación Cuadrática\"\n    elif tipo == 'exponencial':\n        y = np.exp(x/2) + np.random.normal(0, noise, n)\n        titulo = \"Relación Exponencial\"\n    elif tipo == 'sinusoidal':\n        y = 5 * np.sin(2*x) + x + np.random.normal(0, noise, n)\n        titulo = \"Relación Sinusoidal\"\n    elif tipo == 'logaritmico':\n        x = np.linspace(0.1, 10, n)\n        y = 3 * np.log(x) + 2 + np.random.normal(0, noise, n)\n        titulo = \"Relación Logarítmica\"\n    \n    return x, y, titulo\n\n# Crear subplots para diferentes tipos de no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Violaciones del Supuesto de Linealidad', fontsize=16, fontweight='bold')\n\ntipos = ['cuadratico', 'exponencial', 'sinusoidal', 'logaritmico']\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    \n    # Gráfico\n    axes[row, col].scatter(x, y, alpha=0.6, label='Datos reales')\n    axes[row, col].plot(x, y_pred, 'r-', linewidth=2, label='Regresión lineal')\n    axes[row, col].set_title(titulo)\n    axes[row, col].set_xlabel('X')\n    axes[row, col].set_ylabel('Y')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n    \n    # Calcular R²\n    r2 = lr.score(X_reshaped, y)\n    axes[row, col].text(0.05, 0.95, f'R² = {r2:.3f}', \n                        transform=axes[row, col].transAxes, \n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-linealidad",
    "href": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-linealidad",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "",
    "text": "Diagnóstico: Gráficos de Residuos vs Valores Predichos\nUna forma de detectar no linealidad es examinar los residuos. Si hay patrones en los residuos, indica problemas con el modelo.\n\n# Crear gráficos de residuos para detectar no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Gráficos de Residuos: Detectando No Linealidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    residuos = y - y_pred\n    \n    # Gráfico de residuos\n    axes[row, col].scatter(y_pred, residuos, alpha=0.6)\n    axes[row, col].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[row, col].set_title(f'Residuos - {titulo}')\n    axes[row, col].set_xlabel('Valores Predichos')\n    axes[row, col].set_ylabel('Residuos')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 INTERPRETACIÓN:\")\nprint(\"✅ Residuos aleatorios alrededor de 0 = Linealidad OK\")\nprint(\"❌ Patrones en residuos = Violación de linealidad\")\nprint(\"   - Curva: Relación cuadrática o polinomial\")\nprint(\"   - Tendencia: Relación exponencial o logarítmica\")\nprint(\"   - Oscilación: Efectos cíclicos o sinusoidales\")\n\n\n\n\n\n\n\n\n📊 INTERPRETACIÓN:\n✅ Residuos aleatorios alrededor de 0 = Linealidad OK\n❌ Patrones en residuos = Violación de linealidad\n   - Curva: Relación cuadrática o polinomial\n   - Tendencia: Relación exponencial o logarítmica\n   - Oscilación: Efectos cíclicos o sinusoidales",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#solución",
    "href": "violaciones_supuestos_regresion.html#solución",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "Solución",
    "text": "Solución\n\n# Crear subplots para diferentes tipos de no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Violaciones del Supuesto de Linealidad', fontsize=16, fontweight='bold')\n\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    if tipo == \"cuadratico\":\n        X_reshaped = X_reshaped**2\n    if tipo == \"exponencial\":\n        X_reshaped = np.exp(X_reshaped)\n    if tipo == \"sinusoidal\":\n        X_reshaped = np.concatenate((np.sin(2*X_reshaped), X_reshaped), axis = 1)\n    if tipo == \"logaritmico\":\n        X_reshaped = np.log(X_reshaped)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    \n    # Gráfico\n    axes[row, col].scatter(x, y, alpha=0.6, label='Datos reales')\n    axes[row, col].plot(x, y_pred, 'r-', linewidth=2, label='Regresión lineal')\n    axes[row, col].set_title(titulo)\n    axes[row, col].set_xlabel('X')\n    axes[row, col].set_ylabel('Y')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n    \n    # Calcular R²\n    r2 = lr.score(X_reshaped, y)\n    axes[row, col].text(0.05, 0.95, f'R² = {r2:.3f}', \n                        transform=axes[row, col].transAxes, \n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Crear gráficos de residuos para detectar no linealidad\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Gráficos de Residuos: Detectando No Linealidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos):\n    row = i // 2\n    col = i % 2\n    \n    x, y, titulo = crear_datos_no_lineales(tipo=tipo, noise=1.5)\n    \n    # Ajustar modelo lineal\n    lr = LinearRegression()\n    X_reshaped = x.reshape(-1, 1)\n    if tipo == \"cuadratico\":\n        X_reshaped = X_reshaped**2\n    if tipo == \"exponencial\":\n        X_reshaped = np.exp(X_reshaped)\n    if tipo == \"sinusoidal\":\n        X_reshaped = np.concatenate((np.sin(2*X_reshaped), X_reshaped), axis = 1)\n    if tipo == \"logaritmico\":\n        X_reshaped = np.log(X_reshaped)\n    lr.fit(X_reshaped, y)\n    y_pred = lr.predict(X_reshaped)\n    residuos = y - y_pred\n    \n    # Gráfico de residuos\n    axes[row, col].scatter(y_pred, residuos, alpha=0.6)\n    axes[row, col].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[row, col].set_title(f'Residuos - {titulo}')\n    axes[row, col].set_xlabel('Valores Predichos')\n    axes[row, col].set_ylabel('Residuos')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-independencia",
    "href": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-independencia",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "2. Violación del Supuesto de Independencia",
    "text": "2. Violación del Supuesto de Independencia\nLos errores deben ser independientes entre sí. Esto es especialmente importante en datos de series temporales.\n\ndef crear_datos_correlacionados(n=200, correlacion=0.7, seed=42):\n    \"\"\"Genera datos con errores correlacionados (autocorrelación)\"\"\"\n    np.random.seed(seed)\n    \n    # Datos independientes (correctos)\n    x = np.linspace(0, 10, n)\n    errores_independientes = np.random.normal(0, 2, n)\n    y_independiente = 2 + 1.5 * x + errores_independientes\n    \n    # Datos con autocorrelación (violación)\n    errores_correlacionados = np.zeros(n)\n    errores_correlacionados[0] = np.random.normal(0, 2)\n    \n    for i in range(1, n):\n        errores_correlacionados[i] = (correlacion * errores_correlacionados[i-1] + \n                                     np.sqrt(1 - correlacion**2) * np.random.normal(0, 2))\n    \n    y_correlacionado = 2 + 1.5 * x + errores_correlacionados\n    \n    return x, y_independiente, y_correlacionado, errores_independientes, errores_correlacionados\n\n# Generar datos\nx, y_indep, y_corr, err_indep, err_corr = crear_datos_correlacionados()\n\n# Ajustar modelos\nlr_indep = LinearRegression().fit(x.reshape(-1, 1), y_indep)\nlr_corr = LinearRegression().fit(x.reshape(-1, 1), y_corr)\n\ny_pred_indep = lr_indep.predict(x.reshape(-1, 1))\ny_pred_corr = lr_corr.predict(x.reshape(-1, 1))\n\nresiduos_indep = y_indep - y_pred_indep\nresiduos_corr = y_corr - y_pred_corr\n\n# Crear gráficos\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Violación del Supuesto de Independencia', fontsize=16, fontweight='bold')\n\n# Fila 1: Datos independientes (correctos)\naxes[0, 0].scatter(x, y_indep, alpha=0.6, color='blue')\naxes[0, 0].plot(x, y_pred_indep, 'r-', linewidth=2)\naxes[0, 0].set_title('Datos con Errores Independientes ✅')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('Y')\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(residuos_indep, 'o-', alpha=0.7, color='blue')\naxes[0, 1].axhline(y=0, color='red', linestyle='--')\naxes[0, 1].set_title('Residuos vs Tiempo')\naxes[0, 1].set_xlabel('Observación')\naxes[0, 1].set_ylabel('Residuos')\naxes[0, 1].grid(True, alpha=0.3)\n\naxes[0, 2].scatter(residuos_indep[:-1], residuos_indep[1:], alpha=0.6, color='blue')\naxes[0, 2].set_title('Residuos(t) vs Residuos(t-1)')\naxes[0, 2].set_xlabel('Residuo en t-1')\naxes[0, 2].set_ylabel('Residuo en t')\naxes[0, 2].grid(True, alpha=0.3)\n\n# Calcular correlación\ncorr_indep = np.corrcoef(residuos_indep[:-1], residuos_indep[1:])[0, 1]\naxes[0, 2].text(0.05, 0.95, f'r = {corr_indep:.3f}', \n                transform=axes[0, 2].transAxes,\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n# Fila 2: Datos correlacionados (violación)\naxes[1, 0].scatter(x, y_corr, alpha=0.6, color='orange')\naxes[1, 0].plot(x, y_pred_corr, 'r-', linewidth=2)\naxes[1, 0].set_title('Datos con Errores Correlacionados ❌')\naxes[1, 0].set_xlabel('X')\naxes[1, 0].set_ylabel('Y')\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(residuos_corr, 'o-', alpha=0.7, color='orange')\naxes[1, 1].axhline(y=0, color='red', linestyle='--')\naxes[1, 1].set_title('Residuos vs Tiempo')\naxes[1, 1].set_xlabel('Observación')\naxes[1, 1].set_ylabel('Residuos')\naxes[1, 1].grid(True, alpha=0.3)\n\naxes[1, 2].scatter(residuos_corr[:-1], residuos_corr[1:], alpha=0.6, color='orange')\naxes[1, 2].set_title('Residuos(t) vs Residuos(t-1)')\naxes[1, 2].set_xlabel('Residuo en t-1')\naxes[1, 2].set_ylabel('Residuo en t')\naxes[1, 2].grid(True, alpha=0.3)\n\n# Calcular correlación\ncorr_corr = np.corrcoef(residuos_corr[:-1], residuos_corr[1:])[0, 1]\naxes[1, 2].text(0.05, 0.95, f'r = {corr_corr:.3f}', \n                transform=axes[1, 2].transAxes,\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 INTERPRETACIÓN:\")\nprint(f\"✅ Errores independientes: correlación = {corr_indep:.3f} (cerca de 0)\")\nprint(f\"❌ Errores correlacionados: correlación = {corr_corr:.3f} (lejos de 0)\")\nprint(\"\\n🔍 DIAGNÓSTICO:\")\nprint(\"- Gráfico temporal: patrones o tendencias en residuos\")\nprint(\"- Autocorrelación: correlación significativa entre residuos consecutivos\")\nprint(\"- Prueba Durbin-Watson: estadístico cerca de 2 = independencia\")\n\n\n\n\n\n\n\n\n📊 INTERPRETACIÓN:\n✅ Errores independientes: correlación = -0.058 (cerca de 0)\n❌ Errores correlacionados: correlación = 0.670 (lejos de 0)\n\n🔍 DIAGNÓSTICO:\n- Gráfico temporal: patrones o tendencias en residuos\n- Autocorrelación: correlación significativa entre residuos consecutivos\n- Prueba Durbin-Watson: estadístico cerca de 2 = independencia",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-homocedasticidad",
    "href": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-homocedasticidad",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "3. Violación del Supuesto de Homocedasticidad",
    "text": "3. Violación del Supuesto de Homocedasticidad\nLa varianza de los errores debe ser constante. Cuando varía, tenemos heterocedasticidad.\n\ndef crear_datos_heteroscedasticos(n=200, tipo='creciente', seed=42):\n    \"\"\"Genera datos con diferentes tipos de heterocedasticidad\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(1, 10, n)\n    \n    if tipo == 'homoscedastico':\n        # Varianza constante (correcto)\n        error = np.random.normal(0, 2, n)\n        titulo = \"Homocedasticidad (Correcto) ✅\"\n    elif tipo == 'creciente':\n        # Varianza aumenta con X\n        error = np.random.normal(0, 0.5 * x, n)\n        titulo = \"Heterocedasticidad Creciente ❌\"\n    elif tipo == 'decreciente':\n        # Varianza disminuye con X\n        error = np.random.normal(0, 5 / x, n)\n        titulo = \"Heterocedasticidad Decreciente ❌\"\n    elif tipo == 'embudo':\n        # Forma de embudo\n        error = np.random.normal(0, 0.1 + 0.8 * np.abs(x - 5.5), n)\n        titulo = \"Heterocedasticidad en Embudo ❌\"\n    \n    y = 2 + 1.5 * x + error\n    return x, y, titulo\n\n# Crear diferentes tipos de heterocedasticidad\ntipos_het = ['homoscedastico', 'creciente', 'decreciente', 'embudo']\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\nfig.suptitle('Violación del Supuesto de Homocedasticidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos_het):\n    x, y, titulo = crear_datos_heteroscedasticos(tipo=tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    # Gráfico de datos\n    color = 'green' if tipo == 'homoscedastico' else 'red'\n    axes[0, i].scatter(x, y, alpha=0.6, color=color)\n    axes[0, i].plot(x, y_pred, 'black', linewidth=2)\n    axes[0, i].set_title(titulo)\n    axes[0, i].set_xlabel('X')\n    axes[0, i].set_ylabel('Y')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Gráfico de residuos\n    axes[1, i].scatter(y_pred, residuos, alpha=0.6, color=color)\n    axes[1, i].axhline(y=0, color='black', linestyle='--')\n    axes[1, i].set_title(f'Residuos vs Predichos')\n    axes[1, i].set_xlabel('Valores Predichos')\n    axes[1, i].set_ylabel('Residuos')\n    axes[1, i].grid(True, alpha=0.3)\n    \n    # Añadir líneas de tendencia en residuos para visualizar heterocedasticidad\n    if tipo != 'homoscedastico':\n        z = np.polyfit(y_pred, np.abs(residuos), 1)\n        p = np.poly1d(z)\n        axes[1, i].plot(y_pred, p(y_pred), \"r--\", alpha=0.8)\n        axes[1, i].plot(y_pred, -p(y_pred), \"r--\", alpha=0.8)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 INTERPRETACIÓN:\")\nprint(\"✅ Homocedasticidad: Residuos con dispersión constante\")\nprint(\"❌ Heterocedasticidad: Residuos con dispersión variable\")\nprint(\"\\n🔍 PATRONES COMUNES:\")\nprint(\"- Embudo creciente: Varianza aumenta con valores predichos\")\nprint(\"- Embudo decreciente: Varianza disminuye con valores predichos\")\nprint(\"- Forma de diamante: Varianza máxima en valores medios\")\n\n\n\n\n\n\n\n\n📊 INTERPRETACIÓN:\n✅ Homocedasticidad: Residuos con dispersión constante\n❌ Heterocedasticidad: Residuos con dispersión variable\n\n🔍 PATRONES COMUNES:\n- Embudo creciente: Varianza aumenta con valores predichos\n- Embudo decreciente: Varianza disminuye con valores predichos\n- Forma de diamante: Varianza máxima en valores medios\n\n\n\nPrueba Estadística: Test de Breusch-Pagan\nUna prueba formal para detectar heterocedasticidad.\n\nfrom scipy.stats import chi2\n\ndef breusch_pagan_test(residuos, x):\n    \"\"\"Implementa el test de Breusch-Pagan para heterocedasticidad\"\"\"\n    n = len(residuos)\n    \n    # Regresión de residuos al cuadrado sobre X\n    residuos_cuadrados = residuos**2\n    lr_bp = LinearRegression()\n    lr_bp.fit(x.reshape(-1, 1), residuos_cuadrados)\n    \n    # R² de la regresión auxiliar\n    r2_bp = lr_bp.score(x.reshape(-1, 1), residuos_cuadrados)\n    \n    # Estadístico de prueba\n    lm_statistic = n * r2_bp\n    \n    # P-value (distribución chi-cuadrado con 1 grado de libertad)\n    p_value = 1 - chi2.cdf(lm_statistic, df=1)\n    \n    return lm_statistic, p_value\n\nprint(\"🧪 TEST DE BREUSCH-PAGAN PARA HETEROCEDASTICIDAD\\n\")\nprint(\"H₀: Homocedasticidad (varianza constante)\")\nprint(\"H₁: Heterocedasticidad (varianza no constante)\\n\")\n\nfor tipo in tipos_het:\n    x, y, titulo = crear_datos_heteroscedasticos(tipo=tipo)\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    residuos = y - lr.predict(x.reshape(-1, 1))\n    \n    lm_stat, p_val = breusch_pagan_test(residuos, x)\n    \n    resultado = \"RECHAZA H₀\" if p_val &lt; 0.05 else \"NO RECHAZA H₀\"\n    emoji = \"❌\" if p_val &lt; 0.05 else \"✅\"\n    \n    print(f\"{emoji} {tipo.upper():15} | LM = {lm_stat:6.2f} | p-value = {p_val:.4f} | {resultado}\")\n\nprint(\"\\n💡 Interpretación: p-value &lt; 0.05 indica heterocedasticidad\")\n\n🧪 TEST DE BREUSCH-PAGAN PARA HETEROCEDASTICIDAD\n\nH₀: Homocedasticidad (varianza constante)\nH₁: Heterocedasticidad (varianza no constante)\n\n✅ HOMOSCEDASTICO  | LM =   0.00 | p-value = 0.9588 | NO RECHAZA H₀\n❌ CRECIENTE       | LM =  27.79 | p-value = 0.0000 | RECHAZA H₀\n❌ DECRECIENTE     | LM =  31.91 | p-value = 0.0000 | RECHAZA H₀\n✅ EMBUDO          | LM =   0.14 | p-value = 0.7122 | NO RECHAZA H₀\n\n💡 Interpretación: p-value &lt; 0.05 indica heterocedasticidad",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-normalidad",
    "href": "violaciones_supuestos_regresion.html#violación-del-supuesto-de-normalidad",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "4. Violación del Supuesto de Normalidad",
    "text": "4. Violación del Supuesto de Normalidad\nLos errores deben seguir una distribución normal. Esto es crucial para las pruebas de hipótesis y intervalos de confianza.\n\ndef crear_datos_no_normales(n=200, tipo='normal', seed=42):\n    \"\"\"Genera datos con diferentes distribuciones de error\"\"\"\n    np.random.seed(seed)\n    x = np.linspace(0, 10, n)\n    \n    if tipo == 'normal':\n        error = np.random.normal(0, 2, n)\n        titulo = \"Errores Normales ✅\"\n    elif tipo == 'asimetrico':\n        # Distribución asimétrica (exponencial)\n        error = np.random.exponential(2, n) - 2\n        titulo = \"Errores Asimétricos ❌\"\n    elif tipo == 'colas_pesadas':\n        # Distribución t con colas pesadas\n        error = stats.t.rvs(df=3, scale=2, size=n)\n        titulo = \"Errores con Colas Pesadas ❌\"\n    elif tipo == 'bimodal':\n        # Distribución bimodal\n        mask = np.random.binomial(1, 0.5, n).astype(bool)\n        error = np.where(mask, \n                        np.random.normal(-2, 1, n),\n                        np.random.normal(2, 1, n))\n        titulo = \"Errores Bimodales ❌\"\n    \n    y = 2 + 1.5 * x + error\n    return x, y, error, titulo\n\n# Crear diferentes tipos de no normalidad\ntipos_norm = ['normal', 'asimetrico', 'colas_pesadas', 'bimodal']\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\nfig.suptitle('Violación del Supuesto de Normalidad', fontsize=16, fontweight='bold')\n\nfor i, tipo in enumerate(tipos_norm):\n    x, y, error_real, titulo = crear_datos_no_normales(tipo=tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    color = 'green' if tipo == 'normal' else 'red'\n    \n    # Fila 1: Datos originales\n    axes[0, i].scatter(x, y, alpha=0.6, color=color)\n    axes[0, i].plot(x, y_pred, 'black', linewidth=2)\n    axes[0, i].set_title(titulo)\n    axes[0, i].set_xlabel('X')\n    axes[0, i].set_ylabel('Y')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Fila 2: Histograma de residuos\n    axes[1, i].hist(residuos, bins=25, density=True, alpha=0.7, color=color, edgecolor='black')\n    \n    # Superponer distribución normal teórica\n    x_norm = np.linspace(residuos.min(), residuos.max(), 100)\n    y_norm = stats.norm.pdf(x_norm, residuos.mean(), residuos.std())\n    axes[1, i].plot(x_norm, y_norm, 'blue', linewidth=2, label='Normal teórica')\n    \n    axes[1, i].set_title(f'Histograma de Residuos')\n    axes[1, i].set_xlabel('Residuos')\n    axes[1, i].set_ylabel('Densidad')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n    \n    # Fila 3: Q-Q plot\n    stats.probplot(residuos, dist=\"norm\", plot=axes[2, i])\n    axes[2, i].set_title(f'Q-Q Plot')\n    axes[2, i].grid(True, alpha=0.3)\n    \n    # Calcular test de normalidad\n    shapiro_stat, shapiro_p = stats.shapiro(residuos)\n    axes[2, i].text(0.05, 0.95, f'Shapiro p={shapiro_p:.3f}', \n                    transform=axes[2, i].transAxes,\n                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 INTERPRETACIÓN:\")\nprint(\"✅ Q-Q plot lineal + Shapiro p &gt; 0.05 = Normalidad\")\nprint(\"❌ Q-Q plot no lineal + Shapiro p &lt; 0.05 = No normalidad\")\nprint(\"\\n🔍 PATRONES EN Q-Q PLOTS:\")\nprint(\"- Curva S: Asimetría\")\nprint(\"- Colas alejadas de la línea: Colas pesadas/ligeras\")\nprint(\"- Múltiples segmentos: Multimodalidad\")\n\n\n\n\n\n\n\n\n📊 INTERPRETACIÓN:\n✅ Q-Q plot lineal + Shapiro p &gt; 0.05 = Normalidad\n❌ Q-Q plot no lineal + Shapiro p &lt; 0.05 = No normalidad\n\n🔍 PATRONES EN Q-Q PLOTS:\n- Curva S: Asimetría\n- Colas alejadas de la línea: Colas pesadas/ligeras\n- Múltiples segmentos: Multimodalidad\n\n\n\nResumen de Pruebas Estadísticas para Normalidad\n\nfrom scipy.stats import jarque_bera, anderson\n\nprint(\"🧪 PRUEBAS DE NORMALIDAD EN RESIDUOS\\n\")\nprint(\"H₀: Los residuos siguen distribución normal\")\nprint(\"H₁: Los residuos NO siguen distribución normal\\n\")\nprint(f\"{'Tipo':15} | {'Shapiro':&gt;10} | {'Jarque-Bera':&gt;12} | {'Anderson':&gt;10} | {'Conclusión'}\")\nprint(\"-\" * 75)\n\nfor tipo in tipos_norm:\n    x, y, _, titulo = crear_datos_no_normales(tipo=tipo)\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    residuos = y - lr.predict(x.reshape(-1, 1))\n    \n    # Shapiro-Wilk test\n    shapiro_stat, shapiro_p = stats.shapiro(residuos)\n    \n    # Jarque-Bera test\n    jb_stat, jb_p = jarque_bera(residuos)\n    \n    # Anderson-Darling test\n    ad_stat, ad_critical, ad_significance = anderson(residuos, dist='norm')\n    ad_result = \"Rechaza\" if ad_stat &gt; ad_critical[2] else \"No rechaza\"  # 5% nivel\n    \n    # Conclusión general\n    tests_reject = sum([shapiro_p &lt; 0.05, jb_p &lt; 0.05, ad_stat &gt; ad_critical[2]])\n    \n    if tests_reject &gt;= 2:\n        conclusion = \"❌ No Normal\"\n    elif tests_reject == 1:\n        conclusion = \"⚠️  Dudoso\"\n    else:\n        conclusion = \"✅ Normal\"\n    \n    print(f\"{tipo:15} | {shapiro_p:10.4f} | {jb_p:12.4f} | {ad_result:&gt;10} | {conclusion}\")\n\nprint(\"\\n💡 Interpretación: p-value &lt; 0.05 indica violación de normalidad\")\nprint(\"💡 Anderson-Darling: Estadístico &gt; valor crítico indica no normalidad\")\n\n🧪 PRUEBAS DE NORMALIDAD EN RESIDUOS\n\nH₀: Los residuos siguen distribución normal\nH₁: Los residuos NO siguen distribución normal\n\nTipo            |    Shapiro |  Jarque-Bera |   Anderson | Conclusión\n---------------------------------------------------------------------------\nnormal          |     0.7354 |       0.7293 | No rechaza | ✅ Normal\nasimetrico      |     0.0000 |       0.0000 |    Rechaza | ❌ No Normal\ncolas_pesadas   |     0.0003 |       0.0000 |    Rechaza | ❌ No Normal\nbimodal         |     0.0000 |       0.0010 |    Rechaza | ❌ No Normal\n\n💡 Interpretación: p-value &lt; 0.05 indica violación de normalidad\n💡 Anderson-Darling: Estadístico &gt; valor crítico indica no normalidad",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#consecuencias-de-violar-los-supuestos",
    "href": "violaciones_supuestos_regresion.html#consecuencias-de-violar-los-supuestos",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "5. Consecuencias de Violar los Supuestos",
    "text": "5. Consecuencias de Violar los Supuestos\nVamos a demostrar qué pasa cuando violamos cada supuesto en términos de: - Precisión de las predicciones - Validez de los intervalos de confianza - Confiabilidad de las pruebas de hipótesis\n\ndef simular_consecuencias(n_simulaciones=1000, n_datos=100):\n    \"\"\"Simula las consecuencias de violar supuestos\"\"\"\n    \n    resultados = {\n        'correcto': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'no_lineal': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'heteroscedastico': {'mse': [], 'coef_estimados': [], 'p_values': []},\n        'no_normal': {'mse': [], 'coef_estimados': [], 'p_values': []}\n    }\n    \n    coef_verdadero = 1.5  # Coeficiente real\n    \n    for _ in range(n_simulaciones):\n        x = np.linspace(0, 10, n_datos)\n        X = x.reshape(-1, 1)\n        \n        # Caso correcto\n        y_correcto = 2 + coef_verdadero * x + np.random.normal(0, 1, n_datos)\n        lr_correcto = LinearRegression().fit(X, y_correcto)\n        \n        # Caso no lineal (usando modelo lineal en datos cuadráticos)\n        y_no_lineal = 2 + 0.5 * x**2 + np.random.normal(0, 1, n_datos)\n        lr_no_lineal = LinearRegression().fit(X, y_no_lineal)\n        \n        # Caso heteroscedástico\n        y_hetero = 2 + coef_verdadero * x + np.random.normal(0, 0.1 + 0.2 * x, n_datos)\n        lr_hetero = LinearRegression().fit(X, y_hetero)\n        \n        # Caso no normal (distribución t)\n        y_no_normal = 2 + coef_verdadero * x + stats.t.rvs(df=3, scale=1, size=n_datos)\n        lr_no_normal = LinearRegression().fit(X, y_no_normal)\n        \n        # Calcular métricas para datos de prueba\n        x_test = np.linspace(0, 10, 50).reshape(-1, 1)\n        y_test_verdadero = 2 + coef_verdadero * x_test.flatten()\n        \n        # MSE en datos de prueba\n        resultados['correcto']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_correcto.predict(x_test))\n        )\n        resultados['no_lineal']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_no_lineal.predict(x_test))\n        )\n        resultados['heteroscedastico']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_hetero.predict(x_test))\n        )\n        resultados['no_normal']['mse'].append(\n            mean_squared_error(y_test_verdadero, lr_no_normal.predict(x_test))\n        )\n        \n        # Coeficientes estimados\n        resultados['correcto']['coef_estimados'].append(lr_correcto.coef_[0])\n        resultados['no_lineal']['coef_estimados'].append(lr_no_lineal.coef_[0])\n        resultados['heteroscedastico']['coef_estimados'].append(lr_hetero.coef_[0])\n        resultados['no_normal']['coef_estimados'].append(lr_no_normal.coef_[0])\n    \n    return resultados\n\nprint(\"🔄 Ejecutando simulación (esto puede tomar unos segundos...)\")\nresultados_sim = simular_consecuencias(n_simulaciones=500)\n\n# Crear gráficos de resultados\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Consecuencias de Violar Supuestos de Regresión', fontsize=16, fontweight='bold')\n\ntipos_casos = ['correcto', 'no_lineal', 'heteroscedastico', 'no_normal']\ncolores = ['green', 'red', 'orange', 'purple']\nnombres = ['Correcto ✅', 'No Lineal ❌', 'Heteroscedástico ❌', 'No Normal ❌']\n\n# MSE Distribución\nmse_data = [resultados_sim[caso]['mse'] for caso in tipos_casos]\naxes[0, 0].boxplot(mse_data, labels=nombres)\naxes[0, 0].set_title('Distribución del Error de Predicción (MSE)')\naxes[0, 0].set_ylabel('MSE')\naxes[0, 0].tick_params(axis='x', rotation=45)\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribución de coeficientes estimados\ncoef_data = [resultados_sim[caso]['coef_estimados'] for caso in tipos_casos]\naxes[0, 1].boxplot(coef_data, labels=nombres)\naxes[0, 1].axhline(y=1.5, color='black', linestyle='--', label='Valor verdadero')\naxes[0, 1].set_title('Distribución de Coeficientes Estimados')\naxes[0, 1].set_ylabel('Coeficiente β₁')\naxes[0, 1].tick_params(axis='x', rotation=45)\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Histograma comparativo de MSE\nfor i, (caso, color, nombre) in enumerate(zip(tipos_casos, colores, nombres)):\n    axes[1, 0].hist(resultados_sim[caso]['mse'], bins=30, alpha=0.6, \n                    color=color, label=nombre, density=True)\naxes[1, 0].set_title('Comparación de Distribuciones de MSE')\naxes[1, 0].set_xlabel('MSE')\naxes[1, 0].set_ylabel('Densidad')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Estadísticas resumidas\naxes[1, 1].axis('off')\ntabla_texto = \"RESUMEN ESTADÍSTICO:\\n\\n\"\ntabla_texto += f\"{'Caso':15} | {'MSE Promedio':&gt;12} | {'Sesgo β₁':&gt;10}\\n\"\ntabla_texto += \"-\" * 45 + \"\\n\"\n\nfor caso, nombre in zip(tipos_casos, nombres):\n    mse_prom = np.mean(resultados_sim[caso]['mse'])\n    sesgo = np.mean(resultados_sim[caso]['coef_estimados']) - 1.5\n    tabla_texto += f\"{nombre:15} | {mse_prom:12.4f} | {sesgo:10.4f}\\n\"\n\naxes[1, 1].text(0.1, 0.9, tabla_texto, transform=axes[1, 1].transAxes, \n                fontfamily='monospace', fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 CONCLUSIONES:\")\nprint(\"✅ Supuestos correctos: Estimaciones insesgadas y predicciones precisas\")\nprint(\"❌ No linealidad: Mayor error de predicción y estimaciones sesgadas\")\nprint(\"❌ Heterocedasticidad: Intervalos de confianza incorrectos\")\nprint(\"❌ No normalidad: Pruebas de hipótesis no confiables\")\n\n🔄 Ejecutando simulación (esto puede tomar unos segundos...)\n\n\n\n\n\n\n\n\n\n\n📊 CONCLUSIONES:\n✅ Supuestos correctos: Estimaciones insesgadas y predicciones precisas\n❌ No linealidad: Mayor error de predicción y estimaciones sesgadas\n❌ Heterocedasticidad: Intervalos de confianza incorrectos\n❌ No normalidad: Pruebas de hipótesis no confiables",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#ejercicios-prácticos",
    "href": "violaciones_supuestos_regresion.html#ejercicios-prácticos",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "6. Ejercicios Prácticos",
    "text": "6. Ejercicios Prácticos\n\nEjercicio 1: Identificación de Violaciones\nExamina los siguientes conjuntos de datos y determina qué supuestos se violan:\n\n# Generar datos de ejercicio\nnp.random.seed(123)\n\ndef generar_datos_ejercicio(tipo, n=150):\n    x = np.linspace(0, 10, n)\n    \n    if tipo == 'A':\n        # Múltiples violaciones\n        y = 2 + 0.5 * x**1.5 + np.random.normal(0, 0.1 + 0.3 * x, n)\n    elif tipo == 'B':\n        # Solo heterocedasticidad\n        y = 1 + 2 * x + np.random.normal(0, 0.5 * np.sqrt(x + 1), n)\n    elif tipo == 'C':\n        # Solo no normalidad\n        y = 3 + 1.5 * x + stats.chi2.rvs(df=2, size=n) - 2\n    elif tipo == 'D':\n        # Datos correctos\n        y = 1 + 2 * x + np.random.normal(0, 1.5, n)\n    \n    return x, y\n\n# Crear ejercicios\nfig, axes = plt.subplots(4, 3, figsize=(18, 20))\nfig.suptitle('EJERCICIO: Identifica las Violaciones de Supuestos', fontsize=16, fontweight='bold')\n\ntipos_ejercicio = ['A', 'B', 'C', 'D']\n\nfor i, tipo in enumerate(tipos_ejercicio):\n    x, y = generar_datos_ejercicio(tipo)\n    \n    # Ajustar modelo\n    lr = LinearRegression()\n    lr.fit(x.reshape(-1, 1), y)\n    y_pred = lr.predict(x.reshape(-1, 1))\n    residuos = y - y_pred\n    \n    # Columna 1: Datos y modelo\n    axes[i, 0].scatter(x, y, alpha=0.6)\n    axes[i, 0].plot(x, y_pred, 'r-', linewidth=2)\n    axes[i, 0].set_title(f'Conjunto {tipo}: Datos y Modelo')\n    axes[i, 0].set_xlabel('X')\n    axes[i, 0].set_ylabel('Y')\n    axes[i, 0].grid(True, alpha=0.3)\n    \n    # Columna 2: Residuos vs Predichos\n    axes[i, 1].scatter(y_pred, residuos, alpha=0.6)\n    axes[i, 1].axhline(y=0, color='red', linestyle='--')\n    axes[i, 1].set_title(f'Conjunto {tipo}: Residuos vs Predichos')\n    axes[i, 1].set_xlabel('Valores Predichos')\n    axes[i, 1].set_ylabel('Residuos')\n    axes[i, 1].grid(True, alpha=0.3)\n    \n    # Columna 3: Q-Q Plot\n    stats.probplot(residuos, dist=\"norm\", plot=axes[i, 2])\n    axes[i, 2].set_title(f'Conjunto {tipo}: Q-Q Plot')\n    axes[i, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"🤔 PREGUNTA: ¿Qué supuestos se violan en cada conjunto?\")\nprint(\"\\n📝 Analiza cada fila:\")\nprint(\"- Conjunto A: ¿Qué patrones observas?\")\nprint(\"- Conjunto B: ¿Los residuos tienen varianza constante?\")\nprint(\"- Conjunto C: ¿Los residuos siguen distribución normal?\")\nprint(\"- Conjunto D: ¿Este conjunto cumple todos los supuestos?\")\nprint(\"\\n💡 Pista: Examina los patrones en residuos y la forma del Q-Q plot\")\n\n\n\n\n\n\n\n\n🤔 PREGUNTA: ¿Qué supuestos se violan en cada conjunto?\n\n📝 Analiza cada fila:\n- Conjunto A: ¿Qué patrones observas?\n- Conjunto B: ¿Los residuos tienen varianza constante?\n- Conjunto C: ¿Los residuos siguen distribución normal?\n- Conjunto D: ¿Este conjunto cumple todos los supuestos?\n\n💡 Pista: Examina los patrones en residuos y la forma del Q-Q plot",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "violaciones_supuestos_regresion.html#resumen-y-recomendaciones",
    "href": "violaciones_supuestos_regresion.html#resumen-y-recomendaciones",
    "title": "Violaciones de los Supuestos de Regresión Lineal",
    "section": "7. Resumen y Recomendaciones",
    "text": "7. Resumen y Recomendaciones\n\n¿Cómo diagnosticar violaciones?\n\n\n\n\n\n\n\n\nSupuesto\nHerramientas de Diagnóstico\nQué Buscar\n\n\n\n\nLinealidad\nResiduos vs PredichosGráficos parciales\nPatrones curvosTendencias sistemáticas\n\n\nIndependencia\nResiduos vs TiempoAutocorrelación\nPatrones temporalesCorrelación serial\n\n\nHomocedasticidad\nResiduos vs PredichosTest Breusch-Pagan\nForma de embudop-value &lt; 0.05\n\n\nNormalidad\nQ-Q PlotTest Shapiro-Wilk\nDesviación de línea rectap-value &lt; 0.05\n\n\n\n\n\n¿Qué hacer cuando se violan?\n\n\n\n\n\n\n\nViolación\nSoluciones Posibles\n\n\n\n\nNo Linealidad\n• Transformaciones (log, cuadrática)• Modelos no lineales• Splines, polinomios\n\n\nDependencia\n• Modelos de series temporales• Errores estándar robustos• GLS con estructura de correlación\n\n\nHeterocedasticidad\n• Transformaciones (log Y)• Errores estándar robustos• Mínimos cuadrados ponderados\n\n\nNo Normalidad\n• Transformaciones• Métodos no paramétricos• Modelos robustos\n\n\n\n\n\n🎯 Puntos Clave para Recordar\n\nLos gráficos de residuos son tu mejor amigo para diagnosticar problemas\nLas violaciones no siempre son fatales - depende de tu objetivo\nPara predicción: La no normalidad es menos crítica\nPara inferencia: Todos los supuestos son importantes\nSiempre visualiza antes de modelar y después de ajustar\n\n¡Felicidades! 🎉 Ahora tienes las herramientas para identificar y entender las violaciones de los supuestos de regresión lineal.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Violaciones de los Supuestos de Regresión Lineal</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html",
    "href": "analisis_advertising_dataset.html",
    "title": "Análisis de Regresión Lineal con el Dataset Advertising",
    "section": "",
    "text": "Descripción del Dataset\nEl dataset Advertising es un conjunto de datos clásico utilizado para enseñar conceptos de regresión lineal. Contiene información sobre presupuestos de publicidad en tres medios diferentes (TV, Radio y Periódico) y las ventas resultantes de productos.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análisis de Regresión Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#descripción-del-dataset",
    "href": "analisis_advertising_dataset.html#descripción-del-dataset",
    "title": "Análisis de Regresión Lineal con el Dataset Advertising",
    "section": "",
    "text": "Variables del dataset:\n\nTV: Presupuesto de publicidad en televisión (en miles de dólares)\nRadio: Presupuesto de publicidad en radio (en miles de dólares)\nNewspaper: Presupuesto de publicidad en periódicos (en miles de dólares)\nSales: Ventas del producto (en miles de unidades)\n\nEl objetivo es predecir las ventas basándose en los presupuestos de publicidad, lo que permite a las empresas optimizar su inversión en marketing.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análisis de Regresión Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#importar-librerías-y-descargar-datos",
    "href": "analisis_advertising_dataset.html#importar-librerías-y-descargar-datos",
    "title": "Análisis de Regresión Lineal con el Dataset Advertising",
    "section": "1. Importar librerías y descargar datos",
    "text": "1. Importar librerías y descargar datos\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurar estilo de gráficos\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n\n# Descargar y leer el dataset\nurl = 'https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/Advertising.csv'\nadvertising = pd.read_csv(url, index_col=0)\n\nprint(\"Dataset cargado exitosamente!\")\nprint(f\"Dimensiones del dataset: {advertising.shape}\")\nprint(f\"Columnas: {advertising.columns.tolist()}\")\n\nDataset cargado exitosamente!\nDimensiones del dataset: (200, 4)\nColumnas: ['TV', 'Radio', 'Newspaper', 'Sales']",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análisis de Regresión Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#análisis-exploratorio-de-datos-eda",
    "href": "analisis_advertising_dataset.html#análisis-exploratorio-de-datos-eda",
    "title": "Análisis de Regresión Lineal con el Dataset Advertising",
    "section": "2. Análisis Exploratorio de Datos (EDA)",
    "text": "2. Análisis Exploratorio de Datos (EDA)\n\n# Primeras filas del dataset\nadvertising.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n1\n230.1\n37.8\n69.2\n22.1\n\n\n2\n44.5\n39.3\n45.1\n10.4\n\n\n3\n17.2\n45.9\n69.3\n9.3\n\n\n4\n151.5\n41.3\n58.5\n18.5\n\n\n5\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n\n\n\n\n# Información general del dataset\nadvertising.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 200 entries, 1 to 200\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   TV         200 non-null    float64\n 1   Radio      200 non-null    float64\n 2   Newspaper  200 non-null    float64\n 3   Sales      200 non-null    float64\ndtypes: float64(4)\nmemory usage: 7.8 KB\n\n\n\n# Estadísticas descriptivas\nadvertising.describe().round(2)\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\ncount\n200.00\n200.00\n200.00\n200.00\n\n\nmean\n147.04\n23.26\n30.55\n14.02\n\n\nstd\n85.85\n14.85\n21.78\n5.22\n\n\nmin\n0.70\n0.00\n0.30\n1.60\n\n\n25%\n74.38\n9.98\n12.75\n10.38\n\n\n50%\n149.75\n22.90\n25.75\n12.90\n\n\n75%\n218.82\n36.52\n45.10\n17.40\n\n\nmax\n296.40\n49.60\n114.00\n27.00\n\n\n\n\n\n\n\n\n# Verificar valores nulos\nprint(\"Valores nulos por columna:\")\nprint(advertising.isnull().sum())\n\nValores nulos por columna:\nTV           0\nRadio        0\nNewspaper    0\nSales        0\ndtype: int64\n\n\n\n# Matriz de correlación\ncorrelation_matrix = advertising.corr()\nprint(\"\\nMatriz de correlación:\")\nprint(correlation_matrix.round(3))\n\n\nMatriz de correlación:\n              TV  Radio  Newspaper  Sales\nTV         1.000  0.055      0.057  0.782\nRadio      0.055  1.000      0.354  0.576\nNewspaper  0.057  0.354      1.000  0.228\nSales      0.782  0.576      0.228  1.000\n\n\n\n# Visualización de la matriz de correlación\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=1, fmt='.2f')\nplt.title('Matriz de Correlación - Dataset Advertising', fontsize=16, pad=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Distribución de las variables\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Distribución de Variables', fontsize=16, y=1.02)\n\nvariables = ['TV', 'Radio', 'Newspaper', 'Sales']\ncolors = ['steelblue', 'coral', 'lightgreen', 'gold']\n\nfor idx, (ax, var, color) in enumerate(zip(axes.flat, variables, colors)):\n    ax.hist(advertising[var], bins=20, edgecolor='black', alpha=0.7, color=color)\n    ax.set_title(f'Distribución de {var}', fontsize=12)\n    ax.set_xlabel(var)\n    ax.set_ylabel('Frecuencia')\n    ax.grid(True, alpha=0.3)\n    \n    # Agregar estadísticas\n    mean_val = advertising[var].mean()\n    median_val = advertising[var].median()\n    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Media: {mean_val:.2f}')\n    ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Mediana: {median_val:.2f}')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Relación entre cada variable predictora y las ventas\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfig.suptitle('Relación entre Variables Predictoras y Ventas', fontsize=16, y=1.02)\n\npredictors = ['TV', 'Radio', 'Newspaper']\ncolors = ['steelblue', 'coral', 'lightgreen']\n\nfor ax, predictor, color in zip(axes, predictors, colors):\n    ax.scatter(advertising[predictor], advertising['Sales'], alpha=0.6, color=color, edgecolors='black', linewidth=0.5)\n    ax.set_xlabel(f'{predictor} (miles de dólares)', fontsize=11)\n    ax.set_ylabel('Sales (miles de unidades)', fontsize=11)\n    ax.set_title(f'{predictor} vs Sales', fontsize=12)\n    ax.grid(True, alpha=0.3)\n    \n    # Agregar línea de tendencia\n    z = np.polyfit(advertising[predictor], advertising['Sales'], 1)\n    p = np.poly1d(z)\n    ax.plot(advertising[predictor].sort_values(), p(advertising[predictor].sort_values()), \n            \"r--\", alpha=0.8, linewidth=2)\n    \n    # Agregar correlación\n    corr = advertising[[predictor, 'Sales']].corr().iloc[0, 1]\n    ax.text(0.05, 0.95, f'Correlación: {corr:.3f}', transform=ax.transAxes, \n            fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pairplot para visualizar todas las relaciones\nplt.figure(figsize=(12, 10))\nsns.pairplot(advertising, diag_kind='kde', corner=True, \n             plot_kws={'alpha': 0.6, 'edgecolor': 'black', 'linewidth': 0.5})\nplt.suptitle('Matriz de Dispersión - Dataset Advertising', y=1.02, fontsize=16)\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 1200x1000 with 0 Axes&gt;",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análisis de Regresión Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#preparación-de-datos-división-train-test",
    "href": "analisis_advertising_dataset.html#preparación-de-datos-división-train-test",
    "title": "Análisis de Regresión Lineal con el Dataset Advertising",
    "section": "3. Preparación de Datos: División Train-Test",
    "text": "3. Preparación de Datos: División Train-Test\n\n# Separar características (X) y variable objetivo (y)\nX = advertising[['TV', 'Radio', 'Newspaper']]\ny = advertising['Sales']\n\nprint(\"Características (X):\")\nprint(X.head())\nprint(f\"\\nForma de X: {X.shape}\")\nprint(f\"Forma de y: {y.shape}\")\n\nCaracterísticas (X):\n      TV  Radio  Newspaper\n1  230.1   37.8       69.2\n2   44.5   39.3       45.1\n3   17.2   45.9       69.3\n4  151.5   41.3       58.5\n5  180.8   10.8       58.4\n\nForma de X: (200, 3)\nForma de y: (200,)\n\n\n\n# División en conjuntos de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"División de datos completada:\")\nprint(f\"Tamaño del conjunto de entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Tamaño del conjunto de prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n\nDivisión de datos completada:\nTamaño del conjunto de entrenamiento: 160 muestras (80.0%)\nTamaño del conjunto de prueba: 40 muestras (20.0%)\n\n\n\n# Estandarización de características para Ridge y Lasso\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"Datos estandarizados para Ridge y Lasso\")\nprint(f\"Media de X_train_scaled: {X_train_scaled.mean(axis=0).round(10)}\")\nprint(f\"Desviación estándar de X_train_scaled: {X_train_scaled.std(axis=0).round(2)}\")\n\nDatos estandarizados para Ridge y Lasso\nMedia de X_train_scaled: [-0. -0.  0.]\nDesviación estándar de X_train_scaled: [1. 1. 1.]",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análisis de Regresión Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#modelos-de-regresión",
    "href": "analisis_advertising_dataset.html#modelos-de-regresión",
    "title": "Análisis de Regresión Lineal con el Dataset Advertising",
    "section": "4. Modelos de Regresión",
    "text": "4. Modelos de Regresión\n\n4.1 Regresión Lineal Normal (OLS)\n\n# Entrenar modelo de regresión lineal\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# Predicciones\ny_pred_lr_train = lr_model.predict(X_train)\ny_pred_lr_test = lr_model.predict(X_test)\n\n# Métricas\nmse_lr_train = mean_squared_error(y_train, y_pred_lr_train)\nmse_lr_test = mean_squared_error(y_test, y_pred_lr_test)\nrmse_lr_train = np.sqrt(mse_lr_train)\nrmse_lr_test = np.sqrt(mse_lr_test)\nr2_lr_train = r2_score(y_train, y_pred_lr_train)\nr2_lr_test = r2_score(y_test, y_pred_lr_test)\nmae_lr_train = mean_absolute_error(y_train, y_pred_lr_train)\nmae_lr_test = mean_absolute_error(y_test, y_pred_lr_test)\n\n\nprint(\"=\"*50)\nprint(\"REGRESIÓN LINEAL NORMAL (OLS)\")\nprint(\"=\"*50)\nprint(f\"\\nCoeficientes:\")\nfor feature, coef in zip(X.columns, lr_model.coef_):\n    print(f\"  {feature}: {coef:.6f}\")\nprint(f\"\\nIntercepto: {lr_model.intercept_:.6f}\")\nprint(f\"\\nMétricas en Entrenamiento:\")\nprint(f\"  RMSE: {rmse_lr_train:.4f}\")\nprint(f\"  R²: {r2_lr_train:.4f}\")\nprint(f\"  MAE: {mae_lr_train:.4f}\")\nprint(f\"\\nMétricas en Prueba:\")\nprint(f\"  RMSE: {rmse_lr_test:.4f}\")\nprint(f\"  R²: {r2_lr_test:.4f}\")\nprint(f\"  MAE: {mae_lr_test:.4f}\")\n\n==================================================\nREGRESIÓN LINEAL NORMAL (OLS)\n==================================================\n\nCoeficientes:\n  TV: 0.044730\n  Radio: 0.189195\n  Newspaper: 0.002761\n\nIntercepto: 2.979067\n\nMétricas en Entrenamiento:\n  RMSE: 1.6447\n  R²: 0.8957\n  MAE: 1.1985\n\nMétricas en Prueba:\n  RMSE: 1.7816\n  R²: 0.8994\n  MAE: 1.4608\n\n\n\n\n4.2 Regresión Ridge\n\n# Probar diferentes valores de alpha\nalphas_ridge = [0.001, 0.01, 0.1, 1, 10, 100]\nridge_results = []\n\nfor alpha in alphas_ridge:\n    ridge_model = Ridge(alpha=alpha, random_state=42)\n    ridge_model.fit(X_train_scaled, y_train)\n    \n    y_pred_train = ridge_model.predict(X_train_scaled)\n    y_pred_test = ridge_model.predict(X_test_scaled)\n    \n    ridge_results.append({\n        'alpha': alpha,\n        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n        'r2_train': r2_score(y_train, y_pred_train),\n        'r2_test': r2_score(y_test, y_pred_test)\n    })\n\n# Convertir a DataFrame para mejor visualización\nridge_df = pd.DataFrame(ridge_results)\nprint(\"Resultados de Ridge con diferentes valores de alpha:\")\nprint(ridge_df.to_string(index=False))\n\nResultados de Ridge con diferentes valores de alpha:\n  alpha  rmse_train  rmse_test  r2_train  r2_test\n  0.001    1.644728   1.781605  0.895701 0.899437\n  0.010    1.644728   1.781654  0.895701 0.899432\n  0.100    1.644730   1.782143  0.895700 0.899377\n  1.000    1.644987   1.787220  0.895668 0.898803\n 10.000    1.667636   1.853716  0.892775 0.891132\n100.000    2.427323   2.786288  0.772832 0.754039\n\n\n\n# Seleccionar el mejor alpha basado en RMSE de prueba\nbest_alpha_ridge = ridge_df.loc[ridge_df['rmse_test'].idxmin(), 'alpha']\nprint(f\"\\nMejor alpha para Ridge: {best_alpha_ridge}\")\n\n# Entrenar modelo final con mejor alpha\nridge_model_best = Ridge(alpha=best_alpha_ridge, random_state=42)\nridge_model_best.fit(X_train_scaled, y_train)\n\n# Predicciones\ny_pred_ridge_train = ridge_model_best.predict(X_train_scaled)\ny_pred_ridge_test = ridge_model_best.predict(X_test_scaled)\n\n# Métricas\nmse_ridge_train = mean_squared_error(y_train, y_pred_ridge_train)\nmse_ridge_test = mean_squared_error(y_test, y_pred_ridge_test)\nrmse_ridge_train = np.sqrt(mse_ridge_train)\nrmse_ridge_test = np.sqrt(mse_ridge_test)\nr2_ridge_train = r2_score(y_train, y_pred_ridge_train)\nr2_ridge_test = r2_score(y_test, y_pred_ridge_test)\nmae_ridge_train = mean_absolute_error(y_train, y_pred_ridge_train)\nmae_ridge_test = mean_absolute_error(y_test, y_pred_ridge_test)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"REGRESIÓN RIDGE (alpha={best_alpha_ridge})\")\nprint(\"=\"*50)\nprint(f\"\\nCoeficientes:\")\nfor feature, coef in zip(X.columns, ridge_model_best.coef_):\n    print(f\"  {feature}: {coef:.6f}\")\nprint(f\"\\nIntercepto: {ridge_model_best.intercept_:.6f}\")\nprint(f\"\\nMétricas en Entrenamiento:\")\nprint(f\"  RMSE: {rmse_ridge_train:.4f}\")\nprint(f\"  R²: {r2_ridge_train:.4f}\")\nprint(f\"  MAE: {mae_ridge_train:.4f}\")\nprint(f\"\\nMétricas en Prueba:\")\nprint(f\"  RMSE: {rmse_ridge_test:.4f}\")\nprint(f\"  R²: {r2_ridge_test:.4f}\")\nprint(f\"  MAE: {mae_ridge_test:.4f}\")\n\n\nMejor alpha para Ridge: 0.001\n\n==================================================\nREGRESIÓN RIDGE (alpha=0.001)\n==================================================\n\nCoeficientes:\n  TV: 3.764174\n  Radio: 2.792288\n  Newspaper: 0.055983\n\nIntercepto: 14.100000\n\nMétricas en Entrenamiento:\n  RMSE: 1.6447\n  R²: 0.8957\n  MAE: 1.1985\n\nMétricas en Prueba:\n  RMSE: 1.7816\n  R²: 0.8994\n  MAE: 1.4608\n\n\n\n\n4.3 Regresión Lasso\n\n# Probar diferentes valores de alpha\nalphas_lasso = [0.001, 0.01, 0.1, 1, 10, 100]\nlasso_results = []\n\nfor alpha in alphas_lasso:\n    lasso_model = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n    lasso_model.fit(X_train_scaled, y_train)\n    \n    y_pred_train = lasso_model.predict(X_train_scaled)\n    y_pred_test = lasso_model.predict(X_test_scaled)\n    \n    lasso_results.append({\n        'alpha': alpha,\n        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n        'r2_train': r2_score(y_train, y_pred_train),\n        'r2_test': r2_score(y_test, y_pred_test),\n        'n_features': np.sum(lasso_model.coef_ != 0)\n    })\n\n# Convertir a DataFrame para mejor visualización\nlasso_df = pd.DataFrame(lasso_results)\nprint(\"Resultados de Lasso con diferentes valores de alpha:\")\nprint(lasso_df.to_string(index=False))\n\nResultados de Lasso con diferentes valores de alpha:\n  alpha  rmse_train  rmse_test  r2_train   r2_test  n_features\n  0.001    1.644728   1.781587  0.895701  0.899439           3\n  0.010    1.644799   1.781551  0.895692  0.899443           3\n  0.100    1.651293   1.791334  0.894867  0.898336           2\n  1.000    2.146055   2.396806  0.822428  0.817997           2\n 10.000    5.092764   5.631496  0.000000 -0.004757           0\n100.000    5.092764   5.631496  0.000000 -0.004757           0\n\n\n\n# Seleccionar el mejor alpha basado en RMSE de prueba\nbest_alpha_lasso = lasso_df.loc[lasso_df['rmse_test'].idxmin(), 'alpha']\nprint(f\"\\nMejor alpha para Lasso: {best_alpha_lasso}\")\n\n# Entrenar modelo final con mejor alpha\nlasso_model_best = Lasso(alpha=best_alpha_lasso, random_state=42, max_iter=10000)\nlasso_model_best.fit(X_train_scaled, y_train)\n\n# Predicciones\ny_pred_lasso_train = lasso_model_best.predict(X_train_scaled)\ny_pred_lasso_test = lasso_model_best.predict(X_test_scaled)\n\n# Métricas\nmse_lasso_train = mean_squared_error(y_train, y_pred_lasso_train)\nmse_lasso_test = mean_squared_error(y_test, y_pred_lasso_test)\nrmse_lasso_train = np.sqrt(mse_lasso_train)\nrmse_lasso_test = np.sqrt(mse_lasso_test)\nr2_lasso_train = r2_score(y_train, y_pred_lasso_train)\nr2_lasso_test = r2_score(y_test, y_pred_lasso_test)\nmae_lasso_train = mean_absolute_error(y_train, y_pred_lasso_train)\nmae_lasso_test = mean_absolute_error(y_test, y_pred_lasso_test)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"REGRESIÓN LASSO (alpha={best_alpha_lasso})\")\nprint(\"=\"*50)\nprint(f\"\\nCoeficientes:\")\nfor feature, coef in zip(X.columns, lasso_model_best.coef_):\n    if coef != 0:\n        print(f\"  {feature}: {coef:.6f}\")\n    else:\n        print(f\"  {feature}: {coef:.6f} (eliminado)\")\nprint(f\"\\nIntercepto: {lasso_model_best.intercept_:.6f}\")\nprint(f\"\\nCaracterísticas seleccionadas: {np.sum(lasso_model_best.coef_ != 0)} de {len(X.columns)}\")\nprint(f\"\\nMétricas en Entrenamiento:\")\nprint(f\"  RMSE: {rmse_lasso_train:.4f}\")\nprint(f\"  R²: {r2_lasso_train:.4f}\")\nprint(f\"  MAE: {mae_lasso_train:.4f}\")\nprint(f\"\\nMétricas en Prueba:\")\nprint(f\"  RMSE: {rmse_lasso_test:.4f}\")\nprint(f\"  R²: {r2_lasso_test:.4f}\")\nprint(f\"  MAE: {mae_lasso_test:.4f}\")\n\n\nMejor alpha para Lasso: 0.01\n\n==================================================\nREGRESIÓN LASSO (alpha=0.01)\n==================================================\n\nCoeficientes:\n  TV: 3.754675\n  Radio: 2.785686\n  Newspaper: 0.048727\n\nIntercepto: 14.100000\n\nCaracterísticas seleccionadas: 3 de 3\n\nMétricas en Entrenamiento:\n  RMSE: 1.6448\n  R²: 0.8957\n  MAE: 1.1983\n\nMétricas en Prueba:\n  RMSE: 1.7816\n  R²: 0.8994\n  MAE: 1.4590\n\n\n\n\n4.4 Ejercicio de clase\nAhora les toca implementar la validación cruzada que se vio en Selección de hiperparámetros",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análisis de Regresión Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#comparación-de-modelos",
    "href": "analisis_advertising_dataset.html#comparación-de-modelos",
    "title": "Análisis de Regresión Lineal con el Dataset Advertising",
    "section": "5. Comparación de Modelos",
    "text": "5. Comparación de Modelos\n\n# Crear tabla comparativa\ncomparison_data = {\n    'Modelo': ['Linear Regression', f'Ridge (α={best_alpha_ridge})', f'Lasso (α={best_alpha_lasso})'],\n    'RMSE Train': [rmse_lr_train, rmse_ridge_train, rmse_lasso_train],\n    'RMSE Test': [rmse_lr_test, rmse_ridge_test, rmse_lasso_test],\n    'R² Train': [r2_lr_train, r2_ridge_train, r2_lasso_train],\n    'R² Test': [r2_lr_test, r2_ridge_test, r2_lasso_test],\n    'MAE Test': [mae_lr_test, mae_ridge_test, mae_lasso_test]\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.round(4)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARACIÓN DE MODELOS\")\nprint(\"=\"*70)\nprint(comparison_df.to_string(index=False))\n\n# Identificar el mejor modelo\nbest_model_idx = comparison_df['RMSE Test'].idxmin()\nbest_model_name = comparison_df.loc[best_model_idx, 'Modelo']\nprint(f\"\\n🏆 Mejor modelo basado en RMSE de prueba: {best_model_name}\")\n\n\n======================================================================\nCOMPARACIÓN DE MODELOS\n======================================================================\n           Modelo  RMSE Train  RMSE Test  R² Train  R² Test  MAE Test\nLinear Regression      1.6447     1.7816    0.8957   0.8994    1.4608\n  Ridge (α=0.001)      1.6447     1.7816    0.8957   0.8994    1.4608\n   Lasso (α=0.01)      1.6448     1.7816    0.8957   0.8994    1.4590\n\n🏆 Mejor modelo basado en RMSE de prueba: Linear Regression\n\n\n\n# Visualización de métricas\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfig.suptitle('Comparación de Métricas entre Modelos', fontsize=16, y=1.05)\n\nmodels = comparison_df['Modelo'].tolist()\nx_pos = np.arange(len(models))\ncolors = ['steelblue', 'coral', 'lightgreen']\n\n# RMSE\nax1 = axes[0]\nwidth = 0.35\nbars1 = ax1.bar(x_pos - width/2, comparison_df['RMSE Train'], width, label='Train', color='lightblue', edgecolor='black')\nbars2 = ax1.bar(x_pos + width/2, comparison_df['RMSE Test'], width, label='Test', color='orange', edgecolor='black')\nax1.set_xlabel('Modelo')\nax1.set_ylabel('RMSE')\nax1.set_title('Root Mean Squared Error')\nax1.set_xticks(x_pos)\nax1.set_xticklabels(models, rotation=15, ha='right')\nax1.legend()\nax1.grid(True, alpha=0.3, axis='y')\n\n# R²\nax2 = axes[1]\nbars3 = ax2.bar(x_pos - width/2, comparison_df['R² Train'], width, label='Train', color='lightblue', edgecolor='black')\nbars4 = ax2.bar(x_pos + width/2, comparison_df['R² Test'], width, label='Test', color='orange', edgecolor='black')\nax2.set_xlabel('Modelo')\nax2.set_ylabel('R²')\nax2.set_title('Coeficiente de Determinación (R²)')\nax2.set_xticks(x_pos)\nax2.set_xticklabels(models, rotation=15, ha='right')\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\nax2.set_ylim([0, 1])\n\n# MAE\nax3 = axes[2]\nbars5 = ax3.bar(x_pos, comparison_df['MAE Test'], color=colors, edgecolor='black', alpha=0.7)\nax3.set_xlabel('Modelo')\nax3.set_ylabel('MAE')\nax3.set_title('Mean Absolute Error (Test)')\nax3.set_xticks(x_pos)\nax3.set_xticklabels(models, rotation=15, ha='right')\nax3.grid(True, alpha=0.3, axis='y')\n\n# Agregar valores en las barras\nfor bars in [bars1, bars2, bars3, bars4, bars5]:\n    for bar in bars:\n        height = bar.get_height()\n        if bars in [bars3, bars4]:  # Para R²\n            ax = ax2\n        elif bars in [bars1, bars2]:  # Para RMSE\n            ax = ax1\n        else:  # Para MAE\n            ax = ax3\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualización de coeficientes\nfig, ax = plt.subplots(figsize=(12, 6))\n\nfeatures = X.columns.tolist()\nx_pos = np.arange(len(features))\nwidth = 0.25\n\n# Coeficientes de cada modelo\ncoef_lr = lr_model.coef_\ncoef_ridge = ridge_model_best.coef_\ncoef_lasso = lasso_model_best.coef_\n\nbars1 = ax.bar(x_pos - width, coef_lr, width, label='Linear Regression', color='steelblue', edgecolor='black')\nbars2 = ax.bar(x_pos, coef_ridge, width, label=f'Ridge (α={best_alpha_ridge})', color='coral', edgecolor='black')\nbars3 = ax.bar(x_pos + width, coef_lasso, width, label=f'Lasso (α={best_alpha_lasso})', color='lightgreen', edgecolor='black')\n\nax.set_xlabel('Características', fontsize=12)\nax.set_ylabel('Valor del Coeficiente', fontsize=12)\nax.set_title('Comparación de Coeficientes entre Modelos', fontsize=14)\nax.set_xticks(x_pos)\nax.set_xticklabels(features)\nax.legend(loc='upper right')\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n\n# Agregar valores en las barras\nfor bars in [bars1, bars2, bars3]:\n    for bar in bars:\n        height = bar.get_height()\n        if abs(height) &gt; 0.001:\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{height:.3f}', ha='center', va='bottom' if height &gt; 0 else 'top', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Gráfico de predicciones vs valores reales para el mejor modelo\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nfig.suptitle('Predicciones vs Valores Reales (Conjunto de Prueba)', fontsize=16, y=1.02)\n\nmodels_pred = [\n    ('Linear Regression', y_pred_lr_test, 'steelblue'),\n    (f'Ridge (α={best_alpha_ridge})', y_pred_ridge_test, 'coral'),\n    (f'Lasso (α={best_alpha_lasso})', y_pred_lasso_test, 'lightgreen')\n]\n\nfor ax, (name, predictions, color) in zip(axes, models_pred):\n    ax.scatter(y_test, predictions, alpha=0.6, color=color, edgecolors='black', linewidth=0.5)\n    \n    # Línea perfecta de predicción\n    min_val = min(y_test.min(), predictions.min())\n    max_val = max(y_test.max(), predictions.max())\n    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicción Perfecta')\n    \n    ax.set_xlabel('Valores Reales', fontsize=11)\n    ax.set_ylabel('Predicciones', fontsize=11)\n    ax.set_title(name, fontsize=12)\n    ax.grid(True, alpha=0.3)\n    ax.legend()\n    \n    # Agregar R² en el gráfico\n    r2 = r2_score(y_test, predictions)\n    ax.text(0.05, 0.95, f'R² = {r2:.4f}', transform=ax.transAxes,\n            fontsize=10, verticalalignment='top',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Análisis de residuos\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('Análisis de Residuos', fontsize=16, y=1.02)\n\nmodels_resid = [\n    ('Linear Regression', y_pred_lr_test, 'steelblue'),\n    (f'Ridge (α={best_alpha_ridge})', y_pred_ridge_test, 'coral'),\n    (f'Lasso (α={best_alpha_lasso})', y_pred_lasso_test, 'lightgreen')\n]\n\nfor idx, (name, predictions, color) in enumerate(models_resid):\n    residuals = y_test - predictions\n    \n    # Gráfico de residuos vs predicciones\n    ax1 = axes[0, idx]\n    ax1.scatter(predictions, residuals, alpha=0.6, color=color, edgecolors='black', linewidth=0.5)\n    ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n    ax1.set_xlabel('Predicciones', fontsize=10)\n    ax1.set_ylabel('Residuos', fontsize=10)\n    ax1.set_title(f'{name} - Residuos vs Predicciones', fontsize=11)\n    ax1.grid(True, alpha=0.3)\n    \n    # Histograma de residuos\n    ax2 = axes[1, idx]\n    ax2.hist(residuals, bins=20, edgecolor='black', alpha=0.7, color=color)\n    ax2.set_xlabel('Residuos', fontsize=10)\n    ax2.set_ylabel('Frecuencia', fontsize=10)\n    ax2.set_title(f'{name} - Distribución de Residuos', fontsize=11)\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    # Agregar línea de media\n    mean_resid = residuals.mean()\n    ax2.axvline(mean_resid, color='red', linestyle='--', linewidth=2, label=f'Media: {mean_resid:.3f}')\n    ax2.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análisis de Regresión Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "analisis_advertising_dataset.html#conclusiones",
    "href": "analisis_advertising_dataset.html#conclusiones",
    "title": "Análisis de Regresión Lineal con el Dataset Advertising",
    "section": "6. Conclusiones",
    "text": "6. Conclusiones\n\nResumen de Resultados:\n\nRegresión Lineal Normal (OLS):\n\nModelo base sin regularización\nUtiliza todos los predictores sin penalización\nPuede ser propenso al sobreajuste con más características\n\nRidge Regression:\n\nAplica regularización L2 (penaliza la suma de los cuadrados de los coeficientes)\nMantiene todas las características pero reduce su magnitud\nÚtil cuando todas las características son potencialmente relevantes\n\nLasso Regression:\n\nAplica regularización L1 (penaliza la suma de los valores absolutos de los coeficientes)\nPuede llevar algunos coeficientes exactamente a cero (selección de características)\nÚtil para identificar las características más importantes\n\n\n\n\nObservaciones Clave:\n\nTV tiene la correlación más fuerte con las ventas\nRadio también muestra una correlación positiva significativa\nNewspaper tiene la correlación más débil y puede ser eliminada por Lasso\nLos tres modelos tienen un rendimiento similar, lo que sugiere que el problema es relativamente simple\nLa regularización ayuda a prevenir el sobreajuste, especialmente con conjuntos de datos más complejos\n\n\n# Interpretación práctica del mejor modelo\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETACIÓN PRÁCTICA DEL MODELO\")\nprint(\"=\"*70)\n\n# Usar coeficientes del modelo de regresión lineal para interpretación\nprint(\"\\nInterpretación de los coeficientes (Regresión Lineal):\")\nprint(\"-\" * 60)\nfor feature, coef in zip(X.columns, lr_model.coef_):\n    print(f\"\\n{feature}:\")\n    print(f\"  • Por cada $1000 adicionales invertidos en {feature}\")\n    print(f\"  • Las ventas aumentan en {coef*1000:.0f} unidades\")\n    print(f\"  • Coeficiente: {coef:.6f}\")\n\nprint(f\"\\nIntercepto: {lr_model.intercept_:.2f}\")\nprint(\"  • Ventas base esperadas sin inversión en publicidad (miles de unidades)\")\n\n# Ejemplo de predicción\nprint(\"\\n\" + \"=\"*70)\nprint(\"EJEMPLO DE PREDICCIÓN\")\nprint(\"=\"*70)\nexample_budget = pd.DataFrame({\n    'TV': [150],\n    'Radio': [30],\n    'Newspaper': [10]\n})\n\nprediction_lr = lr_model.predict(example_budget)[0]\nprint(\"\\nPresupuesto de ejemplo:\")\nprint(f\"  • TV: ${example_budget['TV'][0]:,} mil\")\nprint(f\"  • Radio: ${example_budget['Radio'][0]:,} mil\")\nprint(f\"  • Newspaper: ${example_budget['Newspaper'][0]:,} mil\")\nprint(f\"\\nVentas predichas: {prediction_lr:.2f} mil unidades\")\nprint(f\"Equivalente a: {prediction_lr*1000:,.0f} unidades\")\n\n\n======================================================================\nINTERPRETACIÓN PRÁCTICA DEL MODELO\n======================================================================\n\nInterpretación de los coeficientes (Regresión Lineal):\n------------------------------------------------------------\n\nTV:\n  • Por cada $1000 adicionales invertidos en TV\n  • Las ventas aumentan en 45 unidades\n  • Coeficiente: 0.044730\n\nRadio:\n  • Por cada $1000 adicionales invertidos en Radio\n  • Las ventas aumentan en 189 unidades\n  • Coeficiente: 0.189195\n\nNewspaper:\n  • Por cada $1000 adicionales invertidos en Newspaper\n  • Las ventas aumentan en 3 unidades\n  • Coeficiente: 0.002761\n\nIntercepto: 2.98\n  • Ventas base esperadas sin inversión en publicidad (miles de unidades)\n\n======================================================================\nEJEMPLO DE PREDICCIÓN\n======================================================================\n\nPresupuesto de ejemplo:\n  • TV: $150 mil\n  • Radio: $30 mil\n  • Newspaper: $10 mil\n\nVentas predichas: 15.39 mil unidades\nEquivalente a: 15,392 unidades",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análisis de Regresión Lineal con el Dataset Advertising</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html",
    "href": "ejercicio_wine_quality.html",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "",
    "text": "Descripción del Dataset\nEl dataset Wine Quality contiene resultados de análisis fisicoquímicos de vinos portugueses “Vinho Verde” y su calidad evaluada por expertos. El objetivo es predecir la calidad del vino basándose en sus propiedades químicas.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#descripción-del-dataset",
    "href": "ejercicio_wine_quality.html#descripción-del-dataset",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "",
    "text": "Variables del dataset:\n\nfixed acidity: Acidez fija (g/L de ácido tartárico)\nvolatile acidity: Acidez volátil (g/L de ácido acético)\ncitric acid: Ácido cítrico (g/L)\nresidual sugar: Azúcar residual (g/L)\nchlorides: Cloruros (g/L de cloruro de sodio)\nfree sulfur dioxide: Dióxido de azufre libre (mg/L)\ntotal sulfur dioxide: Dióxido de azufre total (mg/L)\ndensity: Densidad (g/cm³)\npH: pH del vino\nsulphates: Sulfatos (g/L de sulfato de potasio)\nalcohol: Contenido de alcohol (% vol)\nquality: Calidad del vino (puntuación de 0-10) - Variable objetivo\n\nEn este ejercicio, trabajarás con el dataset de vinos tintos y aplicarás diferentes técnicas de regresión para predecir la calidad del vino.",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#importar-librerías-y-cargar-datos",
    "href": "ejercicio_wine_quality.html#importar-librerías-y-cargar-datos",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "1. Importar librerías y cargar datos",
    "text": "1. Importar librerías y cargar datos\n\n# Importar las librerías necesarias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurar estilo de gráficos\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Configurar seed para reproducibilidad\nnp.random.seed(42)\n\n\n# Cargar el dataset de vinos tintos\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\nwine_data = pd.read_csv(url, sep=';')\n\nprint(\"Dataset cargado exitosamente!\")\nprint(f\"Dimensiones del dataset: {wine_data.shape}\")\nprint(f\"\\nColumnas del dataset:\")\nprint(wine_data.columns.tolist())",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#análisis-exploratorio-de-datos-eda",
    "href": "ejercicio_wine_quality.html#análisis-exploratorio-de-datos-eda",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "2. Análisis Exploratorio de Datos (EDA)",
    "text": "2. Análisis Exploratorio de Datos (EDA)\n\nEjercicio 2.1: Exploración inicial\nCompleta el análisis exploratorio inicial del dataset.\n\n# Mostrar las primeras filas del dataset\nwine_data.head()\n\n\n# TODO: Muestra la información general del dataset (tipos de datos, valores no nulos)\n# Tu código aquí\n\n\n# TODO: Calcula y muestra las estadísticas descriptivas del dataset\n# Tu código aquí\n\n\n# TODO: Verifica si hay valores nulos en el dataset\n# Tu código aquí\n\n\n\nEjercicio 2.2: Análisis de la variable objetivo\n\n# Analizar la distribución de la calidad del vino\nplt.figure(figsize=(10, 6))\nwine_data['quality'].value_counts().sort_index().plot(kind='bar', color='steelblue', edgecolor='black')\nplt.xlabel('Calidad del Vino', fontsize=12)\nplt.ylabel('Frecuencia', fontsize=12)\nplt.title('Distribución de la Calidad del Vino', fontsize=14)\nplt.xticks(rotation=0)\nplt.grid(axis='y', alpha=0.3)\n\n# Agregar estadísticas\nmean_quality = wine_data['quality'].mean()\nmedian_quality = wine_data['quality'].median()\nplt.axhline(y=wine_data['quality'].value_counts().mean(), color='red', \n            linestyle='--', label=f'Media de frecuencia')\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(f\"Estadísticas de la calidad del vino:\")\nprint(f\"Media: {mean_quality:.2f}\")\nprint(f\"Mediana: {median_quality:.2f}\")\nprint(f\"Desviación estándar: {wine_data['quality'].std():.2f}\")\n\n\n\nEjercicio 2.3: Matriz de correlación\n\n# TODO: Calcula la matriz de correlación y visualízala con un heatmap\n# Pista: Usa sns.heatmap() con annot=True para mostrar los valores\n# Tu código aquí\n\nplt.figure(figsize=(14, 10))\n# Completa el código para crear el heatmap\n\n\n# TODO: Identifica y muestra las 5 variables más correlacionadas con 'quality'\n# Tu código aquí\n\n\n\nEjercicio 2.4: Visualización de relaciones\n\n# Visualizar las 4 variables más correlacionadas con quality\ntop_features = ['alcohol', 'volatile acidity', 'citric acid', 'sulphates']\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Relación entre Variables Principales y Calidad del Vino', fontsize=16)\n\nfor idx, (ax, feature) in enumerate(zip(axes.flat, top_features)):\n    # TODO: Crea un scatter plot para cada variable vs quality\n    # Agrega una línea de tendencia\n    # Tu código aquí\n    pass\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#preparación-de-datos",
    "href": "ejercicio_wine_quality.html#preparación-de-datos",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "3. Preparación de Datos",
    "text": "3. Preparación de Datos\n\n# Separar características (X) y variable objetivo (y)\nX = wine_data.drop('quality', axis=1)\ny = wine_data['quality']\n\nprint(f\"Forma de X: {X.shape}\")\nprint(f\"Forma de y: {y.shape}\")\nprint(f\"\\nCaracterísticas: {X.columns.tolist()}\")\n\n\n# TODO: Divide los datos en conjuntos de entrenamiento y prueba\n# Usa test_size=0.2 y random_state=42\n# Tu código aquí\n\n# X_train, X_test, y_train, y_test = ...\n\n# print(f\"Tamaño del conjunto de entrenamiento: ...\")\n# print(f\"Tamaño del conjunto de prueba: ...\")\n\n\n# TODO: Estandariza las características\n# Recuerda: ajusta el scaler solo con los datos de entrenamiento\n# Tu código aquí\n\n# scaler = StandardScaler()\n# X_train_scaled = ...\n# X_test_scaled = ...",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#validación-cruzada-para-selección-de-hiperparámetros",
    "href": "ejercicio_wine_quality.html#validación-cruzada-para-selección-de-hiperparámetros",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "4. Validación Cruzada para Selección de Hiperparámetros",
    "text": "4. Validación Cruzada para Selección de Hiperparámetros\nLa validación cruzada es fundamental para seleccionar los mejores hiperparámetros sin usar el conjunto de prueba.\n\nEjercicio 4.1: Implementación manual de validación cruzada\n\n# Ejemplo: Validación cruzada manual para Ridge\ndef manual_cross_validation(X, y, alpha, n_folds=5):\n    \"\"\"\n    Implementa validación cruzada manualmente para Ridge regression\n    \"\"\"\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    scores = []\n    \n    for train_idx, val_idx in kf.split(X):\n        # Dividir datos\n        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # Estandarizar\n        scaler_cv = StandardScaler()\n        X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n        X_val_cv_scaled = scaler_cv.transform(X_val_cv)\n        \n        # Entrenar modelo\n        model = Ridge(alpha=alpha)\n        model.fit(X_train_cv_scaled, y_train_cv)\n        \n        # Evaluar\n        y_pred = model.predict(X_val_cv_scaled)\n        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n        scores.append(rmse)\n    \n    return np.mean(scores), np.std(scores)\n\n# Probar diferentes valores de alpha\nalphas_to_test = [0.001, 0.01, 0.1, 1, 10, 100]\ncv_results_manual = []\n\nprint(\"Validación Cruzada Manual para Ridge Regression:\")\nprint(\"-\" * 50)\n\nfor alpha in alphas_to_test:\n    mean_rmse, std_rmse = manual_cross_validation(X_train.values, y_train, alpha)\n    cv_results_manual.append({'alpha': alpha, 'mean_rmse': mean_rmse, 'std_rmse': std_rmse})\n    print(f\"Alpha: {alpha:7.3f} | RMSE: {mean_rmse:.4f} (+/- {std_rmse:.4f})\")\n\n# TODO: Identifica el mejor alpha basado en el RMSE medio más bajo\n# Tu código aquí\n\n\n\nEjercicio 4.2: Usar RidgeCV para validación cruzada automática\n\n# TODO: Usa RidgeCV para encontrar automáticamente el mejor alpha\n# Pista: RidgeCV tiene un parámetro 'alphas' y 'cv'\n# Tu código aquí\n\n# alphas = np.logspace(-3, 3, 100)  # 100 valores entre 0.001 y 1000\n# ridge_cv = RidgeCV(...)\n# ridge_cv.fit(...)\n\n# print(f\"Mejor alpha encontrado por RidgeCV: ...\")\n\n\n\nEjercicio 4.3: GridSearchCV para búsqueda exhaustiva\n\n# Ejemplo completo con GridSearchCV para Ridge\nfrom sklearn.model_selection import GridSearchCV\n\n# Definir parámetros a buscar\nparam_grid_ridge = {\n    'alpha': np.logspace(-3, 3, 20)  # 20 valores entre 0.001 y 1000\n}\n\n# Crear modelo base\nridge_base = Ridge(random_state=42)\n\n# TODO: Implementa GridSearchCV\n# Usa cv=5, scoring='neg_mean_squared_error'\n# Tu código aquí\n\n# grid_search_ridge = GridSearchCV(...)\n# grid_search_ridge.fit(...)\n\n# print(f\"Mejor alpha: ...\")\n# print(f\"Mejor score (RMSE): ...\")\n\n\n# TODO: Visualiza los resultados de la validación cruzada\n# Crea un gráfico que muestre cómo cambia el RMSE con diferentes valores de alpha\n# Tu código aquí\n\nplt.figure(figsize=(10, 6))\n# Completa el código para visualizar los resultados",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#modelos-de-regresión",
    "href": "ejercicio_wine_quality.html#modelos-de-regresión",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "5. Modelos de Regresión",
    "text": "5. Modelos de Regresión\n\n5.1 Regresión Lineal Normal\n\n# TODO: Implementa y entrena un modelo de regresión lineal\n# Tu código aquí\n\n# lr_model = LinearRegression()\n# lr_model.fit(...)\n\n# Hacer predicciones\n# y_pred_lr_train = ...\n# y_pred_lr_test = ...\n\n# Calcular métricas\n# rmse_lr_train = ...\n# rmse_lr_test = ...\n# r2_lr_train = ...\n# r2_lr_test = ...\n\n# print(\"Regresión Lineal Normal:\")\n# print(f\"RMSE Train: ...\")\n# print(f\"RMSE Test: ...\")\n# print(f\"R² Train: ...\")\n# print(f\"R² Test: ...\")\n\n\n\n5.2 Ridge Regression con mejor alpha de CV\n\n# TODO: Entrena Ridge con el mejor alpha encontrado por validación cruzada\n# Tu código aquí\n\n# best_alpha_ridge = ...  # Usa el mejor alpha de la sección anterior\n# ridge_model = Ridge(alpha=best_alpha_ridge)\n# ridge_model.fit(...)\n\n# Predicciones y métricas\n# ...\n\n\n\n5.3 Lasso Regression con validación cruzada\n\n# TODO: Implementa LassoCV para encontrar el mejor alpha automáticamente\n# Tu código aquí\n\n# alphas_lasso = np.logspace(-3, 1, 100)\n# lasso_cv = LassoCV(...)\n# lasso_cv.fit(...)\n\n# print(f\"Mejor alpha para Lasso: ...\")\n# print(f\"Número de características seleccionadas: ...\")\n\n# Predicciones y métricas\n# ...\n\n\n# TODO: Identifica qué características fueron eliminadas por Lasso\n# Tu código aquí",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#comparación-de-modelos",
    "href": "ejercicio_wine_quality.html#comparación-de-modelos",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "6. Comparación de Modelos",
    "text": "6. Comparación de Modelos\n\n# TODO: Crea una tabla comparativa con todos los modelos\n# Incluye: RMSE Train, RMSE Test, R² Train, R² Test, MAE Test\n# Tu código aquí\n\n# comparison_data = {\n#     'Modelo': [...],\n#     'RMSE Train': [...],\n#     'RMSE Test': [...],\n#     'R² Train': [...],\n#     'R² Test': [...],\n#     'MAE Test': [...]\n# }\n\n# comparison_df = pd.DataFrame(comparison_data)\n# print(comparison_df)\n\n\n# TODO: Crea visualizaciones para comparar los modelos\n# 1. Gráfico de barras comparando RMSE\n# 2. Gráfico de barras comparando R²\n# Tu código aquí",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#análisis-de-residuos",
    "href": "ejercicio_wine_quality.html#análisis-de-residuos",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "7. Análisis de Residuos",
    "text": "7. Análisis de Residuos\n\n# TODO: Para el mejor modelo, crea:\n# 1. Gráfico de residuos vs predicciones\n# 2. Histograma de residuos\n# 3. Q-Q plot de residuos\n# Tu código aquí",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#importancia-de-características",
    "href": "ejercicio_wine_quality.html#importancia-de-características",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "8. Importancia de Características",
    "text": "8. Importancia de Características\n\n# TODO: Visualiza los coeficientes de los tres modelos en un mismo gráfico\n# Esto te ayudará a entender qué características son más importantes\n# Tu código aquí",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#validación-cruzada-final-del-mejor-modelo",
    "href": "ejercicio_wine_quality.html#validación-cruzada-final-del-mejor-modelo",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "9. Validación Cruzada Final del Mejor Modelo",
    "text": "9. Validación Cruzada Final del Mejor Modelo\n\n# TODO: Realiza validación cruzada con 10 folds del mejor modelo\n# Reporta la media y desviación estándar del RMSE\n# Tu código aquí",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#conclusiones-y-preguntas-de-reflexión",
    "href": "ejercicio_wine_quality.html#conclusiones-y-preguntas-de-reflexión",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "10. Conclusiones y Preguntas de Reflexión",
    "text": "10. Conclusiones y Preguntas de Reflexión\n\nPreguntas para responder:\n\n¿Cuál modelo tuvo el mejor desempeño? ¿Por qué crees que fue así?\n\nTu respuesta:\n\n¿Qué características son las más importantes para predecir la calidad del vino?\n\nTu respuesta:\n\n¿Observas señales de sobreajuste en algún modelo? ¿Cómo lo identificaste?\n\nTu respuesta:\n\n¿Cómo cambió el rendimiento de Ridge y Lasso con diferentes valores de alpha?\n\nTu respuesta:\n\n¿Qué ventajas observaste al usar validación cruzada para seleccionar hiperparámetros?\n\nTu respuesta:\n\nSi Lasso eliminó algunas características, ¿crees que esto mejoró o empeoró el modelo? ¿Por qué?\n\nTu respuesta:\n\n¿Qué otros pasos podrías tomar para mejorar el rendimiento del modelo?\n\nTu respuesta:",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "ejercicio_wine_quality.html#ejercicio-extra-ingeniería-de-características",
    "href": "ejercicio_wine_quality.html#ejercicio-extra-ingeniería-de-características",
    "title": "Ejercicio: Análisis de Regresión con el Dataset Wine Quality",
    "section": "Ejercicio Extra: Ingeniería de Características",
    "text": "Ejercicio Extra: Ingeniería de Características\n\nDesafío:\nIntenta mejorar el rendimiento del modelo creando nuevas características:\n\nCrea interacciones entre variables (ej: alcohol × pH)\nCrea características polinomiales\nAgrupa la calidad en categorías (baja: 3-4, media: 5-6, alta: 7-8) y úsala como característica\nCrea ratios entre características relacionadas\n\n\n# TODO: Implementa ingeniería de características y evalúa si mejora el modelo\n# Tu código aquí",
    "crumbs": [
      "Regresion lineal",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html",
    "href": "04-clasificacion.html",
    "title": "Clasificación",
    "section": "",
    "text": "Introducción al Problema de Clasificación\nEn los capítulos anteriores hemos trabajado con problemas de regresión, donde la variable respuesta \\(Y\\) es cuantitativa (continua). En este capítulo estudiaremos los problemas de clasificación, donde la variakbble respuesta \\(Y\\) es cualitativa (categórica o discreta).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clasificación</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html#introducción-al-problema-de-clasificación",
    "href": "04-clasificacion.html#introducción-al-problema-de-clasificación",
    "title": "Clasificación",
    "section": "",
    "text": "Definición Formal\nUn problema de clasificación consiste en asignar una observación \\(\\mathbf{x} = (x_1, x_2, ..., x_p)\\) a una de \\(K\\) clases o categorías posibles. Formalmente:\n\nEntrada: Un vector de características \\(\\mathbf{x} \\in \\mathbb{R}^p\\)\nSalida: Una etiqueta de clase \\(y \\in \\mathcal{C} = \\{C_1, C_2, ..., C_K\\}\\)\n\nDonde \\(\\mathcal{C}\\) es el conjunto finito de clases posibles.\n\n\nEjemplos de Problemas de Clasificación\n\nClasificación binaria (\\(K=2\\)):\n\nDetección de spam en correos electrónicos (spam/no spam)\nDiagnóstico médico (enfermo/sano)\nAprobación de crédito (aprobado/rechazado)\n\nClasificación multiclase (\\(K&gt;2\\)):\n\nReconocimiento de dígitos escritos a mano (0-9)\nClasificación de tipos de flores (setosa/versicolor/virginica)\nCategorización de noticias (deportes/política/tecnología/etc.)\n\n\n\n\nObjetivo del Aprendizaje\nEl objetivo es aprender una función de clasificación \\(f: \\mathbb{R}^p \\rightarrow \\mathcal{C}\\) que minimice el error de clasificación esperado:\n\\[\\mathbb{E}[L(Y, f(\\mathbf{X}))]\\]\nDonde \\(L\\) es una función de pérdida. La función de pérdida más común es la pérdida 0-1:\n\\[L_{0-1}(y, \\hat{y}) = \\begin{cases}\n0 & \\text{si } y = \\hat{y} \\\\\n1 & \\text{si } y \\neq \\hat{y}\n\\end{cases}\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clasificación</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html#funciones-de-pérdida-en-clasificación",
    "href": "04-clasificacion.html#funciones-de-pérdida-en-clasificación",
    "title": "Clasificación",
    "section": "Funciones de Pérdida en Clasificación",
    "text": "Funciones de Pérdida en Clasificación\nAunque la pérdida 0-1 es intuitiva y directamente relacionada con la tasa de error, presenta limitaciones importantes: no es diferenciable y no proporciona información sobre la confianza de las predicciones. Por esto, en la práctica se utilizan funciones de pérdida alternativas que trabajan con probabilidades.\n\nClasificación Binaria: Pérdidas Probabilísticas\nPara clasificación binaria, donde \\(y \\in \\{0, 1\\}\\), consideramos predicciones probabilísticas \\(\\hat{p} = P(\\hat{Y} = 1 | \\mathbf{x})\\). Las funciones de pérdida más importantes son:\n\nPérdida de Brier (Brier Score)\nLa pérdida de Brier o pérdida cuadrática mide el error cuadrático medio entre las probabilidades predichas y los valores reales:\n\\[L_{\\text{Brier}}(y, \\hat{p}) = (y - \\hat{p})^2\\]\nPara un conjunto de \\(n\\) observaciones:\n\\[\\text{Brier Score} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{p}_i)^2\\]\nPropiedades:\n\nRango: \\([0, 1]\\) (menor es mejor)\nEs una regla de puntuación propia (proper scoring rule)\nPenaliza fuertemente predicciones confiadas pero incorrectas\nSe puede descomponer en: calibración + refinamiento\n\n\n\nPérdida Logarítmica (Log Loss o Entropía Cruzada Binaria)\nLa pérdida logarítmica mide la distancia entre la distribución verdadera y la predicha usando la divergencia de Kullback-Leibler:\n\\[L_{\\text{log}}(y, \\hat{p}) = -[y \\log(\\hat{p}) + (1-y) \\log(1-\\hat{p})]\\]\nEquivalentemente: \\[L_{\\text{log}}(y, \\hat{p}) = \\begin{cases}\n-\\log(\\hat{p}) & \\text{si } y = 1 \\\\\n-\\log(1-\\hat{p}) & \\text{si } y = 0\n\\end{cases}\\]\nPara un conjunto de observaciones:\n\\[\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i)]\\]\nPropiedades:\n\nRango: \\([0, \\infty)\\) (menor es mejor)\nTambién es una regla de puntuación propia\nPenaliza infinitamente predicciones completamente incorrectas (\\(\\hat{p} = 0\\) cuando \\(y = 1\\))\nEs la función objetivo en regresión logística\n\n\n\n\nComparación de Funciones de Pérdida\n\n\n\n\n\nFunciones de perdidas para clasificación\n\n\n\n\n\n\nReglas de Puntuación Propias\nUna regla de puntuación propia (proper scoring rule) es una función de pérdida que incentiva al modelo a reportar sus verdaderas probabilidades. Formalmente, una función \\(S(p, y)\\) es propia si:\n\\[\\mathbb{E}_{Y \\sim p^*}[S(p^*, Y)] \\leq \\mathbb{E}_{Y \\sim p^*}[S(p, Y)]\\]\nDonde \\(p^*\\) es la distribución verdadera. Tanto la pérdida de Brier como la log loss son propias, mientras que la pérdida 0-1 no lo es.\n\n\nVentajas y Desventajas\nPérdida de Brier:\n\n✓ Interpretación directa como MSE de probabilidades\n✓ Acotada en \\([0,1]\\)\n✓ Menos sensible a predicciones extremas incorrectas\n✗ Menos utilizada en optimización de modelos\n\nPérdida Logarítmica:\n\n✓ Base teórica sólida (teoría de información)\n✓ Función objetivo natural para muchos modelos (logística, redes neuronales)\n✓ Diferenciable y convexa\n✗ No acotada superiormente\n✗ Muy sensible a predicciones extremas incorrectas",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clasificación</span>"
    ]
  },
  {
    "objectID": "04-clasificacion.html#modelos-para-clasificación-binaria",
    "href": "04-clasificacion.html#modelos-para-clasificación-binaria",
    "title": "Clasificación",
    "section": "Modelos para Clasificación Binaria",
    "text": "Modelos para Clasificación Binaria\n\nClasificador de Bayes para el Caso Binario\nEl clasificador de Bayes es el clasificador óptimo teórico que minimiza el error de clasificación. Para el caso binario con clases \\(\\{0, 1\\}\\), clasifica según:\n\\[\\hat{y}(\\mathbf{x}) = \\begin{cases}\n1 & \\text{si } P(Y = 1 | \\mathbf{X} = \\mathbf{x}) &gt; 0.5 \\\\\n0 & \\text{si } P(Y = 1 | \\mathbf{X} = \\mathbf{x}) \\leq 0.5\n\\end{cases}\\]\nO más generalmente, con un umbral \\(\\tau\\):\n\\[\\hat{y}(\\mathbf{x}) = \\mathbb{1}[P(Y = 1 | \\mathbf{X} = \\mathbf{x}) &gt; \\tau]\\]\n\nEstimación mediante el Teorema de Bayes\nUsando el teorema de Bayes:\n\\[P(Y = k | \\mathbf{X} = \\mathbf{x}) = \\frac{P(\\mathbf{X} = \\mathbf{x} | Y = k) \\cdot P(Y = k)}{P(\\mathbf{X} = \\mathbf{x})}\\]\nDonde:\n\n\\(P(Y = k)\\) es la probabilidad a priori de la clase \\(k\\)\n\\(P(\\mathbf{X} = \\mathbf{x} | Y = k)\\) es la verosimilitud de observar \\(\\mathbf{x}\\) dado que pertenece a la clase \\(k\\)\n\\(P(\\mathbf{X} = \\mathbf{x})\\) es la evidencia (constante de normalización)\n\nComo \\(P(\\mathbf{X} = \\mathbf{x})\\) es igual para todas las clases, la decisión se basa en:\n\\[\\hat{y} = \\arg\\max_k P(\\mathbf{X} = \\mathbf{x} | Y = k) \\cdot P(Y = k)\\]\n\n\nNaive Bayes: Simplificando el Problema\nEl problema principal del clasificador de Bayes es estimar \\(P(\\mathbf{X} = \\mathbf{x} | Y = k)\\) en alta dimensión. Con \\(p\\) características, necesitamos estimar la distribución conjunta de todas las variables, lo cual es computacionalmente intratable cuando \\(p\\) es grande.\nEl clasificador Naive Bayes resuelve este problema mediante una asunción de independencia condicional: asume que las características son condicionalmente independientes dada la clase:\n\\[P(\\mathbf{X} = \\mathbf{x} | Y = k) = P(x_1, x_2, ..., x_p | Y = k) = \\prod_{j=1}^{p} P(x_j | Y = k)\\]\nEsta asunción, aunque “ingenua” (naive), simplifica enormemente el cálculo y funciona sorprendentemente bien en la práctica.\n\n\nTipos de Naive Bayes\nDependiendo del tipo de características, existen diferentes variantes:\n\n1. Gaussian Naive Bayes (características continuas)\nAsume que las características siguen una distribución normal dentro de cada clase:\n\\[P(x_j | Y = k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{jk}^2}} \\exp\\left(-\\frac{(x_j - \\mu_{jk})^2}{2\\sigma_{jk}^2}\\right)\\]\nDonde \\(\\mu_{jk}\\) y \\(\\sigma_{jk}^2\\) son la media y varianza de la característica \\(j\\) en la clase \\(k\\).\n\n\n2. Multinomial Naive Bayes (características discretas/conteos)\nUtilizado para datos de conteo (ej. frecuencia de palabras en clasificación de texto):\n\\[P(\\mathbf{x} | Y = k) = \\frac{N_k!}{\\prod_j x_j!} \\prod_{j=1}^{p} \\theta_{jk}^{x_j}\\]\nDonde \\(\\theta_{jk}\\) es la probabilidad de la característica \\(j\\) en la clase \\(k\\).\n\n\n3. Bernoulli Naive Bayes (características binarias)\nPara características binarias (presencia/ausencia):\n\\[P(\\mathbf{x} | Y = k) = \\prod_{j=1}^{p} \\theta_{jk}^{x_j} (1-\\theta_{jk})^{1-x_j}\\]\n\n\n\nVentajas y Desventajas de Naive Bayes\nVentajas:\n\n✓ Rápido de entrenar y predecir\n✓ Funciona bien con pocos datos de entrenamiento\n✓ Maneja naturalmente múltiples clases\n✓ Robusto ante características irrelevantes\n✓ Proporciona estimaciones de probabilidad\n\nDesventajas:\n\n✗ La asunción de independencia es frecuentemente violada\n✗ Puede dar estimaciones de probabilidad sesgadas\n✗ Sensible a la maldición de la dimensionalidad con Gaussian NB\n\n\n\nEjemplos en Python\n\n1. Ejemplo Básico: Gaussian Naive Bayes\nComenzamos con un ejemplo simple de clasificación binaria usando Gaussian Naive Bayes:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pandas as pd\n\n# Generar datos sintéticos para clasificación binaria\nnp.random.seed(42)\nX, y = make_classification(\n    n_samples=300,\n    n_features=2,        # 2 características para visualización fácil\n    n_informative=2,     # Ambas características son informativas\n    n_redundant=0,       # Sin características redundantes\n    n_clusters_per_class=2,  # 2 grupos por clase\n    flip_y=0.05,         # 5% de ruido en las etiquetas\n    class_sep=0.8,       # Separación entre clases\n    random_state=42\n)\n\n# Dividir en conjunto de entrenamiento (70%) y prueba (30%)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nprint(\"Dimensiones de los datos:\")\nprint(f\"  Entrenamiento: {X_train.shape}\")\nprint(f\"  Prueba: {X_test.shape}\")\n\nDimensiones de los datos:\n  Entrenamiento: (210, 2)\n  Prueba: (90, 2)\n\n\n\n# Crear y entrenar el modelo Gaussian Naive Bayes\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\n# Realizar predicciones\ny_pred = gnb.predict(X_test)\ny_proba = gnb.predict_proba(X_test)\n\n# Evaluar el modelo\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Exactitud (Accuracy): {accuracy:.3f}\")\n\n# Matriz de confusión\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nMatriz de Confusión:\")\nprint(pd.DataFrame(cm,\n                   columns=['Predicho 0', 'Predicho 1'],\n                   index=['Real 0', 'Real 1']))\n\n# Parámetros aprendidos por el modelo\nprint(\"\\n\" + \"=\" * 50)\nprint(\"PARÁMETROS APRENDIDOS\")\nprint(\"=\" * 50)\nprint(f\"\\nProbabilidades a priori (prior):\")\nprint(f\"  P(Y=0) = {gnb.class_prior_[0]:.3f}\")\nprint(f\"  P(Y=1) = {gnb.class_prior_[1]:.3f}\")\n\nprint(f\"\\nMedias de cada característica por clase:\")\nfor i, clase in enumerate([0, 1]):\n    print(f\"  Clase {clase}: μ₁={gnb.theta_[i, 0]:.3f}, μ₂={gnb.theta_[i, 1]:.3f}\")\n\nprint(f\"\\nVarianzas de cada característica por clase:\")\nfor i, clase in enumerate([0, 1]):\n    print(f\"  Clase {clase}: σ²₁={gnb.var_[i, 0]:.3f}, σ²₂={gnb.var_[i, 1]:.3f}\")\n\nExactitud (Accuracy): 0.844\n\nMatriz de Confusión:\n        Predicho 0  Predicho 1\nReal 0          46           8\nReal 1           6          30\n\n==================================================\nPARÁMETROS APRENDIDOS\n==================================================\n\nProbabilidades a priori (prior):\n  P(Y=0) = 0.481\n  P(Y=1) = 0.519\n\nMedias de cada característica por clase:\n  Clase 0: μ₁=-0.072, μ₂=-0.693\n  Clase 1: μ₁=0.048, μ₂=0.726\n\nVarianzas de cada característica por clase:\n  Clase 0: σ²₁=0.872, σ²₂=0.544\n  Clase 1: σ²₁=1.275, σ²₂=0.654\n\n\n\n\n2. Visualización de la Frontera de Decisión\n\n# Función auxiliar para visualizar fronteras de decisión\ndef visualizar_clasificador(X, y, classifier, title):\n    \"\"\"\n    Visualiza la frontera de decisión de un clasificador\n\n    Parámetros:\n    - X: características (n_samples, 2)\n    - y: etiquetas (n_samples,)\n    - classifier: modelo entrenado\n    - title: título del gráfico\n    \"\"\"\n    h = 0.02  # Tamaño del paso en la malla\n\n    # Crear una malla de puntos para evaluar el clasificador\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Predecir probabilidades para cada punto de la malla\n    Z = classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n    Z = Z.reshape(xx.shape)\n\n    # Crear la visualización\n    plt.figure(figsize=(12, 5))\n\n    # Panel 1: Datos y frontera\n    plt.subplot(1, 2, 1)\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu_r', levels=20)\n    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolors='black',\n                s=50, label='Clase 0', alpha=0.7)\n    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolors='black',\n                s=50, label='Clase 1', alpha=0.7)\n    plt.xlabel('Característica 1')\n    plt.ylabel('Característica 2')\n    plt.title(f'{title} - Frontera de Decisión')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Panel 2: Mapa de probabilidades\n    plt.subplot(1, 2, 2)\n    contour = plt.contourf(xx, yy, Z, levels=20, cmap='RdBu_r', alpha=0.8)\n    plt.colorbar(contour, label='P(Y=1|X)')\n    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolors='black',\n                s=30, alpha=0.5)\n    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolors='black',\n                s=30, alpha=0.5)\n    plt.xlabel('Característica 1')\n    plt.ylabel('Característica 2')\n    plt.title(f'{title} - Probabilidades')\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n# Visualizar nuestro modelo entrenado\nvisualizar_clasificador(X_train, y_train, gnb, 'Gaussian Naive Bayes')\n\n\n\n\nFrontera de decisión de Gaussian Naive Bayes\n\n\n\n\n\n\n3. Comparación de Variantes de Naive Bayes\nAhora comparemos las tres variantes principales de Naive Bayes:\n\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Preparar diferentes versiones de los datos para cada variante\n\n# 1. Gaussian NB: usa los datos originales\nX_gaussian = X_train.copy()\n\n# 2. Multinomial NB: necesita valores no negativos (frecuencias)\nscaler = MinMaxScaler()\nX_multinomial = scaler.fit_transform(X_train) + 0.1  # Asegurar valores positivos\n\n# 3. Bernoulli NB: necesita valores binarios\nX_bernoulli = (X_train &gt; np.median(X_train, axis=0)).astype(float)\n\nprint(\"Forma de los datos para cada variante:\")\nprint(f\"  Gaussian: {X_gaussian.shape} - Valores continuos\")\nprint(f\"  Multinomial: {X_multinomial.shape} - Valores positivos\")\nprint(f\"  Bernoulli: {X_bernoulli.shape} - Valores binarios\")\n\n# Mostrar ejemplos de los primeros 3 datos\nprint(\"\\nEjemplo de transformación (primeras 3 muestras, primera característica):\")\nprint(f\"  Original: {X_gaussian[:3, 0]}\")\nprint(f\"  Multinomial: {X_multinomial[:3, 0]}\")\nprint(f\"  Bernoulli: {X_bernoulli[:3, 0]}\")\n\nForma de los datos para cada variante:\n  Gaussian: (210, 2) - Valores continuos\n  Multinomial: (210, 2) - Valores positivos\n  Bernoulli: (210, 2) - Valores binarios\n\nEjemplo de transformación (primeras 3 muestras, primera característica):\n  Original: [-0.98221929  1.48740486  0.62625557]\n  Multinomial: [0.37211974 0.9009729  0.71656365]\n  Bernoulli: [0. 1. 1.]\n\n\n\n# Entrenar las tres variantes\nmodelos = {\n    'Gaussian NB': (GaussianNB(), X_gaussian),\n    'Multinomial NB': (MultinomialNB(), X_multinomial),\n    'Bernoulli NB': (BernoulliNB(), X_bernoulli)\n}\n\nresultados = {}\n\nfor nombre, (modelo, X_train_variant) in modelos.items():\n    # Entrenar\n    modelo.fit(X_train_variant, y_train)\n\n    # Preparar datos de prueba según la variante\n    if nombre == 'Gaussian NB':\n        X_test_variant = X_test\n    elif nombre == 'Multinomial NB':\n        X_test_variant = scaler.transform(X_test) + 0.1\n    else:  # Bernoulli\n        X_test_variant = (X_test &gt; np.median(X_train, axis=0)).astype(float)\n\n    # Predecir\n    y_pred = modelo.predict(X_test_variant)\n\n    # Guardar resultados\n    resultados[nombre] = {\n        'modelo': modelo,\n        'accuracy': accuracy_score(y_test, y_pred),\n        'y_pred': y_pred\n    }\n\n    print(f\"\\n{nombre}:\")\n    print(f\"  Accuracy: {resultados[nombre]['accuracy']:.3f}\")\n\n\nGaussian NB:\n  Accuracy: 0.844\n\nMultinomial NB:\n  Accuracy: 0.544\n\nBernoulli NB:\n  Accuracy: 0.844\n\n\n\n# Visualizar comparación de resultados\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Gráfico de barras de accuracy\nnombres = list(resultados.keys())\naccuracies = [resultados[n]['accuracy'] for n in nombres]\n\nbars = axes[0].bar(nombres, accuracies, color=['blue', 'green', 'red'], alpha=0.7)\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Comparación de Exactitud')\naxes[0].set_ylim([0, 1])\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Añadir valores en las barras\nfor bar, acc in zip(bars, accuracies):\n    height = bar.get_height()\n    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n                 f'{acc:.3f}', ha='center', va='bottom')\n\n# Matrices de confusión\nfrom sklearn.metrics import confusion_matrix\n\naxes[1].axis('off')\nfor i, nombre in enumerate(nombres):\n    cm = confusion_matrix(y_test, resultados[nombre]['y_pred'])\n\n    # Crear subtabla\n    ax_sub = plt.subplot2grid((1, 6), (0, 4 + i*2//3), colspan=2//3 + 1)\n    ax_sub.imshow(cm, cmap='Blues', aspect='auto')\n    ax_sub.set_title(f'{nombre.split()[0]} NB', fontsize=9)\n\n    # Añadir texto en cada celda\n    for (i, j), val in np.ndenumerate(cm):\n        ax_sub.text(j, i, str(val), ha='center', va='center')\n\n    if i == 0:\n        ax_sub.set_ylabel('Real', fontsize=8)\n    ax_sub.set_xlabel('Pred', fontsize=8)\n    ax_sub.set_xticks([0, 1])\n    ax_sub.set_yticks([0, 1])\n    ax_sub.tick_params(labelsize=8)\n\nplt.suptitle('Comparación de Variantes de Naive Bayes', y=1.05)\nplt.tight_layout()\nplt.show()\n\n\n\n\nComparación de variantes de Naive Bayes\n\n\n\n\n\n\n4. Ejemplo Práctico: Clasificación de Texto\n\n# Simular un conjunto de datos de texto\n# Imaginemos que tenemos documentos con conteo de palabras\n\nprint(\"=\" * 60)\nprint(\"EJEMPLO: CLASIFICACIÓN DE DOCUMENTOS\")\nprint(\"=\" * 60)\n\n# Crear datos simulados de texto\nnp.random.seed(42)\nn_docs = 100\nn_palabras = 10\n\n# Nombres de las \"palabras\" para mejor interpretación\npalabras = ['tecnología', 'computadora', 'software', 'datos', 'algoritmo',\n            'deporte', 'equipo', 'juego', 'campeonato', 'jugador']\n\n# Crear matriz de frecuencias\n# Clase 0: documentos sobre tecnología (más palabras 0-4)\n# Clase 1: documentos sobre deportes (más palabras 5-9)\nX_text = np.random.poisson(1, (n_docs, n_palabras))\ny_text = np.array([0] * 50 + [1] * 50)  # 50 docs de cada clase\n\n# Sesgar frecuencias según la clase\nX_text[:50, :5] *= 3   # Docs de tecnología: más palabras técnicas\nX_text[50:, 5:] *= 3   # Docs de deportes: más palabras deportivas\n\n# Crear DataFrame para mejor visualización\ndf_text = pd.DataFrame(X_text, columns=palabras)\ndf_text['clase'] = y_text\ndf_text['tipo_documento'] = df_text['clase'].map({0: 'Tecnología', 1: 'Deportes'})\n\nprint(\"\\nPrimeros 5 documentos:\")\nprint(df_text.head())\n\nprint(\"\\nEstadísticas por clase:\")\nprint(df_text.groupby('tipo_documento')[palabras].mean().round(2))\n\n============================================================\nEJEMPLO: CLASIFICACIÓN DE DOCUMENTOS\n============================================================\n\nPrimeros 5 documentos:\n   tecnología  computadora  software  datos  algoritmo  deporte  equipo  \\\n0           3            6         0      0          9        2       0   \n1           0            3         0      0          3        0       1   \n2           9            0         3      0          3        1       1   \n3           0            0         0      0          3        1       0   \n4           0            9         0      0          0        2       0   \n\n   juego  campeonato  jugador  clase tipo_documento  \n0      0           0        1      0     Tecnología  \n1      0           1        0      0     Tecnología  \n2      1           0        5      0     Tecnología  \n3      1           1        2      0     Tecnología  \n4      0           0        3      0     Tecnología  \n\nEstadísticas por clase:\n                tecnología  computadora  software  datos  algoritmo  deporte  \\\ntipo_documento                                                                 \nDeportes              1.16         1.02      1.00   1.12       1.02     3.12   \nTecnología            2.82         2.94      2.22   2.46       2.28     1.12   \n\n                equipo  juego  campeonato  jugador  \ntipo_documento                                      \nDeportes           3.0   2.76        3.12     3.36  \nTecnología         0.9   0.68        0.92     1.20  \n\n\n\n# Dividir datos de texto\nX_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n    X_text, y_text, test_size=0.3, random_state=42, stratify=y_text\n)\n\n# Entrenar Multinomial Naive Bayes (ideal para datos de conteo)\nmnb_text = MultinomialNB(alpha=1.0)  # alpha: parámetro de suavizado Laplace\nmnb_text.fit(X_text_train, y_text_train)\n\n# Predicciones\ny_pred_text = mnb_text.predict(X_text_test)\ny_proba_text = mnb_text.predict_proba(X_text_test)\n\n# Evaluación\nprint(\"Resultados de Clasificación de Texto:\")\nprint(f\"Accuracy: {accuracy_score(y_text_test, y_pred_text):.3f}\")\n\n# Matriz de confusión\ncm_text = confusion_matrix(y_text_test, y_pred_text)\nprint(\"\\nMatriz de Confusión:\")\nprint(pd.DataFrame(cm_text,\n                   columns=['Pred Tecnología', 'Pred Deportes'],\n                   index=['Real Tecnología', 'Real Deportes']))\n\n# Importancia de las palabras\nprint(\"\\nImportancia de palabras por clase (log-probabilidades):\")\nlog_probs = mnb_text.feature_log_prob_\nimportancia_df = pd.DataFrame(log_probs.T,\n                               columns=['Tecnología', 'Deportes'],\n                               index=palabras)\nprint(importancia_df.round(3))\n\nResultados de Clasificación de Texto:\nAccuracy: 0.833\n\nMatriz de Confusión:\n                 Pred Tecnología  Pred Deportes\nReal Tecnología               14              1\nReal Deportes                  4             11\n\nImportancia de palabras por clase (log-probabilidades):\n             Tecnología  Deportes\ntecnología       -1.924    -2.854\ncomputadora      -1.703    -3.112\nsoftware         -2.162    -3.028\ndatos            -1.960    -2.950\nalgoritmo        -1.997    -3.112\ndeporte          -2.582    -1.894\nequipo           -3.034    -1.947\njuego            -3.188    -2.064\ncampeonato       -3.108    -1.748\njugador          -2.653    -1.843\n\n\n\n# Visualizar importancia de palabras\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Diferencia en log-probabilidades (palabras más discriminativas)\ndiff_log_prob = log_probs[0] - log_probs[1]  # Tecnología - Deportes\nindices_sorted = np.argsort(diff_log_prob)\n\n# Panel 1: Palabras más importantes para cada clase\ny_pos = np.arange(len(palabras))\naxes[0].barh(y_pos, diff_log_prob[indices_sorted],\n             color=['red' if x &lt; 0 else 'blue' for x in diff_log_prob[indices_sorted]],\n             alpha=0.7)\naxes[0].set_yticks(y_pos)\naxes[0].set_yticklabels([palabras[i] for i in indices_sorted])\naxes[0].set_xlabel('Diferencia en log-probabilidad\\n(← Deportes | Tecnología →)')\naxes[0].set_title('Palabras Discriminativas')\naxes[0].grid(True, alpha=0.3, axis='x')\naxes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\n# Panel 2: Matriz de probabilidades\nim = axes[1].imshow(np.exp(log_probs), cmap='YlOrRd', aspect='auto')\naxes[1].set_xticks(range(len(palabras)))\naxes[1].set_xticklabels(palabras, rotation=45, ha='right')\naxes[1].set_yticks([0, 1])\naxes[1].set_yticklabels(['Tecnología', 'Deportes'])\naxes[1].set_title('Probabilidades de Palabras por Clase')\nplt.colorbar(im, ax=axes[1], label='Probabilidad')\n\n# Añadir valores en la matriz\nfor i in range(2):\n    for j in range(len(palabras)):\n        text = axes[1].text(j, i, f'{np.exp(log_probs[i, j]):.2f}',\n                           ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nImportancia de palabras en clasificación de texto\n\n\n\n\n\n\n5. Ejemplo con Dataset Real: Iris\n\nfrom sklearn.datasets import load_iris\n\n# Cargar dataset Iris\niris = load_iris()\nX_iris = iris.data[:, [0, 2]]  # Usar solo 2 características para visualización\ny_iris = iris.target\nnombres_clases = iris.target_names\nnombres_features = [iris.feature_names[0], iris.feature_names[2]]\n\nprint(\"Dataset Iris:\")\nprint(f\"  Número de muestras: {X_iris.shape[0]}\")\nprint(f\"  Número de características: {X_iris.shape[1]}\")\nprint(f\"  Clases: {nombres_clases}\")\nprint(f\"  Características usadas: {nombres_features}\")\n\n# Dividir datos\nX_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n)\n\n# Entrenar Gaussian Naive Bayes\ngnb_iris = GaussianNB()\ngnb_iris.fit(X_iris_train, y_iris_train)\n\n# Predicciones\ny_pred_iris = gnb_iris.predict(X_iris_test)\naccuracy_iris = accuracy_score(y_iris_test, y_pred_iris)\n\nprint(f\"\\nAccuracy en Iris: {accuracy_iris:.3f}\")\n\n# Visualización\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Panel 1: Datos y fronteras de decisión\nh = .02\nx_min, x_max = X_iris[:, 0].min() - 1, X_iris[:, 0].max() + 1\ny_min, y_max = X_iris[:, 1].min() - 1, X_iris[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = gnb_iris.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\naxes[0].contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\nscatter = axes[0].scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris,\n                          cmap='viridis', edgecolors='black', s=50)\naxes[0].set_xlabel(nombres_features[0])\naxes[0].set_ylabel(nombres_features[1])\naxes[0].set_title('Gaussian NB - Dataset Iris (3 clases)')\naxes[0].grid(True, alpha=0.3)\n\n# Añadir leyenda\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=plt.cm.viridis(i/2), label=nombres_clases[i])\n                   for i in range(3)]\naxes[0].legend(handles=legend_elements, loc='upper right')\n\n# Panel 2: Matriz de confusión\ncm_iris = confusion_matrix(y_iris_test, y_pred_iris)\nim = axes[1].imshow(cm_iris, cmap='Blues', aspect='auto')\naxes[1].set_xticks(range(3))\naxes[1].set_yticks(range(3))\naxes[1].set_xticklabels(nombres_clases)\naxes[1].set_yticklabels(nombres_clases)\naxes[1].set_xlabel('Predicción')\naxes[1].set_ylabel('Valor Real')\naxes[1].set_title(f'Matriz de Confusión (Accuracy: {accuracy_iris:.3f})')\n\n# Añadir valores\nfor (i, j), val in np.ndenumerate(cm_iris):\n    axes[1].text(j, i, str(val), ha='center', va='center',\n                 color='white' if val &gt; cm_iris.max()/2 else 'black')\n\nplt.colorbar(im, ax=axes[1])\nplt.tight_layout()\nplt.show()\n\nDataset Iris:\n  Número de muestras: 150\n  Número de características: 2\n  Clases: ['setosa' 'versicolor' 'virginica']\n  Características usadas: ['sepal length (cm)', 'petal length (cm)']\n\nAccuracy en Iris: 0.911\n\n\n\n\n\nClasificación multiclase con Naive Bayes en dataset Iris\n\n\n\n\n\n\n6. Implementación Desde Cero de Gaussian Naive Bayes\nPara comprender mejor el funcionamiento interno del algoritmo, vamos a implementar Gaussian Naive Bayes paso a paso:\n\nclass GaussianNBDesdesCero:\n    \"\"\"\n    Implementación educativa de Gaussian Naive Bayes\n\n    Esta clase implementa el algoritmo paso a paso para\n    fines pedagógicos.\n    \"\"\"\n\n    def __init__(self):\n        self.clases = None\n        self.priors = {}        # P(Y=k) para cada clase k\n        self.medias = {}        # μ para cada clase y característica\n        self.varianzas = {}     # σ² para cada clase y característica\n\n    def entrenar(self, X, y):\n        \"\"\"\n        Fase de entrenamiento: calcular estadísticas\n\n        Parámetros:\n        - X: matriz de características (n_muestras, n_características)\n        - y: vector de etiquetas (n_muestras,)\n        \"\"\"\n        self.clases = np.unique(y)\n        n_muestras = len(y)\n        n_caracteristicas = X.shape[1]\n\n        print(f\"Entrenando con {n_muestras} muestras y {n_caracteristicas} características\")\n        print(f\"Clases encontradas: {self.clases}\")\n\n        for clase in self.clases:\n            # Filtrar datos de esta clase\n            X_clase = X[y == clase]\n            n_clase = len(X_clase)\n\n            # Calcular probabilidad a priori P(Y=clase)\n            self.priors[clase] = n_clase / n_muestras\n\n            # Calcular media y varianza para cada característica\n            self.medias[clase] = np.mean(X_clase, axis=0)\n            self.varianzas[clase] = np.var(X_clase, axis=0) + 1e-9  # Evitar división por cero\n\n            print(f\"\\nClase {clase}: {n_clase} muestras ({self.priors[clase]:.1%})\")\n            print(f\"  Medias: {self.medias[clase]}\")\n            print(f\"  Varianzas: {self.varianzas[clase]}\")\n\n        return self\n\n    def _calcular_gaussiana(self, x, media, varianza):\n        \"\"\"\n        Calcula P(x|μ,σ²) usando la distribución gaussiana\n\n        Fórmula: P(x|μ,σ²) = 1/√(2πσ²) * exp(-(x-μ)²/(2σ²))\n        \"\"\"\n        coeficiente = 1.0 / np.sqrt(2.0 * np.pi * varianza)\n        exponente = -((x - media) ** 2) / (2.0 * varianza)\n        return coeficiente * np.exp(exponente)\n\n    def predecir_probabilidades(self, X):\n        \"\"\"\n        Calcula P(Y=k|X) para cada clase k\n\n        Usa el teorema de Bayes:\n        P(Y=k|X) ∝ P(X|Y=k) * P(Y=k)\n        \"\"\"\n        n_muestras = X.shape[0]\n        n_clases = len(self.clases)\n        probabilidades = np.zeros((n_muestras, n_clases))\n\n        for i, x in enumerate(X):\n            for j, clase in enumerate(self.clases):\n                # Calcular P(Y=clase) - prior\n                prob_prior = self.priors[clase]\n\n                # Calcular P(X|Y=clase) - verosimilitud\n                # Producto de probabilidades (asumiendo independencia)\n                verosimilitud = 1.0\n                for k in range(len(x)):\n                    prob_caracteristica = self._calcular_gaussiana(\n                        x[k],\n                        self.medias[clase][k],\n                        self.varianzas[clase][k]\n                    )\n                    verosimilitud *= prob_caracteristica\n\n                # P(Y=clase|X) ∝ P(X|Y=clase) * P(Y=clase)\n                probabilidades[i, j] = verosimilitud * prob_prior\n\n            # Normalizar para que sumen 1\n            probabilidades[i] = probabilidades[i] / np.sum(probabilidades[i])\n\n        return probabilidades\n\n    def predecir(self, X):\n        \"\"\"\n        Predice la clase con mayor probabilidad posterior\n        \"\"\"\n        probabilidades = self.predecir_probabilidades(X)\n        indices_maximos = np.argmax(probabilidades, axis=1)\n        return self.clases[indices_maximos]\n\n\n# Crear y entrenar nuestro modelo\nprint(\"=\" * 60)\nprint(\"IMPLEMENTACIÓN DESDE CERO\")\nprint(\"=\" * 60)\n\n# Usar un conjunto pequeño para demostración\nX_demo = X_train[:20]\ny_demo = y_train[:20]\nX_test_demo = X_test[:10]\ny_test_demo = y_test[:10]\n\n# Entrenar nuestro modelo\nmodelo_propio = GaussianNBDesdesCero()\nmodelo_propio.entrenar(X_demo, y_demo)\n\n# Hacer predicciones\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PREDICCIONES\")\nprint(\"=\" * 60)\n\ny_pred_propio = modelo_propio.predecir(X_test_demo)\nprobabilidades = modelo_propio.predecir_probabilidades(X_test_demo)\n\n# Mostrar resultados detallados\nfor i in range(5):  # Mostrar solo las primeras 5\n    print(f\"\\nMuestra {i+1}:\")\n    print(f\"  Características: [{X_test_demo[i, 0]:.2f}, {X_test_demo[i, 1]:.2f}]\")\n    print(f\"  Probabilidades: P(Y=0|X)={probabilidades[i, 0]:.3f}, P(Y=1|X)={probabilidades[i, 1]:.3f}\")\n    print(f\"  Predicción: {y_pred_propio[i]}\")\n    print(f\"  Valor real: {y_test_demo[i]}\")\n    print(f\"  {'✓ Correcto' if y_pred_propio[i] == y_test_demo[i] else '✗ Incorrecto'}\")\n\n============================================================\nIMPLEMENTACIÓN DESDE CERO\n============================================================\nEntrenando con 20 muestras y 2 características\nClases encontradas: [0 1]\n\nClase 0: 8 muestras (40.0%)\n  Medias: [ 0.02629741 -1.03784986]\n  Varianzas: [0.69718407 0.29135071]\n\nClase 1: 12 muestras (60.0%)\n  Medias: [0.35445188 0.56734888]\n  Varianzas: [1.40440918 0.69226573]\n\n============================================================\nPREDICCIONES\n============================================================\n\nMuestra 1:\n  Características: [-0.43, -0.48]\n  Probabilidades: P(Y=0|X)=0.672, P(Y=1|X)=0.328\n  Predicción: 0\n  Valor real: 0\n  ✓ Correcto\n\nMuestra 2:\n  Características: [0.85, -0.84]\n  Probabilidades: P(Y=0|X)=0.792, P(Y=1|X)=0.208\n  Predicción: 0\n  Valor real: 0\n  ✓ Correcto\n\nMuestra 3:\n  Características: [-0.60, -1.15]\n  Probabilidades: P(Y=0|X)=0.926, P(Y=1|X)=0.074\n  Predicción: 0\n  Valor real: 0\n  ✓ Correcto\n\nMuestra 4:\n  Características: [0.96, 1.52]\n  Probabilidades: P(Y=0|X)=0.000, P(Y=1|X)=1.000\n  Predicción: 1\n  Valor real: 0\n  ✗ Incorrecto\n\nMuestra 5:\n  Características: [0.72, -0.68]\n  Probabilidades: P(Y=0|X)=0.727, P(Y=1|X)=0.273\n  Predicción: 0\n  Valor real: 0\n  ✓ Correcto\n\n\n\n# Comparación con scikit-learn\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARACIÓN CON SCIKIT-LEARN\")\nprint(\"=\" * 60)\n\n# Entrenar modelo de scikit-learn con los mismos datos\ngnb_sklearn_demo = GaussianNB()\ngnb_sklearn_demo.fit(X_demo, y_demo)\ny_pred_sklearn_demo = gnb_sklearn_demo.predict(X_test_demo)\n\n# Comparar resultados\nprint(\"\\nPredicciones:\")\nprint(f\"  Implementación propia: {y_pred_propio}\")\nprint(f\"  Scikit-learn:         {y_pred_sklearn_demo}\")\nprint(f\"  Valores reales:       {y_test_demo}\")\n\n# Calcular accuracy\nacc_propio = np.mean(y_pred_propio == y_test_demo)\nacc_sklearn = np.mean(y_pred_sklearn_demo == y_test_demo)\n\nprint(f\"\\nAccuracy:\")\nprint(f\"  Implementación propia: {acc_propio:.3f}\")\nprint(f\"  Scikit-learn:         {acc_sklearn:.3f}\")\n\n# Verificar que los parámetros aprendidos son similares\nprint(\"\\n\" + \"=\" * 60)\nprint(\"VERIFICACIÓN DE PARÁMETROS APRENDIDOS\")\nprint(\"=\" * 60)\n\nfor clase in [0, 1]:\n    print(f\"\\nClase {clase}:\")\n    print(f\"  Priors:\")\n    print(f\"    Propio: {modelo_propio.priors[clase]:.3f}\")\n    print(f\"    Sklearn: {gnb_sklearn_demo.class_prior_[clase]:.3f}\")\n    print(f\"  Medias (primera característica):\")\n    print(f\"    Propio: {modelo_propio.medias[clase][0]:.3f}\")\n    print(f\"    Sklearn: {gnb_sklearn_demo.theta_[clase, 0]:.3f}\")\n\n\n============================================================\nCOMPARACIÓN CON SCIKIT-LEARN\n============================================================\n\nPredicciones:\n  Implementación propia: [0 0 0 1 0 0 0 0 1 1]\n  Scikit-learn:         [0 0 0 1 0 0 0 0 1 1]\n  Valores reales:       [0 0 0 0 0 0 0 0 0 1]\n\nAccuracy:\n  Implementación propia: 0.800\n  Scikit-learn:         0.800\n\n============================================================\nVERIFICACIÓN DE PARÁMETROS APRENDIDOS\n============================================================\n\nClase 0:\n  Priors:\n    Propio: 0.400\n    Sklearn: 0.400\n  Medias (primera característica):\n    Propio: 0.026\n    Sklearn: 0.026\n\nClase 1:\n  Priors:\n    Propio: 0.600\n    Sklearn: 0.600\n  Medias (primera característica):\n    Propio: 0.354\n    Sklearn: 0.354\n\n\n\n\n\nCuándo Usar Naive Bayes\nNaive Bayes es particularmente efectivo en:\n\nClasificación de texto y procesamiento de lenguaje natural\n\nFiltrado de spam\nAnálisis de sentimientos\nCategorización de documentos\n\nSistemas de recomendación\n\nPredicción de preferencias basada en características\n\nDiagnóstico médico\n\nCuando las características son síntomas relativamente independientes\n\nAplicaciones en tiempo real\n\nCuando se necesitan predicciones muy rápidas\n\nConjuntos de datos pequeños\n\nCuando hay pocos ejemplos de entrenamiento por clase\n\n\nEl clasificador Naive Bayes, a pesar de su simplicidad, sigue siendo uno de los algoritmos fundamentales en machine learning, especialmente valioso como baseline y en aplicaciones donde la velocidad y simplicidad son críticas.\n\n\n\nRegresión Logística\nLa regresión logística es uno de los modelos más utilizados para clasificación binaria. Modela directamente la probabilidad posterior usando una transformación logística de una combinación lineal de las características.\n\nModelo\nLa regresión logística modela la probabilidad de que \\(Y = 1\\) como:\n\\[P(Y = 1 | \\mathbf{X} = \\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p)}} = \\frac{1}{1 + e^{-\\mathbf{x}^T\\boldsymbol{\\beta}}}\\]\nEsta función se conoce como función sigmoide o logística:\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z}\\]\n\n\nTransformación Logit\nEl modelo puede reescribirse usando la transformación logit (log-odds):\n\\[\\log\\left(\\frac{P(Y = 1 | \\mathbf{x})}{P(Y = 0 | \\mathbf{x})}\\right) = \\log\\left(\\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})}\\right) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\\]\nEsto muestra que el log-odds es una función lineal de las características.\n\n\nEstimación de Parámetros\nLos parámetros \\(\\boldsymbol{\\beta}\\) se estiman maximizando la verosimilitud. Para \\(n\\) observaciones:\n\\[L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p(\\mathbf{x}_i)^{y_i} \\cdot (1-p(\\mathbf{x}_i))^{1-y_i}\\]\nTomando el logaritmo:\n\\[\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} [y_i \\log(p(\\mathbf{x}_i)) + (1-y_i) \\log(1-p(\\mathbf{x}_i))]\\]\nEsta es exactamente la negativa de la pérdida logarítmica. No existe solución analítica, por lo que se utiliza optimización numérica (típicamente Newton-Raphson o gradiente descendente).\n\n\nFrontera de Decisión\nLa frontera de decisión en regresión logística es lineal en el espacio de características:\n\\[\\{\\mathbf{x} : P(Y = 1 | \\mathbf{x}) = 0.5\\} = \\{\\mathbf{x} : \\mathbf{x}^T\\boldsymbol{\\beta} = 0\\}\\]\nEsto define un hiperplano que separa las dos clases.\n\n\nEjemplo en Python\n\n\n\n\n\nRegresión logística: datos, probabilidades y frontera de decisión\n\n\n\n\nIntercepto (β₀): 0.057\nCoeficientes: β₁ = -0.273, β₂ = 2.214\n\n\n\n\nInterpretación de Coeficientes\n\nConceptos Fundamentales: Odds y Log-Odds\nAntes de interpretar los coeficientes, definamos los conceptos clave:\nOdds (momios o chances): La razón entre la probabilidad de éxito y la probabilidad de fracaso:\n\\[\\text{Odds} = \\frac{P(Y = 1)}{P(Y = 0)} = \\frac{p}{1-p}\\]\nSi \\(p = 0.75\\), entonces los odds son \\(\\frac{0.75}{0.25} = 3\\), es decir, el éxito es 3 veces más probable que el fracaso.\nLog-odds (logit): El logaritmo natural de los odds:\n\\[\\text{Log-odds} = \\log\\left(\\frac{p}{1-p}\\right) = \\text{logit}(p)\\]\n\n\nDerivación Matemática\nPartiendo del modelo de regresión logística:\n\\[P(Y = 1 | \\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\sum_{j=1}^p \\beta_j x_j)}}\\]\nCalculemos el log-odds:\n\\[\\log\\left(\\frac{P(Y = 1 | \\mathbf{x})}{1 - P(Y = 1 | \\mathbf{x})}\\right) = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j\\]\nAhora, consideremos qué sucede cuando incrementamos \\(x_k\\) en una unidad (de \\(x_k\\) a \\(x_k + 1\\)):\nLog-odds original: \\[L_0 = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k + ... + \\beta_p x_p\\]\nLog-odds después del incremento: \\[L_1 = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k (x_k + 1) + ... + \\beta_p x_p\\]\nCambio en log-odds: \\[\\Delta L = L_1 - L_0 = \\beta_k\\]\nPor lo tanto, \\(\\beta_k\\) representa el cambio en log-odds cuando \\(x_k\\) aumenta en una unidad.\n\n\nOdds Ratio\nEl odds ratio compara los odds antes y después del cambio:\n\\[\\text{Odds ratio} = \\frac{\\text{Odds}_{\\text{nuevo}}}{\\text{Odds}_{\\text{original}}} = \\frac{e^{L_1}}{e^{L_0}} = e^{L_1 - L_0} = e^{\\beta_k}\\]\nEsto significa que \\(e^{\\beta_k}\\) es el factor por el cual se multiplican los odds cuando \\(x_k\\) aumenta en una unidad.\n\n\nEjemplo Práctico: Clicks en Memes y Edad\nImaginemos un estudio sobre la probabilidad de que una persona haga click en un meme según su edad. Nuestro modelo de regresión logística es:\n\\[\\log\\left(\\frac{P(\\text{click} = 1)}{P(\\text{click} = 0)}\\right) = 2.5 - 0.08 \\cdot \\text{edad}\\]\nDonde: - \\(\\beta_0 = 2.5\\) (intercepto) - \\(\\beta_{\\text{edad}} = -0.08\\) (coeficiente de edad)\nInterpretaciones:\n\nCoeficiente \\(\\beta_{\\text{edad}} = -0.08\\):\n\nPor cada año adicional de edad, el log-odds de hacer click disminuye en 0.08\nEl signo negativo indica que personas mayores tienen menor probabilidad de hacer click\n\nOdds ratio \\(e^{-0.08} \\approx 0.923\\):\n\nPor cada año adicional de edad, los odds de hacer click se multiplican por 0.923\nEquivalentemente: los odds disminuyen un 7.7% por cada año adicional\n\nEjemplo numérico concreto:\n\nPara una persona de 20 años: \\[\\text{Log-odds}_{20} = 2.5 - 0.08(20) = 0.9\\] \\[\\text{Odds}_{20} = e^{0.9} \\approx 2.46\\] \\[P(\\text{click})_{20} = \\frac{2.46}{1 + 2.46} \\approx 0.71\\]\nPara una persona de 30 años: \\[\\text{Log-odds}_{30} = 2.5 - 0.08(30) = 0.1\\] \\[\\text{Odds}_{30} = e^{0.1} \\approx 1.11\\] \\[P(\\text{click})_{30} = \\frac{1.11}{1 + 1.11} \\approx 0.53\\]\nVerificación del odds ratio: \\[\\frac{\\text{Odds}_{30}}{\\text{Odds}_{20}} = \\frac{1.11}{2.46} \\approx 0.45 = e^{-0.08 \\times 10} = (e^{-0.08})^{10}\\]\nEsto confirma que en 10 años (de 20 a 30), los odds se multiplican por \\((0.923)^{10} \\approx 0.45\\).\n\n\nResumen de Interpretaciones\n\n\n\n\n\n\n\n\nParámetro\nInterpretación\nEjemplo (edad y clicks)\n\n\n\n\n\\(\\beta_j &gt; 0\\)\nVariable aumenta log-odds\nLos jóvenes clickean más\n\n\n\\(\\beta_j &lt; 0\\)\nVariable disminuye log-odds\nLos mayores clickean menos\n\n\n\\(\\beta_j\\)\nCambio en log-odds por unidad\n-0.08: cada año reduce log-odds\n\n\n\\(e^{\\beta_j} &gt; 1\\)\nOdds aumentan\n-\n\n\n\\(e^{\\beta_j} &lt; 1\\)\nOdds disminuyen\n0.923: odds bajan 7.7% por año\n\n\n\\(e^{\\beta_j} = 2\\)\nOdds se duplican\n-\n\n\n\\(e^{\\beta_j} = 0.5\\)\nOdds se reducen a la mitad\n-",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clasificación</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and\nJonathan Taylor. 2023. An Introduction to\nStatistical Learning: With Applications in Python. Springer\nTexts in Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Referencias"
    ]
  },
  {
    "objectID": "introduccion.html",
    "href": "introduccion.html",
    "title": "Introducción a Python para Minería de Datos",
    "section": "",
    "text": "1. Librerías Necesarias\nEste notebook cubre los fundamentos de Python necesarios para minería de datos, incluyendo:\nPrimero importamos todas las librerías que usaremos en este notebook:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#fundamentos-de-numpy",
    "href": "introduccion.html#fundamentos-de-numpy",
    "title": "Introducción a Python para Minería de Datos",
    "section": "2. Fundamentos de NumPy",
    "text": "2. Fundamentos de NumPy\nNumPy es la librería fundamental para computación científica en Python. Proporciona arrays multidimensionales eficientes.\n\n2.1 Creación de Arrays\n\n# Creamos un array de 100 números igualmente espaciados entre 0 y 10\nX = np.linspace(0, 10, 100)\nprint(\"Primeros 5 elementos:\", X[:5])\nprint(\"Forma del array:\", X.shape)\nprint(\"Últimos 10 elementos:\", X[90:])\n\nPrimeros 5 elementos: [0.        0.1010101 0.2020202 0.3030303 0.4040404]\nForma del array: (100,)\nÚltimos 10 elementos: [ 9.09090909  9.19191919  9.29292929  9.39393939  9.49494949  9.5959596\n  9.6969697   9.7979798   9.8989899  10.        ]\n\n\n\n\n2.2 Operaciones con Arrays y Generación de Datos\n\n# Acceso a elementos individuales\nprint(\"Elemento en posición 1:\", X[1])\n\n# Generación de datos sintéticos para regresión lineal\n# y = 0.5*x + 10 + ruido_gaussiano\ny_linear = 0.5 * X + 10 + np.random.normal(loc=0, scale=1.5, size=100)\nprint(\"Primeros 5 valores de y:\", y_linear[:5])\n\nElemento en posición 1: 0.10101010101010101\nPrimeros 5 valores de y: [10.947363    7.72186528 10.63466357 10.19993526 11.22329185]",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#fundamentos-de-pandas",
    "href": "introduccion.html#fundamentos-de-pandas",
    "title": "Introducción a Python para Minería de Datos",
    "section": "3. Fundamentos de Pandas",
    "text": "3. Fundamentos de Pandas\nPandas es la librería principal para manipulación y análisis de datos estructurados en Python.\n\n3.1 Creación de DataFrames\n\n# Creamos un DataFrame con nuestros datos\ndf = pd.DataFrame({\"x\": X, \"y\": y_linear})\nprint(\"Primeras 5 filas del DataFrame:\")\nprint(df.head())\nprint(\"\\nInformación del DataFrame:\")\nprint(df.info())\n\nPrimeras 5 filas del DataFrame:\n         x          y\n0  0.00000  10.947363\n1  0.10101   7.721865\n2  0.20202  10.634664\n3  0.30303  10.199935\n4  0.40404  11.223292\n\nInformación del DataFrame:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   x       100 non-null    float64\n 1   y       100 non-null    float64\ndtypes: float64(2)\nmemory usage: 1.7 KB\nNone\n\n\n\n\n3.2 Manipulación de DataFrames\n\n# Filtrado de datos\nprint(\"Filas donde y &gt; 12:\")\ndf_filtrado = df.query(\"y &gt; 12\")\nprint(df_filtrado)\n\n# Estadísticas descriptivas\nprint(\"\\nCuantiles de la variable y:\")\nprint(df.y.quantile(q=[0.25, 0.5, 0.75, 0.99]))\n\nFilas donde y &gt; 12:\n            x          y\n10   1.010101  12.755558\n17   1.717172  16.315991\n28   2.828283  12.890289\n31   3.131313  12.095512\n33   3.333333  12.186190\n35   3.535354  16.519562\n38   3.838384  14.936178\n39   3.939394  12.445576\n43   4.343434  13.407873\n44   4.444444  14.740559\n46   4.646465  14.085751\n47   4.747475  13.406937\n48   4.848485  12.711032\n49   4.949495  14.302195\n51   5.151515  13.419018\n53   5.353535  14.202517\n54   5.454545  13.460248\n55   5.555556  13.570844\n56   5.656566  15.336198\n58   5.858586  13.248156\n59   5.959596  12.120376\n60   6.060606  13.536254\n61   6.161616  13.877174\n62   6.262626  12.643431\n64   6.464646  13.144207\n65   6.565657  16.558753\n66   6.666667  14.174915\n67   6.767677  13.188301\n68   6.868687  15.327148\n69   6.969697  14.229089\n70   7.070707  14.671629\n71   7.171717  15.430842\n72   7.272727  14.078094\n74   7.474747  14.913045\n75   7.575758  16.341642\n76   7.676768  13.882098\n77   7.777778  14.801524\n78   7.878788  16.116168\n79   7.979798  14.168691\n80   8.080808  12.395476\n81   8.181818  12.295980\n82   8.282828  15.132112\n83   8.383838  12.813598\n84   8.484848  14.631908\n86   8.686869  14.303784\n87   8.787879  13.279560\n88   8.888889  12.344803\n89   8.989899  12.711722\n91   9.191919  16.917684\n92   9.292929  14.830744\n93   9.393939  15.292800\n95   9.595960  12.746624\n96   9.696970  16.000313\n97   9.797980  15.209167\n98   9.898990  14.825473\n99  10.000000  13.841673\n\nCuantiles de la variable y:\n0.25    11.154739\n0.50    12.420526\n0.75    14.181815\n0.99    16.562342\nName: y, dtype: float64\n\n\n\n# Renombramiento de columnas\nprint(\"Renombrando columnas...\")\ndf_renamed = df.rename(columns={\"x\": \"variable_independiente\", \"y\": \"variable_dependiente\"})\nprint(df_renamed.head())\n\n# Volvemos a los nombres originales para el resto del notebook\ndf = df.rename(columns={\"x\": \"x\", \"y\": \"y\"})\n\nRenombrando columnas...\n   variable_independiente  variable_dependiente\n0                 0.00000             10.947363\n1                 0.10101              7.721865\n2                 0.20202             10.634664\n3                 0.30303             10.199935\n4                 0.40404             11.223292",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#iteración-en-python",
    "href": "introduccion.html#iteración-en-python",
    "title": "Introducción a Python para Minería de Datos",
    "section": "4. Iteración en Python",
    "text": "4. Iteración en Python\n\n4.1 Iteración sobre Diccionarios\n\n# Ejemplo de diccionario con diferentes tipos de datos\ndiccionario_ejemplo = {\n    \"entero\": 42,\n    \"array_numpy\": np.array([1, 2, 3, 4]),\n    \"cadena\": \"minería de datos\",\n    \"flotante\": 3.14159\n}\n\n# Iteración sobre diccionarios\nprint(\"Iterando sobre el diccionario:\")\nfor clave, valor in diccionario_ejemplo.items():\n    print(f\"Clave: {clave} | Valor: {valor} | Tipo: {type(valor).__name__}\")\n\nIterando sobre el diccionario:\nClave: entero | Valor: 42 | Tipo: int\nClave: array_numpy | Valor: [1 2 3 4] | Tipo: ndarray\nClave: cadena | Valor: minería de datos | Tipo: str\nClave: flotante | Valor: 3.14159 | Tipo: float\n\n\n\n\n4.2 Iteración sobre Listas\n\n# Ejemplo con lista de números\nnumeros = [1, 4, 9, 16, 25]\nprint(\"Iterando sobre lista de números:\")\nfor i, numero in enumerate(numeros):\n    print(f\"Posición {i}: {numero}, raíz cuadrada: {np.sqrt(numero):.2f}\")\n    \n# Ejemplo con diferentes valores de k para KNN\nvalores_k = [1, 5, 10, 20, 50]\nprint(\"\\nIterando sobre valores de k:\")\nfor k in valores_k:\n    print(f\"Valor de k: {k}\")\n\nIterando sobre lista de números:\nPosición 0: 1, raíz cuadrada: 1.00\nPosición 1: 4, raíz cuadrada: 2.00\nPosición 2: 9, raíz cuadrada: 3.00\nPosición 3: 16, raíz cuadrada: 4.00\nPosición 4: 25, raíz cuadrada: 5.00\n\nIterando sobre valores de k:\nValor de k: 1\nValor de k: 5\nValor de k: 10\nValor de k: 20\nValor de k: 50",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#visualización-con-matplotlib-y-seaborn",
    "href": "introduccion.html#visualización-con-matplotlib-y-seaborn",
    "title": "Introducción a Python para Minería de Datos",
    "section": "5. Visualización con Matplotlib y Seaborn",
    "text": "5. Visualización con Matplotlib y Seaborn\n\n5.1 Gráfica de Dispersión Básica\n\n# Gráfica de dispersión básica\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x=\"x\", y=\"y\")\nplt.title(\"Relación entre x y y\")\nplt.xlabel(\"Variable independiente (x)\")\nplt.ylabel(\"Variable dependiente (y)\")\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#funciones-con-type-hints",
    "href": "introduccion.html#funciones-con-type-hints",
    "title": "Introducción a Python para Minería de Datos",
    "section": "6. Funciones con Type Hints",
    "text": "6. Funciones con Type Hints\n\n6.1 Función para Generar Datos\n\ndef generar_datos_lineales(\n    n: int = 100,\n    slope: float = 0.5,\n    intercept: float = 1.0,\n    noise_scale: float = 1.5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Genera datos sintéticos para regresión lineal.\n    \n    Parámetros:\n    -----------\n    n : int\n        Número de puntos de datos a generar\n    slope : float\n        Pendiente de la relación lineal\n    intercept : float\n        Intercepto de la relación lineal\n    noise_scale : float\n        Desviación estándar del ruido gaussiano\n    \n    Retorna:\n    --------\n    pd.DataFrame\n        DataFrame con columnas 'x' y 'y'\n    \"\"\"\n    X = np.linspace(0, 10, n)\n    y = slope * X + intercept + np.random.normal(scale=noise_scale, size=n)\n    return pd.DataFrame({\"x\": X, \"y\": y})\n\n# Ejemplo de uso\ndf_nuevo = generar_datos_lineales(n=50, slope=2.0, intercept=5.0)\nprint(\"Datos generados:\")\nprint(df_nuevo.head())\n\nDatos generados:\n          x         y\n0  0.000000  6.962633\n1  0.204082  5.261446\n2  0.408163  6.349024\n3  0.612245  6.767730\n4  0.816327  5.768821\n\n\n\n\n6.2 Función para Visualización\n\ndef crear_grafica_dispersion(\n    data_frame: pd.DataFrame,\n    x_col: str = \"x\",\n    y_col: str = \"y\",\n    titulo: str = \"Gráfica de Dispersión\",\n    nombre_eje_x: str = \"X\",\n    nombre_eje_y: str = \"Y\"\n) -&gt; None:\n    \"\"\"\n    Crea una gráfica de dispersión con formato personalizado.\n    \n    Parámetros:\n    -----------\n    data_frame : pd.DataFrame\n        DataFrame que contiene los datos\n    x_col : str\n        Nombre de la columna para el eje x\n    y_col : str\n        Nombre de la columna para el eje y\n    titulo : str\n        Título de la gráfica\n    nombre_eje_x : str\n        Etiqueta del eje x\n    nombre_eje_y : str\n        Etiqueta del eje y\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.scatterplot(data=data_frame, x=x_col, y=y_col)\n    plt.title(titulo)\n    plt.xlabel(nombre_eje_x)\n    plt.ylabel(nombre_eje_y)\n    plt.show()\n\n# Ejemplo de uso\ncrear_grafica_dispersion(\n    df_nuevo, \n    titulo=\"Datos Sintéticos Generados\",\n    nombre_eje_x=\"Variable Independiente\",\n    nombre_eje_y=\"Variable Dependiente\"\n)",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#regresión-lineal",
    "href": "introduccion.html#regresión-lineal",
    "title": "Introducción a Python para Minería de Datos",
    "section": "7. Regresión Lineal",
    "text": "7. Regresión Lineal\n\n7.1 Implementación y Ajuste del Modelo\n\n# Generamos datos para trabajar\ndf = generar_datos_lineales(n=1000, slope=1.0, intercept=2.0, noise_scale=1.5)\n\n# 1. Crear el modelo de regresión lineal\nmodelo_lr = LinearRegression()\n\n# 2. Ajustar el modelo (entrenamiento)\n# Nota: sklearn necesita X como matriz (2D) y y como vector (1D)\nX_features = df[[\"x\"]]  # Matriz 2D\ny_target = df[\"y\"]     # Vector 1D\n\nmodelo_lr.fit(X_features, y_target)\n\nprint(f\"Coeficiente (pendiente): {modelo_lr.coef_[0]:.3f}\")\nprint(f\"Intercepto: {modelo_lr.intercept_:.3f}\")\n\nCoeficiente (pendiente): 1.006\nIntercepto: 1.935\n\n\n\n\n7.2 Predicciones y Evaluación\n\n# 3. Hacer predicciones\ny_pred_lr = modelo_lr.predict(X_features)\n\n# 4. Evaluar el modelo\nmse_lr = mean_squared_error(y_target, y_pred_lr)\nprint(f\"Error Cuadrático Medio (MSE): {mse_lr:.3f}\")\n\n# Agregar predicciones al DataFrame para visualización\ndf[\"y_pred_lr\"] = y_pred_lr\n\nError Cuadrático Medio (MSE): 2.369\n\n\n\n\n7.3 Visualización del Modelo\n\n# Visualización de datos originales y línea de regresión\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"x\", y=\"y\", alpha=0.6, label=\"Datos Originales\")\nsns.lineplot(data=df, x=\"x\", y=\"y_pred_lr\", color=\"red\", linewidth=2, label=\"Regresión Lineal\")\nplt.title(\"Regresión Lineal: Datos vs Predicciones\")\nplt.xlabel(\"Variable Independiente (x)\")\nplt.ylabel(\"Variable Dependiente (y)\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#k-nearest-neighbors-knn",
    "href": "introduccion.html#k-nearest-neighbors-knn",
    "title": "Introducción a Python para Minería de Datos",
    "section": "8. K-Nearest Neighbors (KNN)",
    "text": "8. K-Nearest Neighbors (KNN)\n\n8.1 Implementación Básica de KNN\n\n# 1. Crear modelo KNN con k=10\nmodelo_knn = KNeighborsRegressor(n_neighbors=10)\n\n# 2. Ajustar el modelo\nmodelo_knn.fit(X_features, y_target)\n\n# 3. Hacer predicciones\ny_pred_knn = modelo_knn.predict(X_features)\n\n# 4. Evaluar el modelo\nmse_knn = mean_squared_error(y_target, y_pred_knn)\nprint(f\"MSE KNN (k=10): {mse_knn:.3f}\")\nprint(f\"MSE Regresión Lineal: {mse_lr:.3f}\")\n\n# Agregar predicciones al DataFrame\ndf[\"y_pred_knn10\"] = y_pred_knn\n\nMSE KNN (k=10): 2.055\nMSE Regresión Lineal: 2.369\n\n\n\n\n8.2 Función para Evaluar Diferentes Valores de K\n\ndef evaluar_knn_diferentes_k(\n    X: pd.DataFrame, \n    y: pd.Series, \n    valores_k: list\n) -&gt; dict:\n    \"\"\"\n    Evalúa el rendimiento de KNN para diferentes valores de k.\n    \n    Parámetros:\n    -----------\n    X : pd.DataFrame\n        Variables independientes\n    y : pd.Series\n        Variable dependiente\n    valores_k : list\n        Lista de valores de k a evaluar\n    \n    Retorna:\n    --------\n    dict\n        Diccionario con k como clave y MSE como valor\n    \"\"\"\n    resultados = {}\n    \n    for k in valores_k:\n        # Crear y ajustar modelo\n        knn = KNeighborsRegressor(n_neighbors=k)\n        knn.fit(X, y)\n        \n        # Predicciones y evaluación\n        y_pred = knn.predict(X)\n        mse = mean_squared_error(y, y_pred)\n        \n        resultados[k] = mse\n        print(f\"k={k}: MSE={mse:.3f}\")\n    \n    return resultados\n\n# Evaluar diferentes valores de k\nvalores_k = [1, 5, 10, 20, 50, 100]\nprint(\"Evaluando diferentes valores de k:\")\nresultados_k = evaluar_knn_diferentes_k(X_features, y_target, valores_k)\n\nEvaluando diferentes valores de k:\nk=1: MSE=0.000\nk=5: MSE=1.837\nk=10: MSE=2.055\nk=20: MSE=2.206\nk=50: MSE=2.335\nk=100: MSE=2.367\n\n\n\n\n8.3 Visualización de Diferentes Valores de K\n\n# Crear DataFrame para comparar diferentes valores de k\ndf_comparacion = df[[\"x\", \"y\"]].copy()\n\n# Agregar predicciones para diferentes valores de k\nfor k in [1, 10, 100]:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_features, y_target)\n    df_comparacion[f\"y_pred_k{k}\"] = knn.predict(X_features)\n\n# Visualización comparativa\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, k in enumerate([1, 10, 100]):\n    ax = axes[i]\n    ax.scatter(df_comparacion[\"x\"], df_comparacion[\"y\"], alpha=0.6, label=\"Datos\")\n    ax.plot(df_comparacion[\"x\"], df_comparacion[f\"y_pred_k{k}\"], 'r-', linewidth=2, label=f\"KNN k={k}\")\n    ax.set_title(f\"KNN con k={k}\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#selección-del-valor-óptimo-de-k",
    "href": "introduccion.html#selección-del-valor-óptimo-de-k",
    "title": "Introducción a Python para Minería de Datos",
    "section": "9. Selección del Valor Óptimo de K",
    "text": "9. Selección del Valor Óptimo de K\n\n9.1 Evaluación Sistemática de K\n\n# Generar un dataset más grande para la evaluación\ndf_grande = generar_datos_lineales(n=1000, slope=2.0, intercept=3.0, noise_scale=2.0)\nX_grande = df_grande[[\"x\"]]\ny_grande = df_grande[\"y\"]\n\n# Rango de valores k a evaluar\nmax_k = min(200, len(df_grande) // 5)  # k máximo razonable\nvalores_k_rango = np.arange(1, max_k, 10)\n\nprint(f\"Evaluando k desde 1 hasta {max_k-1} (cada 10 valores)\")\nprint(f\"Total de valores a evaluar: {len(valores_k_rango)}\")\n\nEvaluando k desde 1 hasta 199 (cada 10 valores)\nTotal de valores a evaluar: 20\n\n\n\n# Evaluación de todos los valores de k\nerrores_k = []\nfor k in valores_k_rango:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_grande, y_grande)\n    y_pred = knn.predict(X_grande)\n    mse = mean_squared_error(y_grande, y_pred)\n    errores_k.append(mse)\n\n# Encontrar el k óptimo\nk_optimo = valores_k_rango[np.argmin(errores_k)]\nerror_minimo = min(errores_k)\n\nprint(f\"Valor óptimo de k: {k_optimo}\")\nprint(f\"MSE mínimo: {error_minimo:.3f}\")\n\nValor óptimo de k: 1\nMSE mínimo: 0.000\n\n\n\n\n9.2 Visualización de la Curva de Error vs K\n\n# Gráfica de MSE vs k\nplt.figure(figsize=(10, 6))\nplt.plot(valores_k_rango, errores_k, 'b-', linewidth=2, marker='o', markersize=4)\nplt.axvline(x=k_optimo, color='r', linestyle='--', \n            label=f'k óptimo = {k_optimo}\\nMSE = {error_minimo:.3f}')\nplt.xlabel('Valor de k')\nplt.ylabel('Error Cuadrático Medio (MSE)')\nplt.title('Selección del Valor Óptimo de k en KNN')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#estimación-del-error-real",
    "href": "introduccion.html#estimación-del-error-real",
    "title": "Introducción a Python para Minería de Datos",
    "section": "10. Estimación del Error Real",
    "text": "10. Estimación del Error Real\n\n10.1 División Train-Test\n\n# Generar dataset para validación\nnp.random.seed(42)  # Para reproducibilidad\ndf_validacion = generar_datos_lineales(n=2000, slope=1.5, intercept=2.5, noise_scale=2.0)\n\n# División train-test\nX_val = df_validacion[[\"x\"]]\ny_val = df_validacion[\"y\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_val, y_val, test_size=0.3, random_state=42\n)\n\nprint(f\"Datos de entrenamiento: {X_train.shape[0]}\")\nprint(f\"Datos de prueba: {X_test.shape[0]}\")\n\nDatos de entrenamiento: 1400\nDatos de prueba: 600\n\n\n\n\n10.2 Función para Evaluación Train-Test\n\ndef evaluar_modelo_train_test(\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n    k: int\n) -&gt; tuple:\n    \"\"\"\n    Evalúa KNN usando división train-test.\n    \n    Retorna:\n    --------\n    tuple\n        (error_entrenamiento, error_prueba)\n    \"\"\"\n    # Entrenar el modelo\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    # Predicciones\n    y_train_pred = knn.predict(X_train)\n    y_test_pred = knn.predict(X_test)\n    \n    # Errores\n    error_train = mean_squared_error(y_train, y_train_pred)\n    error_test = mean_squared_error(y_test, y_test_pred)\n    \n    return error_train, error_test\n\n# Evaluar diferentes valores de k\nvalores_k_test = [1, 5, 10, 20, 50, 100]\nerrores_train = []\nerrores_test = []\n\nprint(\"Evaluación con división train-test:\")\nprint(\"-\" * 40)\nfor k in valores_k_test:\n    error_train, error_test = evaluar_modelo_train_test(X_train, X_test, y_train, y_test, k)\n    errores_train.append(error_train)\n    errores_test.append(error_test)\n    print(f\"k={k:2d} | Train MSE: {error_train:.3f} | Test MSE: {error_test:.3f}\")\n\nEvaluación con división train-test:\n----------------------------------------\nk= 1 | Train MSE: 0.000 | Test MSE: 7.600\nk= 5 | Train MSE: 3.092 | Test MSE: 5.007\nk=10 | Train MSE: 3.436 | Test MSE: 4.425\nk=20 | Train MSE: 3.641 | Test MSE: 4.214\nk=50 | Train MSE: 3.820 | Test MSE: 4.016\nk=100 | Train MSE: 3.874 | Test MSE: 3.975\n\n\n\n\n10.3 Visualización de Sesgo vs Varianza\n\n# Gráfica de error de entrenamiento vs error de prueba\nplt.figure(figsize=(10, 6))\nplt.plot(valores_k_test, errores_train, 'b-o', linewidth=2, label='Error de Entrenamiento')\nplt.plot(valores_k_test, errores_test, 'r-o', linewidth=2, label='Error de Prueba')\nplt.xlabel('Valor de k')\nplt.ylabel('Error Cuadrático Medio (MSE)')\nplt.title('Curva de Validación: Error de Entrenamiento vs Error de Prueba')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')  # Escala logarítmica para mejor visualización\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.4 Comparación Final: Regresión Lineal vs KNN Óptimo\n\n# Encontrar el k óptimo basado en error de prueba\nk_optimo_test = valores_k_test[np.argmin(errores_test)]\nprint(f\"Valor óptimo de k (basado en error de prueba): {k_optimo_test}\")\n\n# Evaluar regresión lineal\nlr_final = LinearRegression()\nlr_final.fit(X_train, y_train)\ny_test_pred_lr = lr_final.predict(X_test)\nerror_test_lr = mean_squared_error(y_test, y_test_pred_lr)\n\n# Evaluar KNN óptimo\nknn_final = KNeighborsRegressor(n_neighbors=k_optimo_test)\nknn_final.fit(X_train, y_train)\ny_test_pred_knn = knn_final.predict(X_test)\nerror_test_knn = mean_squared_error(y_test, y_test_pred_knn)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"COMPARACIÓN FINAL - ERROR DE PRUEBA (ESTIMACIÓN REAL)\")\nprint(\"=\"*50)\nprint(f\"Regresión Lineal:        {error_test_lr:.3f}\")\nprint(f\"KNN (k={k_optimo_test}):             {error_test_knn:.3f}\")\nprint(\"=\"*50)\n\nif error_test_knn &lt; error_test_lr:\n    mejora = ((error_test_lr - error_test_knn) / error_test_lr) * 100\n    print(f\"KNN es mejor por {mejora:.1f}%\")\nelse:\n    mejora = ((error_test_knn - error_test_lr) / error_test_knn) * 100\n    print(f\"Regresión Lineal es mejor por {mejora:.1f}%\")\n\nValor óptimo de k (basado en error de prueba): 100\n\n==================================================\nCOMPARACIÓN FINAL - ERROR DE PRUEBA (ESTIMACIÓN REAL)\n==================================================\nRegresión Lineal:        3.887\nKNN (k=100):             3.975\n==================================================\nRegresión Lineal es mejor por 2.2%",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "introduccion.html#resumen",
    "href": "introduccion.html#resumen",
    "title": "Introducción a Python para Minería de Datos",
    "section": "Resumen",
    "text": "Resumen\nEn este notebook hemos cubierto:\n\nNumPy: Creación y manipulación de arrays para datos numéricos\nPandas: Manipulación de datos estructurados con DataFrames\nIteración: Técnicas para iterar sobre diccionarios y listas\nVisualización: Creación de gráficas informativas con matplotlib/seaborn\nType Hints: Definición de funciones bien documentadas y tipadas\nRegresión Lineal: Implementación, ajuste y evaluación\nKNN: Implementación y comparación de diferentes valores de k\nSelección de Hiperparámetros: Técnicas para encontrar el k óptimo\nValidación: Estimación del error real usando división train-test\n\n\nConceptos Clave Aprendidos:\n\nSesgo vs Varianza: KNN con k pequeño tiene alta varianza, k grande tiene alto sesgo\nValidación: El error de entrenamiento subestima el error real\nSelección de Modelos: Comparar diferentes algoritmos usando datos de prueba\nType Hints: Mejoran la legibilidad y mantenibilidad del código",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a Python para Minería de Datos</span>"
    ]
  },
  {
    "objectID": "regresion_lineal.html",
    "href": "regresion_lineal.html",
    "title": "Descenso en gradiente con regresión lineal",
    "section": "",
    "text": "Implementación multivariada\ndef generar_datos_lineales_multivariados(n_muestras: int, n_caracteristicas: int, ruido: float = 0.5) -&gt; tuple[pd.DataFrame, np.ndarray, float]:\n    \"\"\"\n    Genera un conjunto de datos sintético para regresión lineal multivariada.\n\n    Args:\n        n_muestras (int): El número de puntos de datos a generar (filas).\n        n_caracteristicas (int): El número de variables independientes (características).\n        ruido (float): La desviación estándar del ruido gaussiano a añadir.\n                       Controla la dispersión de los puntos.\n\n    Returns:\n        tuple[pd.DataFrame, np.ndarray, float]:\n            - Un DataFrame de pandas con las características (x1, x2, ...) y la variable objetivo (y).\n            - El array de coeficientes (pesos) reales que se usaron para generar los datos.\n            - El intercepto (sesgo) real que se usó.\n    \"\"\"\n    # 1. Generar las características (X) con valores aleatorios entre 0 y 10\n    X = 10 * np.random.rand(n_muestras, n_caracteristicas)\n\n    # 2. Generar coeficientes e intercepto reales aleatorios\n    # Estos son los \"verdaderos\" parámetros que un modelo intentaría encontrar.\n    coeficientes_reales = np.random.randn(n_caracteristicas) * 2\n    intercepto_real = np.random.randn() * 5\n\n    # 3. Generar el ruido gaussiano\n    # El ruido simula la variabilidad aleatoria en los datos del mundo real.\n    ruido_gaussiano = np.random.randn(n_muestras) * ruido\n\n    # 4. Calcular la variable objetivo (y) usando la ecuación lineal\n    # y = (X • coeficientes) + intercepto + ruido\n    y = np.dot(X, coeficientes_reales) + intercepto_real + ruido_gaussiano\n\n    # 5. Formatear la salida en un DataFrame de pandas\n    nombres_columnas = [f'x{i+1}' for i in range(n_caracteristicas)]\n    datos = pd.DataFrame(X, columns=nombres_columnas)\n    datos['y'] = y\n\n    return datos, coeficientes_reales, intercepto_real\nnum_muestras = 200\nnum_caracteristicas = 10 \nnivel_ruido = 1.5\n\n# Generar los datos\ndatos_generados, coeficientes, intercepto = generar_datos_lineales_multivariados(\n    n_muestras=num_muestras,\n    n_caracteristicas=num_caracteristicas,\n    ruido=nivel_ruido\n)\ndef descenso_gradiente_multivariado(datos: pd.DataFrame, learning_rate: float, iteraciones: int) -&gt; tuple[np.ndarray, float, list]:\n    \"\"\"\n    Realiza el descenso de gradiente para una regresión lineal multivariada.\n\n    Args:\n        datos (pd.DataFrame): DataFrame que contiene las características y la variable objetivo 'y'.\n        learning_rate (float): La tasa de aprendizaje.\n        iteraciones (int): El número de iteraciones para ejecutar el algoritmo.\n\n    Returns:\n        tuple[np.ndarray, float, list]:\n            - El array de coeficientes (pesos) optimizados.\n            - El intercepto (sesgo) optimizado.\n            - Una lista con el historial del Error Cuadrático Medio (MSE) en cada iteración.\n    \"\"\"\n    # 1. Preparar los datos\n    X = datos.drop('y', axis=1).values  # Matriz de características\n    y = datos['y'].values              # Vector de la variable objetivo\n    n_muestras, n_caracteristicas = X.shape\n\n    # 2. Inicializar parámetros\n    coeficientes = np.zeros(n_caracteristicas)\n    intercepto = 0.0\n    historial_error = []\n\n    # 3. Iterar para optimizar los parámetros\n    for i in range(iteraciones):\n        # Calcular las predicciones (producto punto de X y coeficientes)\n        # Ecuación: y_pred = (X • coeficientes) + intercepto\n        y_pred = np.dot(X, coeficientes) + intercepto\n\n        # Calcular el Error Cuadrático Medio (MSE) y guardarlo\n        error = np.mean((y - y_pred) ** 2)\n        historial_error.append(error)\n\n        # Calcular los gradientes (derivadas parciales)\n        # El gradiente es la dirección de máximo ascenso del error.\n        # Lo calculamos de forma vectorizada para eficiencia.\n        D_coeficientes = -(2/n_muestras) * np.dot(X.T, (y - y_pred))\n        D_intercepto = -(2/n_muestras) * np.sum(y - y_pred)\n\n        # 4. Actualizar los parámetros (moverse en dirección opuesta al gradiente)\n        coeficientes = coeficientes - learning_rate * D_coeficientes\n        intercepto = intercepto - learning_rate * D_intercepto\n\n    return coeficientes, intercepto, historial_error\nnum_muestras = 200\nnum_caracteristicas = 3\nnivel_ruido = 1.5\ndatos_generados, coef_reales, int_real = generar_datos_lineales_multivariados(\n    n_muestras=num_muestras,\n    n_caracteristicas=num_caracteristicas,\n    ruido=nivel_ruido\n)\n\nX_original = datos_generados.drop('y', axis=1)\ny = datos_generados['y']\n\nscaler = StandardScaler()\nX_escalado = scaler.fit_transform(X_original)\n\ndatos_escalados = pd.DataFrame(X_escalado, columns=X_original.columns)\ndatos_escalados['y'] = y.values\n\nlearning_rate = 0.01\niteraciones = 1000\ncoef_aprendidos, int_aprendido, error_hist = descenso_gradiente_multivariado(\n    datos=datos_escalados,\n    learning_rate=learning_rate,\n    iteraciones=iteraciones\n)\n\nmodelo_sklearn = LinearRegression()\nmodelo_sklearn.fit(X_escalado, y)\ncoef_sklearn = modelo_sklearn.coef_\nint_sklearn = modelo_sklearn.intercept_\nprint(\"--- Comparación de Parámetros (con datos escalados) ---\")\nprint(f\"Intercepto GD: {int_aprendido:.4f}  |  Intercepto Sklearn: {int_sklearn:.4f}\")\nfor i in range(num_caracteristicas):\n    print(f\"Coef. x{i+1} GD: {coef_aprendidos[i]:.4f} |  Coef. x{i+1} Sklearn: {coef_sklearn[i]:.4f}\")\n\n--- Comparación de Parámetros (con datos escalados) ---\nIntercepto GD: -16.0492  |  Intercepto Sklearn: -16.0492\nCoef. x1 GD: -6.1331 |  Coef. x1 Sklearn: -6.1331\nCoef. x2 GD: -2.4005 |  Coef. x2 Sklearn: -2.4005\nCoef. x3 GD: -2.0273 |  Coef. x3 Sklearn: -2.0273",
    "crumbs": [
      "Ejemplos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Descenso en gradiente con regresión lineal</span>"
    ]
  }
]