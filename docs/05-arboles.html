<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Árboles de Decisión – Minería de Datos</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./04-clasificacion.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-2453fe3dad938b07a2e5eff64ea8abce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-c1367505ed6638c8d4e510e1459ae853.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-2453fe3dad938b07a2e5eff64ea8abce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Árboles de Decisión – Minería de Datos">
<meta property="og:description" content="">
<meta property="og:image" content="05-arboles_files/figure-html/feature-importance-output-1.png">
<meta property="og:site_name" content="Minería de Datos">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-arboles.html"><span class="chapter-title">Árboles de Decisión</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Minería de Datos</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Alternar modo oscuro"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Temario</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-requerimientos-computacion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Requerimientos computacionales</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduccion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-principios.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Principios de aprendizaje supervisado</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Regresion lineal</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-regresion_lineal.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Regresión lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./violaciones_supuestos_regresion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Violaciones de los Supuestos de Regresión Lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./analisis_advertising_dataset.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Análisis de Regresión Lineal con el Dataset Advertising</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ejercicio_wine_quality.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clasificacion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Clasificación</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-arboles.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Árboles de Decisión</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Ejemplos</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduccion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introducción a Python para Minería de Datos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regresion_lineal.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Descenso en gradiente con regresión lineal</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción">Introducción</a>
  <ul class="collapse">
  <li><a href="#motivación-limitaciones-de-los-métodos-lineales" id="toc-motivación-limitaciones-de-los-métodos-lineales" class="nav-link" data-scroll-target="#motivación-limitaciones-de-los-métodos-lineales">Motivación: Limitaciones de los Métodos Lineales</a></li>
  <li><a href="#estructura-de-un-árbol-de-decisión" id="toc-estructura-de-un-árbol-de-decisión" class="nav-link" data-scroll-target="#estructura-de-un-árbol-de-decisión">Estructura de un Árbol de Decisión</a></li>
  <li><a href="#ejemplo-visual-simple" id="toc-ejemplo-visual-simple" class="nav-link" data-scroll-target="#ejemplo-visual-simple">Ejemplo Visual Simple</a></li>
  </ul></li>
  <li><a href="#construcción-de-árboles-de-decisión" id="toc-construcción-de-árboles-de-decisión" class="nav-link" data-scroll-target="#construcción-de-árboles-de-decisión">Construcción de Árboles de Decisión</a>
  <ul class="collapse">
  <li><a href="#particionamiento-recursivo-del-espacio" id="toc-particionamiento-recursivo-del-espacio" class="nav-link" data-scroll-target="#particionamiento-recursivo-del-espacio">Particionamiento Recursivo del Espacio</a></li>
  <li><a href="#criterios-de-impureza" id="toc-criterios-de-impureza" class="nav-link" data-scroll-target="#criterios-de-impureza">Criterios de Impureza</a></li>
  <li><a href="#comparación-visual-de-criterios-de-impureza" id="toc-comparación-visual-de-criterios-de-impureza" class="nav-link" data-scroll-target="#comparación-visual-de-criterios-de-impureza">Comparación Visual de Criterios de Impureza</a></li>
  <li><a href="#algoritmo-de-construcción-cart" id="toc-algoritmo-de-construcción-cart" class="nav-link" data-scroll-target="#algoritmo-de-construcción-cart">Algoritmo de Construcción CART</a></li>
  <li><a href="#ejemplo-construcción-paso-a-paso" id="toc-ejemplo-construcción-paso-a-paso" class="nav-link" data-scroll-target="#ejemplo-construcción-paso-a-paso">Ejemplo: Construcción Paso a Paso</a></li>
  </ul></li>
  <li><a href="#sobreajuste-y-control-de-complejidad" id="toc-sobreajuste-y-control-de-complejidad" class="nav-link" data-scroll-target="#sobreajuste-y-control-de-complejidad">Sobreajuste y Control de Complejidad</a>
  <ul class="collapse">
  <li><a href="#el-problema-del-sobreajuste" id="toc-el-problema-del-sobreajuste" class="nav-link" data-scroll-target="#el-problema-del-sobreajuste">El Problema del Sobreajuste</a></li>
  <li><a href="#estrategias-de-control-de-complejidad" id="toc-estrategias-de-control-de-complejidad" class="nav-link" data-scroll-target="#estrategias-de-control-de-complejidad">Estrategias de Control de Complejidad</a></li>
  </ul></li>
  <li><a href="#interpretabilidad-y-análisis" id="toc-interpretabilidad-y-análisis" class="nav-link" data-scroll-target="#interpretabilidad-y-análisis">Interpretabilidad y Análisis</a>
  <ul class="collapse">
  <li><a href="#importancia-de-variables" id="toc-importancia-de-variables" class="nav-link" data-scroll-target="#importancia-de-variables">Importancia de Variables</a></li>
  <li><a href="#extracción-de-reglas" id="toc-extracción-de-reglas" class="nav-link" data-scroll-target="#extracción-de-reglas">Extracción de Reglas</a></li>
  </ul></li>
  <li><a href="#ventajas-y-desventajas" id="toc-ventajas-y-desventajas" class="nav-link" data-scroll-target="#ventajas-y-desventajas">Ventajas y Desventajas</a>
  <ul class="collapse">
  <li><a href="#ventajas-de-los-árboles-de-decisión" id="toc-ventajas-de-los-árboles-de-decisión" class="nav-link" data-scroll-target="#ventajas-de-los-árboles-de-decisión">Ventajas de los Árboles de Decisión</a></li>
  <li><a href="#desventajas-de-los-árboles-de-decisión" id="toc-desventajas-de-los-árboles-de-decisión" class="nav-link" data-scroll-target="#desventajas-de-los-árboles-de-decisión">Desventajas de los Árboles de Decisión</a></li>
  <li><a href="#comparación-visual-árbol-vs-regresión-logística" id="toc-comparación-visual-árbol-vs-regresión-logística" class="nav-link" data-scroll-target="#comparación-visual-árbol-vs-regresión-logística">Comparación Visual: Árbol vs Regresión Logística</a></li>
  </ul></li>
  <li><a href="#aplicación-práctica-dataset-real" id="toc-aplicación-práctica-dataset-real" class="nav-link" data-scroll-target="#aplicación-práctica-dataset-real">Aplicación Práctica: Dataset Real</a></li>
  <li><a href="#bagging-de-árboles" id="toc-bagging-de-árboles" class="nav-link" data-scroll-target="#bagging-de-árboles">Bagging de Árboles</a>
  <ul class="collapse">
  <li><a href="#motivación-el-problema-de-la-alta-varianza" id="toc-motivación-el-problema-de-la-alta-varianza" class="nav-link" data-scroll-target="#motivación-el-problema-de-la-alta-varianza">Motivación: El Problema de la Alta Varianza</a></li>
  <li><a href="#qué-es-bagging" id="toc-qué-es-bagging" class="nav-link" data-scroll-target="#qué-es-bagging">¿Qué es Bagging?</a></li>
  <li><a href="#el-algoritmo-de-bagging" id="toc-el-algoritmo-de-bagging" class="nav-link" data-scroll-target="#el-algoritmo-de-bagging">El Algoritmo de Bagging</a></li>
  <li><a href="#implementación-en-python" id="toc-implementación-en-python" class="nav-link" data-scroll-target="#implementación-en-python">Implementación en Python</a></li>
  <li><a href="#análisis-de-bias-variance-con-bagging" id="toc-análisis-de-bias-variance-con-bagging" class="nav-link" data-scroll-target="#análisis-de-bias-variance-con-bagging">Análisis de Bias-Variance con Bagging</a></li>
  <li><a href="#error-out-of-bag-oob" id="toc-error-out-of-bag-oob" class="nav-link" data-scroll-target="#error-out-of-bag-oob">Error Out-of-Bag (OOB)</a></li>
  <li><a href="#importancia-de-variables-en-bagging" id="toc-importancia-de-variables-en-bagging" class="nav-link" data-scroll-target="#importancia-de-variables-en-bagging">Importancia de Variables en Bagging</a></li>
  <li><a href="#fronteras-de-decisión-árbol-individual-vs-bagging" id="toc-fronteras-de-decisión-árbol-individual-vs-bagging" class="nav-link" data-scroll-target="#fronteras-de-decisión-árbol-individual-vs-bagging">Fronteras de Decisión: Árbol Individual vs Bagging</a></li>
  <li><a href="#ventajas-y-desventajas-de-bagging" id="toc-ventajas-y-desventajas-de-bagging" class="nav-link" data-scroll-target="#ventajas-y-desventajas-de-bagging">Ventajas y Desventajas de Bagging</a></li>
  <li><a href="#cuántos-árboles-usar" id="toc-cuántos-árboles-usar" class="nav-link" data-scroll-target="#cuántos-árboles-usar">¿Cuántos Árboles Usar?</a></li>
  <li><a href="#bagging-vs-random-forest" id="toc-bagging-vs-random-forest" class="nav-link" data-scroll-target="#bagging-vs-random-forest">Bagging vs Random Forest</a></li>
  </ul></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">Random Forest</a>
  <ul class="collapse">
  <li><a href="#motivación-decorrelación-de-árboles" id="toc-motivación-decorrelación-de-árboles" class="nav-link" data-scroll-target="#motivación-decorrelación-de-árboles">Motivación: Decorrelación de Árboles</a></li>
  <li><a href="#el-algoritmo-de-random-forest" id="toc-el-algoritmo-de-random-forest" class="nav-link" data-scroll-target="#el-algoritmo-de-random-forest">El Algoritmo de Random Forest</a></li>
  <li><a href="#hiperparámetros-clave" id="toc-hiperparámetros-clave" class="nav-link" data-scroll-target="#hiperparámetros-clave">Hiperparámetros Clave</a></li>
  <li><a href="#implementación-y-comparación" id="toc-implementación-y-comparación" class="nav-link" data-scroll-target="#implementación-y-comparación">Implementación y Comparación</a></li>
  <li><a href="#análisis-del-efecto-de-max_features" id="toc-análisis-del-efecto-de-max_features" class="nav-link" data-scroll-target="#análisis-del-efecto-de-max_features">Análisis del Efecto de max_features</a></li>
  <li><a href="#curva-de-aprendizaje-número-de-árboles" id="toc-curva-de-aprendizaje-número-de-árboles" class="nav-link" data-scroll-target="#curva-de-aprendizaje-número-de-árboles">Curva de Aprendizaje: Número de Árboles</a></li>
  <li><a href="#importancia-de-variables-en-random-forest" id="toc-importancia-de-variables-en-random-forest" class="nav-link" data-scroll-target="#importancia-de-variables-en-random-forest">Importancia de Variables en Random Forest</a></li>
  <li><a href="#fronteras-de-decisión-visualización-2d" id="toc-fronteras-de-decisión-visualización-2d" class="nav-link" data-scroll-target="#fronteras-de-decisión-visualización-2d">Fronteras de Decisión: Visualización 2D</a></li>
  <li><a href="#análisis-de-sesgo-varianza" id="toc-análisis-de-sesgo-varianza" class="nav-link" data-scroll-target="#análisis-de-sesgo-varianza">Análisis de Sesgo-Varianza</a></li>
  <li><a href="#ventajas-y-desventajas-de-random-forest" id="toc-ventajas-y-desventajas-de-random-forest" class="nav-link" data-scroll-target="#ventajas-y-desventajas-de-random-forest">Ventajas y Desventajas de Random Forest</a></li>
  <li><a href="#aplicación-práctica-dataset-real-1" id="toc-aplicación-práctica-dataset-real-1" class="nav-link" data-scroll-target="#aplicación-práctica-dataset-real-1">Aplicación Práctica: Dataset Real</a></li>
  </ul></li>
  <li><a href="#conclusiones-y-mejores-prácticas" id="toc-conclusiones-y-mejores-prácticas" class="nav-link" data-scroll-target="#conclusiones-y-mejores-prácticas">Conclusiones y Mejores Prácticas</a>
  <ul class="collapse">
  <li><a href="#recomendaciones-para-usar-árboles-de-decisión" id="toc-recomendaciones-para-usar-árboles-de-decisión" class="nav-link" data-scroll-target="#recomendaciones-para-usar-árboles-de-decisión">Recomendaciones para Usar Árboles de Decisión</a></li>
  <li><a href="#cuándo-usar-árboles-de-decisión" id="toc-cuándo-usar-árboles-de-decisión" class="nav-link" data-scroll-target="#cuándo-usar-árboles-de-decisión">Cuándo Usar Árboles de Decisión</a></li>
  <li><a href="#próximos-pasos-métodos-ensemble" id="toc-próximos-pasos-métodos-ensemble" class="nav-link" data-scroll-target="#próximos-pasos-métodos-ensemble">Próximos Pasos: Métodos Ensemble</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Árboles de Decisión</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introducción" class="level2">
<h2 class="anchored" data-anchor-id="introducción">Introducción</h2>
<p>Los <strong>árboles de decisión</strong> son uno de los métodos más intuitivos y ampliamente utilizados en el aprendizaje supervisado. A diferencia de los métodos lineales como la regresión logística, los árboles pueden capturar relaciones no lineales complejas e interacciones entre variables de forma natural, produciendo modelos que son fáciles de interpretar y visualizar.</p>
<section id="motivación-limitaciones-de-los-métodos-lineales" class="level3">
<h3 class="anchored" data-anchor-id="motivación-limitaciones-de-los-métodos-lineales">Motivación: Limitaciones de los Métodos Lineales</h3>
<p>Consideremos un problema donde queremos predecir si un cliente comprará un producto basándonos en su edad y su ingreso. Los métodos lineales (como regresión logística) asumirían que existe una <strong>frontera de decisión lineal</strong>:</p>
<p><span class="math display">\[\beta_0 + \beta_1 \cdot \text{edad} + \beta_2 \cdot \text{ingreso} = 0\]</span></p>
<p>Sin embargo, la realidad puede ser más compleja: tal vez los clientes jóvenes con ingresos altos compran, los clientes mayores con cualquier ingreso compran, pero los clientes jóvenes con ingresos bajos no compran. Esta regla no es lineal y involucra <strong>interacciones</strong> entre variables.</p>
<p>Los árboles de decisión resuelven este problema al <strong>particionar el espacio de características</strong> en regiones rectangulares, donde cada región tiene su propia predicción.</p>
</section>
<section id="estructura-de-un-árbol-de-decisión" class="level3">
<h3 class="anchored" data-anchor-id="estructura-de-un-árbol-de-decisión">Estructura de un Árbol de Decisión</h3>
<p>Un árbol de decisión es una estructura jerárquica compuesta por:</p>
<ol type="1">
<li><strong>Nodo raíz (root node)</strong>: Contiene todos los datos de entrenamiento</li>
<li><strong>Nodos internos (internal nodes)</strong>: Representan decisiones basadas en características</li>
<li><strong>Ramas (branches)</strong>: Representan el resultado de una decisión</li>
<li><strong>Nodos hoja o terminales (leaf nodes)</strong>: Contienen las predicciones finales</li>
</ol>
<p>Cada nodo interno realiza una <strong>pregunta binaria</strong> sobre una característica:</p>
<ul>
<li>“¿Edad ≤ 30?”</li>
<li>“¿Ingreso &gt; $50,000?”</li>
<li>“¿Categoría = A o B?”</li>
</ul>
</section>
<section id="ejemplo-visual-simple" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-visual-simple">Ejemplo Visual Simple</h3>
<pre><code>                    [Edad ≤ 30?]
                    /           \
                  Sí             No
                 /                 \
        [Ingreso ≤ 40K?]        Compra = Sí
           /         \
         Sí          No
        /             \
   Compra = No    Compra = Sí</code></pre>
<p>Este árbol representa las siguientes reglas:</p>
<ul>
<li>Si edad &gt; 30 → Compra = Sí</li>
<li>Si edad ≤ 30 y ingreso &gt; 40K → Compra = Sí</li>
<li>Si edad ≤ 30 y ingreso ≤ 40K → Compra = No</li>
</ul>
</section>
</section>
<section id="construcción-de-árboles-de-decisión" class="level2">
<h2 class="anchored" data-anchor-id="construcción-de-árboles-de-decisión">Construcción de Árboles de Decisión</h2>
<section id="particionamiento-recursivo-del-espacio" class="level3">
<h3 class="anchored" data-anchor-id="particionamiento-recursivo-del-espacio">Particionamiento Recursivo del Espacio</h3>
<p>Los árboles de decisión construyen su estructura mediante <strong>particionamiento recursivo binario</strong> (recursive binary splitting). Este proceso:</p>
<ol type="1">
<li>Comienza con todos los datos en el nodo raíz</li>
<li>Encuentra la mejor división (variable y punto de corte)</li>
<li>Divide los datos en dos nodos hijos</li>
<li>Repite el proceso recursivamente para cada nodo hijo</li>
<li>Se detiene cuando se cumple un criterio de parada</li>
</ol>
<p>Matemáticamente, el espacio de características <span class="math inline">\(\mathbb{R}^p\)</span> se divide en <span class="math inline">\(M\)</span> regiones disjuntas <span class="math inline">\(R_1, R_2, ..., R_M\)</span> tales que:</p>
<p><span class="math display">\[\bigcup_{m=1}^{M} R_m = \mathbb{R}^p, \quad R_i \cap R_j = \emptyset \text{ para } i \neq j\]</span></p>
<p>Cada región <span class="math inline">\(R_m\)</span> es un <strong>hiperrectángulo</strong> paralelo a los ejes de coordenadas.</p>
</section>
<section id="criterios-de-impureza" class="level3">
<h3 class="anchored" data-anchor-id="criterios-de-impureza">Criterios de Impureza</h3>
<p>Para decidir cómo dividir un nodo, necesitamos medir la <strong>impureza</strong> o <strong>heterogeneidad</strong> de un nodo. Un nodo es “puro” si contiene mayormente ejemplos de una sola clase.</p>
<section id="índice-de-gini" class="level4">
<h4 class="anchored" data-anchor-id="índice-de-gini">1. Índice de Gini</h4>
<p>El <strong>índice de Gini</strong> mide la probabilidad de clasificar incorrectamente un elemento elegido aleatoriamente si se etiqueta aleatoriamente según la distribución de clases del nodo:</p>
<p><span class="math display">\[I_G(t) = \sum_{k=1}^{K} p_k(t) \cdot (1 - p_k(t)) = \sum_{k=1}^{K} p_k(t) - \sum_{k=1}^{K} p_k(t)^2 = 1 - \sum_{k=1}^{K} p_k(t)^2\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(K\)</span> es el número de clases</li>
<li><span class="math inline">\(p_k(t)\)</span> es la proporción de ejemplos de la clase <span class="math inline">\(k\)</span> en el nodo <span class="math inline">\(t\)</span></li>
</ul>
<p><strong>Propiedades del índice de Gini:</strong></p>
<ul>
<li><strong>Mínimo</strong> (<span class="math inline">\(I_G = 0\)</span>): Nodo puro (una sola clase)
<ul>
<li>Ejemplo: Si <span class="math inline">\(p_1 = 1, p_2 = 0\)</span> → <span class="math inline">\(I_G = 1 - (1^2 + 0^2) = 0\)</span></li>
</ul></li>
<li><strong>Máximo</strong> (cuando las clases están balanceadas):
<ul>
<li>Para 2 clases con <span class="math inline">\(p_1 = p_2 = 0.5\)</span> → <span class="math inline">\(I_G = 1 - (0.5^2 + 0.5^2) = 0.5\)</span></li>
<li>Para <span class="math inline">\(K\)</span> clases con <span class="math inline">\(p_k = 1/K\)</span> → <span class="math inline">\(I_G = 1 - K(1/K)^2 = (K-1)/K\)</span></li>
</ul></li>
</ul>
</section>
<section id="entropía" class="level4">
<h4 class="anchored" data-anchor-id="entropía">2. Entropía</h4>
<p>La <strong>entropía</strong> mide el desorden o incertidumbre en un nodo, basada en la teoría de la información:</p>
<p><span class="math display">\[H(t) = -\sum_{k=1}^{K} p_k(t) \log_2(p_k(t))\]</span></p>
<p>Por convención, <span class="math inline">\(0 \log(0) = 0\)</span>.</p>
<p><strong>Propiedades de la entropía:</strong></p>
<ul>
<li><strong>Mínimo</strong> (<span class="math inline">\(H = 0\)</span>): Nodo puro (certidumbre completa)</li>
<li><strong>Máximo</strong> (<span class="math inline">\(H = \log_2(K)\)</span>): Clases uniformemente distribuidas (máxima incertidumbre)
<ul>
<li>Para 2 clases: <span class="math inline">\(H_{\max} = 1\)</span> bit</li>
<li>Para 4 clases: <span class="math inline">\(H_{\max} = 2\)</span> bits</li>
</ul></li>
</ul>
<p><strong>Ganancia de Información (Information Gain):</strong></p>
<p>La ganancia de información mide la reducción en entropía al realizar una división:</p>
<p><span class="math display">\[IG = H(t_{\text{padre}}) - \sum_{i \in \{\text{izq, der}\}} \frac{n_i}{n} H(t_i)\]</span></p>
<p>Donde <span class="math inline">\(n_i\)</span> es el número de ejemplos en el nodo hijo <span class="math inline">\(i\)</span> y <span class="math inline">\(n\)</span> es el total en el nodo padre.</p>
</section>
<section id="error-de-clasificación" class="level4">
<h4 class="anchored" data-anchor-id="error-de-clasificación">3. Error de Clasificación</h4>
<p>El <strong>error de clasificación</strong> es la tasa de ejemplos que no pertenecen a la clase mayoritaria:</p>
<p><span class="math display">\[E(t) = 1 - \max_k p_k(t)\]</span></p>
<p>Este criterio es menos sensible a cambios en la distribución de clases y se usa menos en la práctica.</p>
</section>
</section>
<section id="comparación-visual-de-criterios-de-impureza" class="level3">
<h3 class="anchored" data-anchor-id="comparación-visual-de-criterios-de-impureza">Comparación Visual de Criterios de Impureza</h3>
<div id="cell-impurity-comparison" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="impurity-comparison" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/impurity-comparison-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación de criterios de impureza para clasificación binaria</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Valores de impureza en puntos clave:
============================================================
Proporción p    Gini         Entropía     Error       
------------------------------------------------------------
0.0             0.0000       0.0000       0.0000      
0.1             0.1800       0.4690       0.1000      
0.3             0.4200       0.8813       0.3000      
0.5             0.5000       1.0000       0.5000      
0.7             0.4200       0.8813       0.3000      
0.9             0.1800       0.4690       0.1000      
1.0             0.0000       0.0000       0.0000      </code></pre>
</div>
</div>
<p><strong>Observaciones:</strong></p>
<ol type="1">
<li><strong>Gini y Entropía</strong> son muy similares en comportamiento y suelen dar resultados comparables</li>
<li><strong>Error de clasificación</strong> es menos sensible a cambios en las probabilidades</li>
<li>En la práctica, Gini es más común por ser más eficiente computacionalmente</li>
<li>Todas alcanzan su máximo cuando las clases están balanceadas (<span class="math inline">\(p = 0.5\)</span>)</li>
</ol>
</section>
<section id="algoritmo-de-construcción-cart" class="level3">
<h3 class="anchored" data-anchor-id="algoritmo-de-construcción-cart">Algoritmo de Construcción CART</h3>
<p>El algoritmo <strong>CART</strong> (Classification And Regression Trees) es el método más común para construir árboles de decisión:</p>
<p><strong>Algoritmo: Construcción Greedy de Árbol de Decisión</strong></p>
<pre><code>función CONSTRUIR_ARBOL(datos, profundidad_actual, max_profundidad):
    // Criterios de parada
    si profundidad_actual &gt;= max_profundidad O
       nodo es puro O
       número de muestras &lt; min_muestras:
        crear nodo hoja con predicción mayoritaria
        retornar

    // Encontrar mejor división
    mejor_ganancia = -infinito

    para cada característica j en {1, ..., p}:
        para cada posible punto de corte c:
            dividir datos en: {x_j ≤ c} y {x_j &gt; c}
            calcular impureza ponderada de los nodos hijos
            calcular ganancia = impureza_padre - impureza_hijos

            si ganancia &gt; mejor_ganancia:
                mejor_ganancia = ganancia
                mejor_característica = j
                mejor_corte = c

    // Crear división
    crear nodo interno con pregunta: "x[mejor_característica] ≤ mejor_corte?"
    datos_izq = datos donde x[mejor_característica] ≤ mejor_corte
    datos_der = datos donde x[mejor_característica] &gt; mejor_corte

    // Recursión
    hijo_izquierdo = CONSTRUIR_ARBOL(datos_izq, profundidad_actual + 1, max_profundidad)
    hijo_derecho = CONSTRUIR_ARBOL(datos_der, profundidad_actual + 1, max_profundidad)

    retornar nodo_actual</code></pre>
<p><strong>Características clave del algoritmo:</strong></p>
<ol type="1">
<li><strong>Greedy (Voraz)</strong>: En cada paso, elige la mejor división local sin considerar divisiones futuras</li>
<li><strong>Top-down</strong>: Construye desde la raíz hacia las hojas</li>
<li><strong>Recursivo</strong>: Aplica el mismo proceso a cada subárbol</li>
<li><strong>Binario</strong>: Cada división genera exactamente dos nodos hijos</li>
</ol>
</section>
<section id="ejemplo-construcción-paso-a-paso" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-construcción-paso-a-paso">Ejemplo: Construcción Paso a Paso</h3>
<div id="tree-construction-example" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos sintéticos simples (2D para visualización)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    flip_y<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    class_sep<span class="op">=</span><span class="fl">1.5</span>,</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear DataFrame para mejor visualización</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span>[<span class="st">'X1'</span>, <span class="st">'X2'</span>])</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Clase'</span>] <span class="op">=</span> y</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Datos de ejemplo:"</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head(<span class="dv">10</span>))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Total de muestras: </span><span class="sc">{</span><span class="bu">len</span>(df)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Clases: </span><span class="sc">{</span>df[<span class="st">'Clase'</span>]<span class="sc">.</span>value_counts()<span class="sc">.</span>to_dict()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Datos de ejemplo:
============================================================
         X1        X2  Clase
0  1.122201 -3.621909      0
1  2.055968  3.471449      1
2  1.626547 -0.708767      0
3  2.238265  2.357568      1
4  1.010960  2.377681      1
5  0.095620  2.794548      1
6  0.700506  1.005135      1
7  1.873085  2.558868      1
8  1.076216  1.470596      1
9  2.176681  0.741384      1

Total de muestras: 200
Clases: {1: 101, 0: 99}</code></pre>
</div>
</div>
<div id="cell-tree-depths-comparison" class="cell" data-fig-height="10" data-fig-width="14" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> plot_tree</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar árboles con diferentes profundidades</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>profundidades <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, depth <span class="kw">in</span> <span class="bu">enumerate</span>(profundidades):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Entrenar árbol</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    tree <span class="op">=</span> DecisionTreeClassifier(</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span>depth,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span><span class="st">'gini'</span>,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    tree.fit(X, y)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualizar árbol</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    plot_tree(</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        tree,</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=</span>axes[idx],</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        feature_names<span class="op">=</span>[<span class="st">'X1'</span>, <span class="st">'X2'</span>],</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        class_names<span class="op">=</span>[<span class="st">'Clase 0'</span>, <span class="st">'Clase 1'</span>],</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        filled<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        rounded<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcular accuracy en entrenamiento</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> tree.score(X, y)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_title(</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f'Profundidad = </span><span class="sc">{</span>depth<span class="sc">}</span><span class="ss"> | Accuracy = </span><span class="sc">{</span>train_accuracy<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        pad<span class="op">=</span><span class="dv">10</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar información detallada del árbol más complejo</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"INFORMACIÓN DEL ÁRBOL (Profundidad = 5)"</span>)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>tree_detailed <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>tree_detailed.fit(X, y)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Número de nodos: </span><span class="sc">{</span>tree_detailed<span class="sc">.</span>tree_<span class="sc">.</span>node_count<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Número de hojas: </span><span class="sc">{</span>tree_detailed<span class="sc">.</span>get_n_leaves()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Profundidad real: </span><span class="sc">{</span>tree_detailed<span class="sc">.</span>get_depth()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy en entrenamiento: </span><span class="sc">{</span>tree_detailed<span class="sc">.</span>score(X, y)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="tree-depths-comparison" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/tree-depths-comparison-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación de árboles con diferentes profundidades</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
============================================================
INFORMACIÓN DEL ÁRBOL (Profundidad = 5)
============================================================
Número de nodos: 41
Número de hojas: 21
Profundidad real: 5
Accuracy en entrenamiento: 0.950</code></pre>
</div>
</div>
<div id="cell-decision-boundaries" class="cell" data-fig-height="10" data-fig-width="14" data-execution_count="4">
<div class="cell-output cell-output-display">
<div id="decision-boundaries" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/decision-boundaries-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Fronteras de decisión para diferentes profundidades de árbol</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Observaciones importantes:</strong></p>
<ol type="1">
<li><strong>Profundidad = 1</strong> (stump): Una sola división, frontera muy simple</li>
<li><strong>Profundidad = 2-3</strong>: Capturas las principales regiones de decisión</li>
<li><strong>Profundidad = 5</strong>: Frontera muy compleja, posible sobreajuste</li>
<li>Las fronteras son siempre <strong>paralelas a los ejes</strong> (particiones rectangulares)</li>
</ol>
</section>
</section>
<section id="sobreajuste-y-control-de-complejidad" class="level2">
<h2 class="anchored" data-anchor-id="sobreajuste-y-control-de-complejidad">Sobreajuste y Control de Complejidad</h2>
<section id="el-problema-del-sobreajuste" class="level3">
<h3 class="anchored" data-anchor-id="el-problema-del-sobreajuste">El Problema del Sobreajuste</h3>
<p>Los árboles de decisión tienen una tendencia natural al <strong>sobreajuste</strong> (overfitting). Sin restricciones, un árbol puede crecer hasta que cada nodo hoja contenga un solo ejemplo, logrando 100% de accuracy en entrenamiento pero generalizando muy mal.</p>
<p><strong>Causas del sobreajuste:</strong></p>
<ol type="1">
<li><strong>Alta varianza</strong>: Pequeños cambios en los datos pueden producir árboles muy diferentes</li>
<li><strong>Falta de regularización inherente</strong>: Sin restricciones, el árbol memoriza los datos</li>
<li><strong>Captura de ruido</strong>: El árbol aprende patrones específicos del conjunto de entrenamiento</li>
</ol>
</section>
<section id="estrategias-de-control-de-complejidad" class="level3">
<h3 class="anchored" data-anchor-id="estrategias-de-control-de-complejidad">Estrategias de Control de Complejidad</h3>
<section id="pre-poda-pre-pruning" class="level4">
<h4 class="anchored" data-anchor-id="pre-poda-pre-pruning">1. Pre-Poda (Pre-Pruning)</h4>
<p>La <strong>pre-poda</strong> detiene el crecimiento del árbol durante su construcción mediante criterios:</p>
<p><strong>Hiperparámetros comunes:</strong></p>
<ul>
<li><code>max_depth</code>: Profundidad máxima del árbol
<ul>
<li>Valores típicos: 3-10</li>
<li>Menor → Más sesgo, menos varianza</li>
</ul></li>
<li><code>min_samples_split</code>: Mínimo de muestras para dividir un nodo
<ul>
<li>Valores típicos: 2-20</li>
<li>Mayor → Árbol más pequeño</li>
</ul></li>
<li><code>min_samples_leaf</code>: Mínimo de muestras en una hoja
<ul>
<li>Valores típicos: 1-10</li>
<li>Mayor → Hojas más confiables</li>
</ul></li>
<li><code>max_features</code>: Número máximo de características a considerar por división
<ul>
<li><code>'sqrt'</code>: √p características (usado en Random Forest)</li>
<li><code>'log2'</code>: log₂(p) características</li>
<li><code>None</code>: Todas las características</li>
</ul></li>
<li><code>max_leaf_nodes</code>: Número máximo de nodos hoja
<ul>
<li>Controla directamente el tamaño del árbol</li>
</ul></li>
</ul>
</section>
<section id="post-poda-post-pruning" class="level4">
<h4 class="anchored" data-anchor-id="post-poda-post-pruning">2. Post-Poda (Post-Pruning)</h4>
<p>La <strong>post-poda</strong> construye un árbol completo y luego lo reduce eliminando nodos que no aportan suficiente mejora.</p>
<p><strong>Cost-Complexity Pruning (Poda por Costo-Complejidad):</strong></p>
<p>Define una función de costo que balancea error y complejidad:</p>
<p><span class="math display">\[C_\alpha(T) = \sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} L(y_i, \hat{y}_m) + \alpha |T|\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(|T|\)</span> es el número de nodos hoja</li>
<li><span class="math inline">\(\alpha \geq 0\)</span> es el parámetro de complejidad</li>
<li><span class="math inline">\(L\)</span> es la función de pérdida</li>
<li><span class="math inline">\(\hat{y}_m\)</span> es la predicción en el nodo hoja <span class="math inline">\(m\)</span></li>
</ul>
<p><strong>Efecto de <span class="math inline">\(\alpha\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(\alpha = 0\)</span>: Árbol completo (sin poda)</li>
<li><span class="math inline">\(\alpha\)</span> grande: Árbol muy pequeño (mayor regularización)</li>
</ul>
<div id="cell-pruning-demonstration" class="cell" data-fig-height="5" data-fig-width="14" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir datos</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar árbol completo</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>tree_full <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>tree_full.fit(X_train, y_train)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener camino de cost-complexity pruning</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> tree_full.cost_complexity_pruning_path(X_train, y_train)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>ccp_alphas <span class="op">=</span> path.ccp_alphas</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>impurities <span class="op">=</span> path.impurities</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cost-Complexity Pruning Path:"</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Número de valores de alpha: </span><span class="sc">{</span><span class="bu">len</span>(ccp_alphas)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Rango de alpha: [</span><span class="sc">{</span>ccp_alphas[<span class="dv">0</span>]<span class="sc">:.6f}</span><span class="ss">, </span><span class="sc">{</span>ccp_alphas[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.6f}</span><span class="ss">]"</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar árboles para diferentes valores de alpha</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>train_scores <span class="op">=</span> []</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>test_scores <span class="op">=</span> []</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>n_leaves <span class="op">=</span> []</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>depths <span class="op">=</span> []</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> ccp_alphas:</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    tree <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>alpha)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    tree.fit(X_train, y_train)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    train_scores.append(tree.score(X_train, y_train))</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    test_scores.append(tree.score(X_test, y_test))</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    n_leaves.append(tree.get_n_leaves())</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    depths.append(tree.get_depth())</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: Accuracy vs Alpha</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(ccp_alphas, train_scores, label<span class="op">=</span><span class="st">'Entrenamiento'</span>,</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>             marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(ccp_alphas, test_scores, label<span class="op">=</span><span class="st">'Prueba'</span>,</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>             marker<span class="op">=</span><span class="st">'s'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Alpha (ccp_alpha)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Accuracy vs Alpha'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Encontrar mejor alpha</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>best_idx <span class="op">=</span> np.argmax(test_scores)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> ccp_alphas[best_idx]</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axvline(x<span class="op">=</span>best_alpha, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>,</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="ss">f'Mejor α = </span><span class="sc">{</span>best_alpha<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Número de hojas vs Alpha</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(ccp_alphas, n_leaves, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Alpha (ccp_alpha)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Número de Hojas'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Complejidad del Árbol vs Alpha'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axvline(x<span class="op">=</span>best_alpha, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 3: Profundidad vs Alpha</span></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].plot(ccp_alphas, depths, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'purple'</span>)</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'Alpha (ccp_alpha)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'Profundidad del Árbol'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'Profundidad vs Alpha'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].axvline(x<span class="op">=</span>best_alpha, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"COMPARACIÓN: Árbol sin poda vs Árbol podado"</span>)</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Árbol sin poda (α = 0):"</span>)</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Hojas: </span><span class="sc">{</span>n_leaves[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Profundidad: </span><span class="sc">{</span>depths[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Accuracy entrenamiento: </span><span class="sc">{</span>train_scores[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Accuracy prueba: </span><span class="sc">{</span>test_scores[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Árbol podado óptimo (α = </span><span class="sc">{</span>best_alpha<span class="sc">:.4f}</span><span class="ss">):"</span>)</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Hojas: </span><span class="sc">{</span>n_leaves[best_idx]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Profundidad: </span><span class="sc">{</span>depths[best_idx]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Accuracy entrenamiento: </span><span class="sc">{</span>train_scores[best_idx]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Accuracy prueba: </span><span class="sc">{</span>test_scores[best_idx]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cost-Complexity Pruning Path:
============================================================
Número de valores de alpha: 13
Rango de alpha: [0.000000, 0.309700]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="pruning-demonstration" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/pruning-demonstration-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Efecto de la poda en el desempeño del árbol</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
============================================================
COMPARACIÓN: Árbol sin poda vs Árbol podado
============================================================

Árbol sin poda (α = 0):
  Hojas: 24
  Profundidad: 10
  Accuracy entrenamiento: 1.000
  Accuracy prueba: 0.850

Árbol podado óptimo (α = 0.0129):
  Hojas: 4
  Profundidad: 3
  Accuracy entrenamiento: 0.907
  Accuracy prueba: 0.883</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="interpretabilidad-y-análisis" class="level2">
<h2 class="anchored" data-anchor-id="interpretabilidad-y-análisis">Interpretabilidad y Análisis</h2>
<section id="importancia-de-variables" class="level3">
<h3 class="anchored" data-anchor-id="importancia-de-variables">Importancia de Variables</h3>
<p>Una de las grandes ventajas de los árboles es que podemos medir la <strong>importancia</strong> de cada variable basándonos en cuánto reduce la impureza:</p>
<p><span class="math display">\[\text{Importancia}(X_j) = \sum_{t: \text{usa } X_j} \frac{n_t}{n} \cdot \Delta I(t)\]</span></p>
<p>Donde: - <span class="math inline">\(n_t\)</span> es el número de muestras en el nodo <span class="math inline">\(t\)</span> - <span class="math inline">\(n\)</span> es el número total de muestras - <span class="math inline">\(\Delta I(t)\)</span> es la reducción en impureza por la división en el nodo <span class="math inline">\(t\)</span></p>
<div id="cell-feature-importance" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar árbol en dataset con más características</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos con 10 características</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>X_multi, y_multi <span class="op">=</span> make_classification(</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    n_repeated<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Nombres de características</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> [<span class="ss">f'X</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar árbol</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>tree_multi <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>tree_multi.fit(X_multi, y_multi)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener importancias</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> tree_multi.feature_importances_</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: Gráfico de barras</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].barh(<span class="bu">range</span>(<span class="dv">10</span>), importances[indices], color<span class="op">=</span><span class="st">'steelblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticks(<span class="bu">range</span>(<span class="dv">10</span>))</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticklabels([feature_names[i] <span class="cf">for</span> i <span class="kw">in</span> indices])</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Importancia'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Importancia de Variables (Reducción de Impureza)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadir valores</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (idx, imp) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(indices, importances[indices])):</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(imp <span class="op">+</span> <span class="fl">0.005</span>, i, <span class="ss">f'</span><span class="sc">{</span>imp<span class="sc">:.3f}</span><span class="ss">'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Importancia acumulada</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>cumsum_importance <span class="op">=</span> np.cumsum(importances[indices])</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), cumsum_importance, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>,</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>             markersize<span class="op">=</span><span class="dv">8</span>, color<span class="op">=</span><span class="st">'darkgreen'</span>)</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].fill_between(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>), cumsum_importance, alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(y<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>,</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="st">'80</span><span class="sc">% d</span><span class="st">e importancia'</span>)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(y<span class="op">=</span><span class="fl">0.95</span>, color<span class="op">=</span><span class="st">'orange'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>,</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="st">'95</span><span class="sc">% d</span><span class="st">e importancia'</span>)</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Número de Variables'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Importancia Acumulada'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Importancia Acumulada de Variables'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticks(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>))</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Imprimir tabla de importancias</span></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Tabla de Importancias:"</span>)</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Variable'</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Importancia'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Importancia Acum.'</span><span class="sc">:&lt;20}</span><span class="ss">"</span>)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>cumsum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> indices:</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>    cumsum <span class="op">+=</span> importances[idx]</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>feature_names[idx]<span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span>importances[idx]<span class="sc">:&lt;15.4f}</span><span class="ss"> </span><span class="sc">{</span>cumsum<span class="sc">:&lt;20.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="feature-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/feature-importance-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Importancia de variables en árbol de decisión</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Tabla de Importancias:
============================================================
Variable     Importancia     Importancia Acum.   
------------------------------------------------------------
X1           0.3599          0.3599              
X6           0.1732          0.5331              
X8           0.1130          0.6462              
X2           0.0921          0.7383              
X5           0.0771          0.8154              
X10          0.0768          0.8921              
X9           0.0514          0.9436              
X4           0.0385          0.9821              
X7           0.0134          0.9955              
X3           0.0045          1.0000              </code></pre>
</div>
</div>
</section>
<section id="extracción-de-reglas" class="level3">
<h3 class="anchored" data-anchor-id="extracción-de-reglas">Extracción de Reglas</h3>
<p>Los árboles pueden convertirse en reglas IF-THEN interpretables:</p>
<div id="tree-rules" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> export_text</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar árbol simple para mejor interpretabilidad</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>tree_simple <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">3</span>, min_samples_leaf<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>tree_simple.fit(X[:, :<span class="dv">2</span>], y)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exportar reglas como texto</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>tree_rules <span class="op">=</span> export_text(tree_simple, feature_names<span class="op">=</span>[<span class="st">'X1'</span>, <span class="st">'X2'</span>])</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"REGLAS DE DECISIÓN DEL ÁRBOL:"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tree_rules)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Función para extraer rutas de decisión</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_decision_path(tree, feature_names, sample):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extrae la ruta de decisión para una muestra"""</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    node <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> []</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> tree.tree_.feature[node] <span class="op">!=</span> <span class="op">-</span><span class="dv">2</span>:  <span class="co"># -2 indica nodo hoja</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        feature_idx <span class="op">=</span> tree.tree_.feature[node]</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> tree.tree_.threshold[node]</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sample[feature_idx] <span class="op">&lt;=</span> threshold:</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            direction <span class="op">=</span> <span class="st">"&lt;="</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>            node <span class="op">=</span> tree.tree_.children_left[node]</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>            direction <span class="op">=</span> <span class="st">"&gt;"</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>            node <span class="op">=</span> tree.tree_.children_right[node]</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        path.append(<span class="ss">f"</span><span class="sc">{</span>feature_names[feature_idx]<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>direction<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>threshold<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtener predicción</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    class_probs <span class="op">=</span> tree.tree_.value[node][<span class="dv">0</span>]</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    predicted_class <span class="op">=</span> np.argmax(class_probs)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> path, predicted_class, class_probs</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Ejemplo: explicar predicción para algunas muestras</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"EXPLICACIÓN DE PREDICCIONES"</span>)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> X[i, :<span class="dv">2</span>]</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>    path, pred_class, probs <span class="op">=</span> get_decision_path(tree_simple, [<span class="st">'X1'</span>, <span class="st">'X2'</span>], sample)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Muestra </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: X1=</span><span class="sc">{</span>sample[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">, X2=</span><span class="sc">{</span>sample[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Clase real: </span><span class="sc">{</span>y[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Predicción: </span><span class="sc">{</span>pred_class<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Probabilidades: Clase 0 = </span><span class="sc">{</span>probs[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">, Clase 1 = </span><span class="sc">{</span>probs[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Ruta de decisión:"</span>)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> path:</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  → </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>REGLAS DE DECISIÓN DEL ÁRBOL:
============================================================
|--- X2 &lt;= 0.31
|   |--- X1 &lt;= 1.07
|   |   |--- X2 &lt;= -0.96
|   |   |   |--- class: 0
|   |   |--- X2 &gt;  -0.96
|   |   |   |--- class: 0
|   |--- X1 &gt;  1.07
|   |   |--- X2 &lt;= -2.17
|   |   |   |--- class: 0
|   |   |--- X2 &gt;  -2.17
|   |   |   |--- class: 0
|--- X2 &gt;  0.31
|   |--- X1 &lt;= 1.07
|   |   |--- X2 &lt;= 1.20
|   |   |   |--- class: 1
|   |   |--- X2 &gt;  1.20
|   |   |   |--- class: 1
|   |--- X1 &gt;  1.07
|   |   |--- X2 &lt;= 0.82
|   |   |   |--- class: 1
|   |   |--- X2 &gt;  0.82
|   |   |   |--- class: 1


============================================================
EXPLICACIÓN DE PREDICCIONES
============================================================

Muestra 1: X1=1.122, X2=-3.622
Clase real: 0
Predicción: 0
Probabilidades: Clase 0 = 0.800, Clase 1 = 0.200
Ruta de decisión:
  → X2 &lt;= 0.315
  → X1 &gt; 1.072
  → X2 &lt;= -2.166

Muestra 2: X1=2.056, X2=3.471
Clase real: 1
Predicción: 1
Probabilidades: Clase 0 = 0.018, Clase 1 = 0.982
Ruta de decisión:
  → X2 &gt; 0.315
  → X1 &gt; 1.066
  → X2 &gt; 0.821

Muestra 3: X1=1.627, X2=-0.709
Clase real: 0
Predicción: 0
Probabilidades: Clase 0 = 0.961, Clase 1 = 0.039
Ruta de decisión:
  → X2 &lt;= 0.315
  → X1 &gt; 1.072
  → X2 &gt; -2.166</code></pre>
</div>
</div>
</section>
</section>
<section id="ventajas-y-desventajas" class="level2">
<h2 class="anchored" data-anchor-id="ventajas-y-desventajas">Ventajas y Desventajas</h2>
<section id="ventajas-de-los-árboles-de-decisión" class="level3">
<h3 class="anchored" data-anchor-id="ventajas-de-los-árboles-de-decisión">Ventajas de los Árboles de Decisión</h3>
<ol type="1">
<li><p><strong>Interpretabilidad</strong>: Fáciles de entender y explicar, incluso para no expertos</p>
<ul>
<li>Se pueden visualizar completamente</li>
<li>Generan reglas IF-THEN interpretables</li>
</ul></li>
<li><p><strong>Manejo de variables mixtas</strong>: Pueden manejar características numéricas y categóricas sin preprocesamiento</p></li>
<li><p><strong>No requieren normalización</strong>: Las decisiones son invariantes a transformaciones monótonas</p></li>
<li><p><strong>Capturan interacciones automáticamente</strong>: Detectan interacciones sin especificarlas explícitamente</p></li>
<li><p><strong>Robustos a outliers</strong>: Las divisiones son basadas en rankings, no en valores absolutos</p></li>
<li><p><strong>Selección implícita de características</strong>: Variables irrelevantes no se usan en las divisiones</p></li>
</ol>
</section>
<section id="desventajas-de-los-árboles-de-decisión" class="level3">
<h3 class="anchored" data-anchor-id="desventajas-de-los-árboles-de-decisión">Desventajas de los Árboles de Decisión</h3>
<ol type="1">
<li><p><strong>Alta varianza</strong>: Pequeños cambios en datos → árboles muy diferentes</p>
<ul>
<li>Solución: Métodos ensemble (Random Forest, Gradient Boosting)</li>
</ul></li>
<li><p><strong>Dificultad con relaciones lineales</strong>: Necesitan muchas divisiones para aproximar funciones lineales</p></li>
<li><p><strong>Fronteras de decisión restrictivas</strong>: Solo particiones rectangulares paralelas a los ejes</p></li>
<li><p><strong>Sesgo hacia variables con muchos valores</strong>: Tienden a seleccionar variables con más opciones de corte</p></li>
<li><p><strong>Inestabilidad</strong>: Pequeñas variaciones pueden cambiar completamente la estructura</p></li>
<li><p><strong>Sobreajuste natural</strong>: Sin restricciones, memorizan los datos de entrenamiento</p></li>
</ol>
</section>
<section id="comparación-visual-árbol-vs-regresión-logística" class="level3">
<h3 class="anchored" data-anchor-id="comparación-visual-árbol-vs-regresión-logística">Comparación Visual: Árbol vs Regresión Logística</h3>
<div id="cell-tree-vs-logistic" class="cell" data-fig-height="5" data-fig-width="14" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="tree-vs-logistic" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/tree-vs-logistic-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación de fronteras de decisión: Árbol vs Regresión Logística</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Observaciones:
============================================================
- Regresión logística captura mejor la relación lineal subyacente
- Árbol de decisión crea fronteras rectangulares que aproximan la línea
- Para relaciones lineales, la regresión logística es más eficiente
- Para relaciones no lineales, los árboles son más flexibles</code></pre>
</div>
</div>
</section>
</section>
<section id="aplicación-práctica-dataset-real" class="level2">
<h2 class="anchored" data-anchor-id="aplicación-práctica-dataset-real">Aplicación Práctica: Dataset Real</h2>
<div id="real-dataset-application" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score, GridSearchCV</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Cargar dataset</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>cancer <span class="op">=</span> load_breast_cancer()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>X_cancer <span class="op">=</span> cancer.data</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>y_cancer <span class="op">=</span> cancer.target</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DATASET: Wisconsin Breast Cancer"</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Número de muestras: </span><span class="sc">{</span>X_cancer<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Número de características: </span><span class="sc">{</span>X_cancer<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Clases: </span><span class="sc">{</span>cancer<span class="sc">.</span>target_names<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Distribución: </span><span class="sc">{</span>np<span class="sc">.</span>bincount(y_cancer)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir datos</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>X_train_c, X_test_c, y_train_c, y_test_c <span class="op">=</span> train_test_split(</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    X_cancer, y_cancer, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y_cancer</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Árbol sin regularización</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>tree_unreg <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>tree_unreg.fit(X_train_c, y_train_c)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">1. ÁRBOL SIN REGULARIZACIÓN"</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Profundidad: </span><span class="sc">{</span>tree_unreg<span class="sc">.</span>get_depth()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Número de hojas: </span><span class="sc">{</span>tree_unreg<span class="sc">.</span>get_n_leaves()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy entrenamiento: </span><span class="sc">{</span>tree_unreg<span class="sc">.</span>score(X_train_c, y_train_c)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy prueba: </span><span class="sc">{</span>tree_unreg<span class="sc">.</span>score(X_test_c, y_test_c)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Búsqueda de hiperparámetros óptimos</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">10</span>, <span class="va">None</span>],</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>],</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'criterion'</span>: [<span class="st">'gini'</span>, <span class="st">'entropy'</span>]</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">2. BÚSQUEDA DE HIPERPARÁMETROS (Grid Search)"</span>)</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Evaluando combinaciones de hiperparámetros con CV..."</span>)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>    DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>),</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>    param_grid,</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'accuracy'</span>,</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train_c, y_train_c)</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mejor combinación de parámetros:"</span>)</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param, value <span class="kw">in</span> grid_search.best_params_.items():</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>param<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Evaluar mejor modelo</span></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>best_tree <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">3. MEJOR ÁRBOL (después de optimización)"</span>)</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Profundidad: </span><span class="sc">{</span>best_tree<span class="sc">.</span>get_depth()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Número de hojas: </span><span class="sc">{</span>best_tree<span class="sc">.</span>get_n_leaves()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy entrenamiento: </span><span class="sc">{</span>best_tree<span class="sc">.</span>score(X_train_c, y_train_c)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy prueba: </span><span class="sc">{</span>best_tree<span class="sc">.</span>score(X_test_c, y_test_c)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Validación cruzada</span></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> cross_val_score(best_tree, X_train_c, y_train_c, cv<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Validación cruzada (5-fold):"</span>)</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Scores: </span><span class="sc">{</span>cv_scores<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Media: </span><span class="sc">{</span>cv_scores<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss"> (+/- </span><span class="sc">{</span>cv_scores<span class="sc">.</span>std()<span class="sc">:.3f}</span><span class="ss">)"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>DATASET: Wisconsin Breast Cancer
============================================================
Número de muestras: 569
Número de características: 30
Clases: ['malignant' 'benign']
Distribución: [212 357]

1. ÁRBOL SIN REGULARIZACIÓN
------------------------------------------------------------
Profundidad: 6
Número de hojas: 16
Accuracy entrenamiento: 1.000
Accuracy prueba: 0.918

2. BÚSQUEDA DE HIPERPARÁMETROS (Grid Search)
------------------------------------------------------------
Evaluando combinaciones de hiperparámetros con CV...
Mejor combinación de parámetros:
  criterion: gini
  max_depth: 3
  min_samples_leaf: 2
  min_samples_split: 2

3. MEJOR ÁRBOL (después de optimización)
------------------------------------------------------------
Profundidad: 3
Número de hojas: 7
Accuracy entrenamiento: 0.980
Accuracy prueba: 0.924

Validación cruzada (5-fold):
  Scores: [0.9        0.95       0.9        0.97468354 1.        ]
  Media: 0.945 (+/- 0.040)</code></pre>
</div>
</div>
<div id="cell-cancer-feature-importance" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importancia de características</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>importances_cancer <span class="op">=</span> best_tree.feature_importances_</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>indices_cancer <span class="op">=</span> np.argsort(importances_cancer)[::<span class="op">-</span><span class="dv">1</span>][:<span class="dv">10</span>]  <span class="co"># Top 10</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>plt.barh(<span class="bu">range</span>(<span class="dv">10</span>), importances_cancer[indices_cancer], color<span class="op">=</span><span class="st">'coral'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>plt.yticks(<span class="bu">range</span>(<span class="dv">10</span>), [cancer.feature_names[i] <span class="cf">for</span> i <span class="kw">in</span> indices_cancer])</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Importancia (Reducción de Impureza)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Top 10 Características Más Importantes'</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>plt.gca().invert_yaxis()</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadir valores</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, imp <span class="kw">in</span> <span class="bu">enumerate</span>(importances_cancer[indices_cancer]):</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    plt.text(imp <span class="op">+</span> <span class="fl">0.005</span>, i, <span class="ss">f'</span><span class="sc">{</span>imp<span class="sc">:.3f}</span><span class="ss">'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="cancer-feature-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/cancer-feature-importance-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Top 10 características más importantes para clasificar cáncer de mama</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="bagging-de-árboles" class="level2">
<h2 class="anchored" data-anchor-id="bagging-de-árboles">Bagging de Árboles</h2>
<section id="motivación-el-problema-de-la-alta-varianza" class="level3">
<h3 class="anchored" data-anchor-id="motivación-el-problema-de-la-alta-varianza">Motivación: El Problema de la Alta Varianza</h3>
<p>Como hemos visto, los árboles de decisión individuales sufren de <strong>alta varianza</strong>: pequeños cambios en los datos de entrenamiento pueden producir árboles completamente diferentes. Esta inestabilidad limita su capacidad de generalización.</p>
<p>Recordemos la descomposición bias-variance del error esperado:</p>
<p><span class="math display">\[\text{Error esperado} = \text{Bias}^2 + \text{Varianza} + \text{Ruido irreducible}\]</span></p>
<p>Los árboles grandes (sin poda) tienen:</p>
<ul>
<li><strong>Bajo sesgo</strong>: Pueden aproximar relaciones complejas</li>
<li><strong>Alta varianza</strong>: Son muy sensibles a los datos específicos de entrenamiento</li>
</ul>
<p><strong>Bagging</strong> (Bootstrap Aggregating) es una técnica que reduce la varianza sin aumentar significativamente el sesgo, mejorando así el desempeño general del modelo.</p>
</section>
<section id="qué-es-bagging" class="level3">
<h3 class="anchored" data-anchor-id="qué-es-bagging">¿Qué es Bagging?</h3>
<p><strong>Bagging</strong> combina las predicciones de múltiples modelos entrenados en diferentes submuestras de los datos. La idea fundamental es:</p>
<blockquote class="blockquote">
<p>“Si tenemos múltiples estimadores independientes con la misma distribución, el promedio de sus predicciones tiene la misma media (sesgo) pero menor varianza.”</p>
</blockquote>
<p>Matemáticamente, si tenemos <span class="math inline">\(B\)</span> modelos independientes <span class="math inline">\(\hat{f}_1(x), \hat{f}_2(x), ..., \hat{f}_B(x)\)</span> con:</p>
<p><span class="math display">\[\mathbb{E}[\hat{f}_b(x)] = \mu(x), \quad \text{Var}[\hat{f}_b(x)] = \sigma^2(x)\]</span></p>
<p>Entonces el promedio tiene:</p>
<p><span class="math display">\[\mathbb{E}\left[\frac{1}{B}\sum_{b=1}^B \hat{f}_b(x)\right] = \mu(x) \quad \text{(mismo sesgo)}\]</span></p>
<p><span class="math display">\[\text{Var}\left[\frac{1}{B}\sum_{b=1}^B \hat{f}_b(x)\right] = \frac{\sigma^2(x)}{B} \quad \text{(varianza reducida)}\]</span></p>
<p>El problema es que en la práctica <strong>no tenemos múltiples conjuntos de entrenamiento independientes</strong>. Bagging resuelve esto usando <strong>bootstrap</strong>.</p>
</section>
<section id="el-algoritmo-de-bagging" class="level3">
<h3 class="anchored" data-anchor-id="el-algoritmo-de-bagging">El Algoritmo de Bagging</h3>
<p><strong>Algoritmo: Bagging para Árboles de Decisión</strong></p>
<pre><code>Entrada:
  - Conjunto de entrenamiento D = {(x₁, y₁), ..., (xₙ, yₙ)}
  - Número de árboles B

Para b = 1 hasta B:
  1. Generar muestra bootstrap D*ᵦ:
     - Muestrear n observaciones de D con reemplazo
     - Aproximadamente 63% de las observaciones originales aparecerán al menos una vez

  2. Entrenar árbol completo T*ᵦ en D*ᵦ:
     - Sin poda (dejar crecer hasta profundidad máxima)
     - min_samples_leaf puede ser mayor (ej: 5-10) para árboles más estables

Para predecir y = f(x) para nueva observación x:
  - Regresión: ŷ(x) = (1/B) ∑ᵇ₌₁ᴮ T*ᵦ(x)
  - Clasificación: ŷ(x) = mayoría de votos o promedio de probabilidades</code></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Muestreo Bootstrap
</div>
</div>
<div class="callout-body-container callout-body">
<p>En cada muestra bootstrap de tamaño <span class="math inline">\(n\)</span>:</p>
<ul>
<li><strong>≈ 63.2%</strong> de las observaciones originales aparecen al menos una vez</li>
<li><strong>≈ 36.8%</strong> de las observaciones nunca son seleccionadas (llamadas <strong>out-of-bag</strong> o OOB)</li>
</ul>
<p>Esto ocurre porque la probabilidad de que una observación NO sea seleccionada en <span class="math inline">\(n\)</span> extracciones es:</p>
<p><span class="math display">\[\left(1 - \frac{1}{n}\right)^n \to \frac{1}{e} \approx 0.368 \quad \text{cuando } n \to \infty\]</span></p>
</div>
</div>
</section>
<section id="implementación-en-python" class="level3">
<h3 class="anchored" data-anchor-id="implementación-en-python">Implementación en Python</h3>
<div id="bagging-implementation" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos sintéticos</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">400</span>,</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    flip_y<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir datos</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Árbol individual (sin bagging)</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>single_tree <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>single_tree.fit(X_train, y_train)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Bagging con diferentes números de árboles</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>n_trees_list <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>]</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>bagging_models <span class="op">=</span> []</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_trees <span class="kw">in</span> n_trees_list:</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    bagging <span class="op">=</span> BaggingClassifier(</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        estimator<span class="op">=</span>DecisionTreeClassifier(),</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_trees,</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        max_samples<span class="op">=</span><span class="fl">1.0</span>,  <span class="co"># Usar 100% de los datos en cada bootstrap</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        max_features<span class="op">=</span><span class="fl">1.0</span>,  <span class="co"># Usar todas las características</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        bootstrap<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    bagging.fit(X_train, y_train)</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    bagging_models.append(bagging)</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar accuracy</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"COMPARACIÓN: Árbol Individual vs Bagging"</span>)</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Modelo'</span><span class="sc">:&lt;30}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Train Acc'</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Test Acc'</span><span class="sc">:&lt;12}</span><span class="ss">"</span>)</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>train_acc_single <span class="op">=</span> single_tree.score(X_train, y_train)</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>test_acc_single <span class="op">=</span> single_tree.score(X_test, y_test)</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Árbol individual'</span><span class="sc">:&lt;30}</span><span class="ss"> </span><span class="sc">{</span>train_acc_single<span class="sc">:&lt;12.3f}</span><span class="ss"> </span><span class="sc">{</span>test_acc_single<span class="sc">:&lt;12.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_trees, model <span class="kw">in</span> <span class="bu">zip</span>(n_trees_list, bagging_models):</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">=</span> model.score(X_train, y_train)</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    test_acc <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Bagging (B='</span> <span class="op">+</span> <span class="bu">str</span>(n_trees) <span class="op">+</span> <span class="st">')'</span><span class="sc">:&lt;30}</span><span class="ss"> </span><span class="sc">{</span>train_acc<span class="sc">:&lt;12.3f}</span><span class="ss"> </span><span class="sc">{</span>test_acc<span class="sc">:&lt;12.3f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>COMPARACIÓN: Árbol Individual vs Bagging
============================================================
Modelo                         Train Acc    Test Acc    
------------------------------------------------------------
Árbol individual               1.000        0.675       
Bagging (B=1)                  0.918        0.733       
Bagging (B=10)                 0.971        0.783       
Bagging (B=50)                 1.000        0.758       
Bagging (B=100)                1.000        0.758       
Bagging (B=200)                1.000        0.750       </code></pre>
</div>
</div>
</section>
<section id="análisis-de-bias-variance-con-bagging" class="level3">
<h3 class="anchored" data-anchor-id="análisis-de-bias-variance-con-bagging">Análisis de Bias-Variance con Bagging</h3>
<p>La reducción de varianza en bagging depende de la <strong>correlación entre los árboles</strong>. La varianza real del ensemble es:</p>
<p><span class="math display">\[\text{Var}[\bar{T}(x)] = \sigma^2(x) \left[\frac{1}{B} + \left(1 - \frac{1}{B}\right)\rho(x)\right]\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(\sigma^2(x)\)</span> es la varianza de un árbol individual</li>
<li><span class="math inline">\(\rho(x)\)</span> es la correlación promedio entre pares de árboles</li>
<li><span class="math inline">\(B\)</span> es el número de árboles en el ensemble</li>
</ul>
<p><strong>Análisis del límite cuando <span class="math inline">\(B \to \infty\)</span>:</strong></p>
<p><span class="math display">\[\lim_{B \to \infty} \text{Var}[\bar{T}(x)] = \sigma^2(x) \cdot \rho(x)\]</span></p>
<p><strong>Implicaciones:</strong></p>
<ol type="1">
<li><strong>Si <span class="math inline">\(\rho(x) = 0\)</span></strong> (árboles independientes): Varianza → 0 cuando <span class="math inline">\(B \to \infty\)</span> ✓</li>
<li><strong>Si <span class="math inline">\(\rho(x) = 1\)</span></strong> (árboles idénticos): Varianza = <span class="math inline">\(\sigma^2(x)\)</span> (sin mejora) ✗</li>
<li><strong>En la práctica</strong>: <span class="math inline">\(0 &lt; \rho(x) &lt; 1\)</span>, mejora limitada pero significativa</li>
</ol>
<p>Como las muestras bootstrap <strong>no son independientes</strong> (se extraen del mismo conjunto de datos), existe correlación positiva entre los árboles, lo que limita la reducción de varianza.</p>
<div id="cell-bagging-variance-analysis" class="cell" data-fig-height="5" data-fig-width="14" data-execution_count="12">
<div class="cell-output cell-output-display">
<div id="bagging-variance-analysis" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/bagging-variance-analysis-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Análisis de la reducción de varianza con Bagging</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Varianza de predicciones por número de árboles:
============================================================
B (árboles)     Varianza        Reducción %    
------------------------------------------------------------
1               0.0124          0.0            
5               0.0183          -47.3          
10              0.0095          23.3           
25              0.0038          69.4           
50              0.0018          85.8           
100             0.0009          92.8           </code></pre>
</div>
</div>
</section>
<section id="error-out-of-bag-oob" class="level3">
<h3 class="anchored" data-anchor-id="error-out-of-bag-oob">Error Out-of-Bag (OOB)</h3>
<p>Una ventaja única de bagging es que podemos estimar el <strong>error de test</strong> sin necesidad de un conjunto de validación separado, usando las observaciones <strong>out-of-bag</strong>.</p>
<p><strong>Algoritmo para calcular OOB Error:</strong></p>
<pre><code>Para cada observación i en el conjunto de entrenamiento:
  1. Identificar qué árboles NO usaron la observación i (≈ 36.8% de los árboles)
  2. Obtener predicción promediando solo esos árboles: ŷᵢ^OOB
  3. Comparar ŷᵢ^OOB con yᵢ

OOB Error = (1/n) ∑ᵢ₌₁ⁿ L(yᵢ, ŷᵢ^OOB)</code></pre>
<p>El error OOB es una estimación <strong>casi insesgada</strong> del error de test, similar a validación cruzada leave-one-out pero mucho más eficiente computacionalmente.</p>
<div id="cell-oob-error-demonstration" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="13">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar bagging con OOB habilitado</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>n_trees_range <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">5</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>oob_errors <span class="op">=</span> []</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>test_errors <span class="op">=</span> []</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_trees <span class="kw">in</span> n_trees_range:</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    bagging_oob <span class="op">=</span> BaggingClassifier(</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        estimator<span class="op">=</span>DecisionTreeClassifier(),</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_trees,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        bootstrap<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        oob_score<span class="op">=</span><span class="va">True</span>,  <span class="co"># Calcular OOB score</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    bagging_oob.fit(X_train, y_train)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># OOB error</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    oob_accuracy <span class="op">=</span> bagging_oob.oob_score_</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    oob_errors.append(<span class="dv">1</span> <span class="op">-</span> oob_accuracy)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test error</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>    test_accuracy <span class="op">=</span> bagging_oob.score(X_test, y_test)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    test_errors.append(<span class="dv">1</span> <span class="op">-</span> test_accuracy)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: Curvas de error</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(n_trees_range, oob_errors, <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'OOB Error'</span>,</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>            linewidth<span class="op">=</span><span class="dv">2</span>, markersize<span class="op">=</span><span class="dv">4</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(n_trees_range, test_errors, <span class="st">'s-'</span>, label<span class="op">=</span><span class="st">'Test Error'</span>,</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>            linewidth<span class="op">=</span><span class="dv">2</span>, markersize<span class="op">=</span><span class="dv">4</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Número de Árboles'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Tasa de Error'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'OOB Error vs Test Error'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Diferencia entre OOB y Test</span></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>difference <span class="op">=</span> np.array(oob_errors) <span class="op">-</span> np.array(test_errors)</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(n_trees_range, difference, <span class="st">'o-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, markersize<span class="op">=</span><span class="dv">4</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].fill_between(n_trees_range, <span class="dv">0</span>, difference, alpha<span class="op">=</span><span class="fl">0.3</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Número de Árboles'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'OOB Error - Test Error'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Diferencia entre OOB y Test Error'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Comparación OOB vs Test Error:"</span>)</span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlación entre OOB y Test Error: </span><span class="sc">{</span>np<span class="sc">.</span>corrcoef(oob_errors, test_errors)[<span class="dv">0</span>,<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Diferencia promedio: </span><span class="sc">{</span>np<span class="sc">.</span>mean(difference)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Desviación estándar de la diferencia: </span><span class="sc">{</span>np<span class="sc">.</span>std(difference)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:917: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.
  warn(
/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:923: RuntimeWarning: invalid value encountered in divide
  oob_decision_function = predictions / predictions.sum(axis=1)[:, np.newaxis]
/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:917: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.
  warn(
/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:923: RuntimeWarning: invalid value encountered in divide
  oob_decision_function = predictions / predictions.sum(axis=1)[:, np.newaxis]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="oob-error-demonstration" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/oob-error-demonstration-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Comparación entre OOB Error y Test Error</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Comparación OOB vs Test Error:
============================================================
Correlación entre OOB y Test Error: 0.345
Diferencia promedio: -0.0104
Desviación estándar de la diferencia: 0.0371</code></pre>
</div>
</div>
</section>
<section id="importancia-de-variables-en-bagging" class="level3">
<h3 class="anchored" data-anchor-id="importancia-de-variables-en-bagging">Importancia de Variables en Bagging</h3>
<p>Bagging permite calcular la <strong>importancia de variables</strong> de manera más robusta que un árbol individual, usando el <strong>método de permutación</strong> propuesto por Breiman.</p>
<p><strong>Algoritmo: Importancia por Permutación con OOB</strong></p>
<pre><code>Para cada variable k:
  1. Para cada árbol T*ᵦ en el ensemble:
     a. Calcular error OOB normal: Error_OOBᵦ
     b. Permutar aleatoriamente los valores de variable k en datos OOB
     c. Calcular error OOB con permutación: Error_OOB_permᵦ(k)
     d. Degradación: Dₖ(T*ᵦ) = Error_OOB_permᵦ(k) - Error_OOBᵦ

  2. Importancia(k) = (1/B) ∑ᵇ₌₁ᴮ Dₖ(T*ᵦ)

Variables importantes → Mayor degradación al permutar
Variables irrelevantes → Poca o ninguna degradación</code></pre>
<p><strong>Intuición:</strong> Si una variable es importante, romper su relación con la variable respuesta (mediante permutación) degrada significativamente las predicciones.</p>
<div id="cell-bagging-feature-importance" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="14">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos con más características</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>X_multi, y_multi <span class="op">=</span> make_classification(</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    n_repeated<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>X_train_m, X_test_m, y_train_m, y_test_m <span class="op">=</span> train_test_split(</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    X_multi, y_multi, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>feature_names_m <span class="op">=</span> [<span class="ss">f'X</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Árbol individual</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>tree_single <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>tree_single.fit(X_train_m, y_train_m)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>importance_tree <span class="op">=</span> tree_single.feature_importances_</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Bagging</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>bagging_multi <span class="op">=</span> BaggingClassifier(</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>DecisionTreeClassifier(),</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>    bootstrap<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>    oob_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>bagging_multi.fit(X_train_m, y_train_m)</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular importancia promediando importancias de árboles individuales</span></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>importance_bagging <span class="op">=</span> np.mean([</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>    tree.feature_importances_ <span class="cf">for</span> tree <span class="kw">in</span> bagging_multi.estimators_</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización comparativa</span></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: Árbol individual</span></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>indices_tree <span class="op">=</span> np.argsort(importance_tree)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].barh(<span class="bu">range</span>(<span class="dv">10</span>), importance_tree[indices_tree], color<span class="op">=</span><span class="st">'steelblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticks(<span class="bu">range</span>(<span class="dv">10</span>))</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticklabels([feature_names_m[i] <span class="cf">for</span> i <span class="kw">in</span> indices_tree])</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Importancia'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Árbol Individual'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Bagging</span></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a>indices_bag <span class="op">=</span> np.argsort(importance_bagging)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].barh(<span class="bu">range</span>(<span class="dv">10</span>), importance_bagging[indices_bag], color<span class="op">=</span><span class="st">'coral'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticks(<span class="bu">range</span>(<span class="dv">10</span>))</span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticklabels([feature_names_m[i] <span class="cf">for</span> i <span class="kw">in</span> indices_bag])</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Importancia'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Bagging (100 árboles)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Comparación de Accuracy:"</span>)</span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Árbol individual: </span><span class="sc">{</span>tree_single<span class="sc">.</span>score(X_test_m, y_test_m)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bagging (OOB):    </span><span class="sc">{</span>bagging_multi<span class="sc">.</span>oob_score_<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bagging (Test):   </span><span class="sc">{</span>bagging_multi<span class="sc">.</span>score(X_test_m, y_test_m)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="bagging-feature-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/bagging-feature-importance-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Importancia de variables en Bagging vs Árbol Individual</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Comparación de Accuracy:
============================================================
Árbol individual: 0.853
Bagging (OOB):    0.866
Bagging (Test):   0.887</code></pre>
</div>
</div>
</section>
<section id="fronteras-de-decisión-árbol-individual-vs-bagging" class="level3">
<h3 class="anchored" data-anchor-id="fronteras-de-decisión-árbol-individual-vs-bagging">Fronteras de Decisión: Árbol Individual vs Bagging</h3>
<div id="cell-bagging-decision-boundaries" class="cell" data-fig-height="5" data-fig-width="14" data-execution_count="15">
<div class="cell-output cell-output-display">
<div id="bagging-decision-boundaries" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/bagging-decision-boundaries-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación de fronteras de decisión: Árbol Individual vs Bagging</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Observaciones:</strong></p>
<ul>
<li>El árbol individual crea fronteras muy complejas y <strong>sobreajustadas</strong></li>
<li>Bagging suaviza las fronteras al promediar múltiples árboles, reduciendo <strong>overfitting</strong></li>
<li>Las regiones de decisión en bagging son más <strong>estables</strong> y <strong>generalizables</strong></li>
</ul>
</section>
<section id="ventajas-y-desventajas-de-bagging" class="level3">
<h3 class="anchored" data-anchor-id="ventajas-y-desventajas-de-bagging">Ventajas y Desventajas de Bagging</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ventajas de Bagging
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Reducción de varianza</strong>: Mejora significativa sobre árboles individuales</li>
<li><strong>Mantiene bajo sesgo</strong>: Usa árboles grandes sin poda</li>
<li><strong>OOB error</strong>: Estimación de test error sin conjunto de validación adicional</li>
<li><strong>Fácil paralelización</strong>: Árboles se entrenan independientemente</li>
<li><strong>Importancia de variables robusta</strong>: Menos sensible a variabilidad en datos</li>
<li><strong>Raramente sobreajusta</strong>: Aumentar <span class="math inline">\(B\)</span> no degrada desempeño en test</li>
<li><strong>Hereda ventajas de árboles</strong>: Robusto a outliers, maneja datos mixtos</li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Desventajas de Bagging
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Correlación entre árboles</strong>: Limita reducción de varianza (todos usan mismos datos)</li>
<li><strong>Pérdida de interpretabilidad</strong>: Ya no tenemos un árbol simple de visualizar</li>
<li><strong>Costo computacional</strong>: Entrenar y almacenar múltiples árboles</li>
<li><strong>Predicción más lenta</strong>: Debe consultar todos los árboles para una predicción</li>
<li><strong>Mejora modesta</strong>: Random Forest supera a bagging al decorrelacionar más los árboles</li>
</ol>
</div>
</div>
</section>
<section id="cuántos-árboles-usar" class="level3">
<h3 class="anchored" data-anchor-id="cuántos-árboles-usar">¿Cuántos Árboles Usar?</h3>
<div id="cell-bagging-convergence" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="16">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar convergencia</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>n_trees_conv <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">5</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>train_scores_conv <span class="op">=</span> []</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>test_scores_conv <span class="op">=</span> []</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_trees <span class="kw">in</span> n_trees_conv:</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    bagging_conv <span class="op">=</span> BaggingClassifier(</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        estimator<span class="op">=</span>DecisionTreeClassifier(),</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_trees,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        bootstrap<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    bagging_conv.fit(X_train, y_train)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    train_scores_conv.append(bagging_conv.score(X_train, y_train))</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    test_scores_conv.append(bagging_conv.score(X_test, y_test))</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>plt.plot(n_trees_conv, train_scores_conv, label<span class="op">=</span><span class="st">'Entrenamiento'</span>,</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>        linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>plt.plot(n_trees_conv, test_scores_conv, label<span class="op">=</span><span class="st">'Prueba'</span>,</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        linewidth<span class="op">=</span><span class="fl">2.5</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Número de Árboles (B)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Convergencia de Bagging con Número de Árboles'</span>, fontsize<span class="op">=</span><span class="dv">13</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar punto de convergencia (cambio &lt; 0.001)</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>diffs <span class="op">=</span> np.<span class="bu">abs</span>(np.diff(test_scores_conv))</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>convergence_idx <span class="op">=</span> np.where(diffs <span class="op">&lt;</span> <span class="fl">0.001</span>)[<span class="dv">0</span>][<span class="dv">0</span>] <span class="cf">if</span> <span class="bu">any</span>(diffs <span class="op">&lt;</span> <span class="fl">0.001</span>) <span class="cf">else</span> <span class="bu">len</span>(n_trees_conv)<span class="op">-</span><span class="dv">1</span></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>convergence_B <span class="op">=</span> n_trees_conv[convergence_idx]</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>convergence_B, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="ss">f'Convergencia ≈ B=</span><span class="sc">{</span>convergence_B<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recomendación sobre número de árboles:"</span>)</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Convergencia aproximada en: B = </span><span class="sc">{</span>convergence_B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy en B=</span><span class="sc">{</span>convergence_B<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>test_scores_conv[convergence_idx]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy en B=</span><span class="sc">{</span>n_trees_conv[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>test_scores_conv[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">En la práctica:"</span>)</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - B = 50-100: Generalmente suficiente"</span>)</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - B = 500-1000: Común en producción para máxima estabilidad"</span>)</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Más árboles → Más computación pero nunca daña (no overfitting)"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="bagging-convergence" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/bagging-convergence-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Convergencia del accuracy con el número de árboles en Bagging</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Recomendación sobre número de árboles:
============================================================
Convergencia aproximada en: B = 11
Test accuracy en B=11: 0.7750
Test accuracy en B=196: 0.7500

En la práctica:
  - B = 50-100: Generalmente suficiente
  - B = 500-1000: Común en producción para máxima estabilidad
  - Más árboles → Más computación pero nunca daña (no overfitting)</code></pre>
</div>
</div>
</section>
<section id="bagging-vs-random-forest" class="level3">
<h3 class="anchored" data-anchor-id="bagging-vs-random-forest">Bagging vs Random Forest</h3>
<p>Aunque bagging es efectivo, en la práctica <strong>Random Forest</strong> es mucho más popular. La principal diferencia es:</p>
<p><strong>Bagging</strong>:</p>
<ul>
<li>Cada árbol usa <strong>todas</strong> las características en cada división</li>
<li>Árboles están correlacionados porque usan las mismas variables</li>
</ul>
<p><strong>Random Forest</strong>:</p>
<ul>
<li>Cada división considera solo una <strong>muestra aleatoria</strong> de características (típicamente <span class="math inline">\(\sqrt{p}\)</span> o <span class="math inline">\(p/3\)</span>)</li>
<li>Mayor <strong>decorrelación</strong> entre árboles → Mayor reducción de varianza</li>
</ul>
<p>Veamos ahora en detalle cómo funciona Random Forest.</p>
</section>
</section>
<section id="random-forest" class="level2">
<h2 class="anchored" data-anchor-id="random-forest">Random Forest</h2>
<section id="motivación-decorrelación-de-árboles" class="level3">
<h3 class="anchored" data-anchor-id="motivación-decorrelación-de-árboles">Motivación: Decorrelación de Árboles</h3>
<p>Como vimos, bagging reduce la varianza promediando múltiples árboles entrenados en muestras bootstrap. Sin embargo, la reducción está limitada por la <strong>correlación entre árboles</strong>:</p>
<p><span class="math display">\[\text{Var}[\bar{T}(x)] = \sigma^2(x) \cdot \rho(x) + \frac{\sigma^2(x)(1-\rho(x))}{B}\]</span></p>
<p>Cuando <span class="math inline">\(B \to \infty\)</span>, la varianza converge a <span class="math inline">\(\sigma^2(x) \cdot \rho(x)\)</span>, no a cero.</p>
<p><strong>Problema en bagging</strong>: Si existe una característica muy predictiva, <strong>todos los árboles</strong> la usarán en las primeras divisiones, haciendo que los árboles se parezcan mucho entre sí.</p>
<p><strong>Solución de Random Forest</strong>: Forzar <strong>decorrelación</strong> restringiendo las características disponibles en cada división.</p>
</section>
<section id="el-algoritmo-de-random-forest" class="level3">
<h3 class="anchored" data-anchor-id="el-algoritmo-de-random-forest">El Algoritmo de Random Forest</h3>
<p><strong>Random Forest</strong> extiende bagging añadiendo <strong>aleatorización en la selección de características</strong>:</p>
<p><strong>Algoritmo: Random Forest</strong></p>
<pre><code>Entrada:
  - Conjunto de entrenamiento D = {(x₁, y₁), ..., (xₙ, yₙ)}
  - Número de árboles B
  - Número de características por división m (típicamente √p para clasificación, p/3 para regresión)

Para b = 1 hasta B:
  1. Generar muestra bootstrap D*ᵦ de tamaño n

  2. Construir árbol T*ᵦ en D*ᵦ con modificación:
     En cada división del árbol:
       a) Seleccionar m características aleatorias del total p
       b) Encontrar mejor división usando SOLO esas m características
       c) Realizar la división

  3. Guardar árbol completo T*ᵦ (sin poda)

Predicción para nueva observación x:
  - Clasificación: ŷ(x) = voto mayoritario de {T*₁(x), ..., T*ᵦ(x)}
  - Regresión: ŷ(x) = (1/B) ∑ᵇ₌₁ᴮ T*ᵦ(x)</code></pre>
<p><strong>Diferencia clave con bagging</strong>: En cada nodo, solo se consideran <span class="math inline">\(m &lt; p\)</span> características aleatorias para la división.</p>
</section>
<section id="hiperparámetros-clave" class="level3">
<h3 class="anchored" data-anchor-id="hiperparámetros-clave">Hiperparámetros Clave</h3>
<p><strong>1. Número de árboles (B o n_estimators)</strong></p>
<ul>
<li>Valores típicos: 100-500</li>
<li>Más árboles → Mejor (no hay overfitting), pero mayor costo computacional</li>
<li>Recomendación: Empezar con 100-200</li>
</ul>
<p><strong>2. Número de características por división (m o max_features)</strong></p>
<ul>
<li><strong>Clasificación</strong>: <span class="math inline">\(m = \sqrt{p}\)</span> (default en scikit-learn)</li>
<li><strong>Regresión</strong>: <span class="math inline">\(m = p/3\)</span> (default en scikit-learn)</li>
<li>Valores más pequeños → Mayor decorrelación pero mayor sesgo</li>
<li>Valores más grandes → Menor decorrelación pero menor sesgo</li>
</ul>
<p><strong>3. Profundidad del árbol (max_depth)</strong></p>
<ul>
<li>Default: <code>None</code> (árboles completos)</li>
<li>Random Forest usa árboles muy profundos, la regularización viene del ensemble</li>
<li>Limitar solo si hay problemas de memoria o tiempo de entrenamiento</li>
</ul>
<p><strong>4. Tamaño mínimo de hoja (min_samples_leaf)</strong></p>
<ul>
<li>Valores típicos: 1 (clasificación), 5 (regresión)</li>
<li>Mayor → Árboles más suaves, menor varianza</li>
</ul>
<p><strong>5. Número de muestras para dividir (min_samples_split)</strong></p>
<ul>
<li>Default: 2</li>
<li>Mayor → Regularización más fuerte</li>
</ul>
</section>
<section id="implementación-y-comparación" class="level3">
<h3 class="anchored" data-anchor-id="implementación-y-comparación">Implementación y Comparación</h3>
<div id="random-forest-implementation" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier, BaggingClassifier</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"COMPARACIÓN: Árbol → Bagging → Random Forest"</span>)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Árbol individual</span></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>tree_single <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>tree_single.fit(X_train, y_train)</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>train_acc_tree <span class="op">=</span> tree_single.score(X_train, y_train)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>test_acc_tree <span class="op">=</span> tree_single.score(X_test, y_test)</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">1. Árbol Individual"</span>)</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Train Accuracy: </span><span class="sc">{</span>train_acc_tree<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Test Accuracy:  </span><span class="sc">{</span>test_acc_tree<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Overfitting:    </span><span class="sc">{</span>train_acc_tree <span class="op">-</span> test_acc_tree<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Bagging</span></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>bagging_model <span class="op">=</span> BaggingClassifier(</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>DecisionTreeClassifier(),</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>    bootstrap<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>    oob_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>bagging_model.fit(X_train, y_train)</span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>train_acc_bag <span class="op">=</span> bagging_model.score(X_train, y_train)</span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>test_acc_bag <span class="op">=</span> bagging_model.score(X_test, y_test)</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>oob_acc_bag <span class="op">=</span> bagging_model.oob_score_</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">2. Bagging (100 árboles)"</span>)</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Train Accuracy: </span><span class="sc">{</span>train_acc_bag<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   OOB Accuracy:   </span><span class="sc">{</span>oob_acc_bag<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Test Accuracy:  </span><span class="sc">{</span>test_acc_bag<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Overfitting:    </span><span class="sc">{</span>train_acc_bag <span class="op">-</span> test_acc_bag<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Random Forest</span></span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestClassifier(</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>    max_features<span class="op">=</span><span class="st">'sqrt'</span>,  <span class="co"># √p características</span></span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>    bootstrap<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a>    oob_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a>train_acc_rf <span class="op">=</span> rf_model.score(X_train, y_train)</span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a>test_acc_rf <span class="op">=</span> rf_model.score(X_test, y_test)</span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a>oob_acc_rf <span class="op">=</span> rf_model.oob_score_</span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-72"><a href="#cb33-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">3. Random Forest (100 árboles, max_features='sqrt')"</span>)</span>
<span id="cb33-73"><a href="#cb33-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb33-74"><a href="#cb33-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Train Accuracy: </span><span class="sc">{</span>train_acc_rf<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-75"><a href="#cb33-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   OOB Accuracy:   </span><span class="sc">{</span>oob_acc_rf<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-76"><a href="#cb33-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Test Accuracy:  </span><span class="sc">{</span>test_acc_rf<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-77"><a href="#cb33-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"   Overfitting:    </span><span class="sc">{</span>train_acc_rf <span class="op">-</span> test_acc_rf<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-78"><a href="#cb33-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-79"><a href="#cb33-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-80"><a href="#cb33-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RESUMEN DE MEJORAS"</span>)</span>
<span id="cb33-81"><a href="#cb33-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb33-82"><a href="#cb33-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Accuracy improvement (Árbol → Bagging):      </span><span class="sc">{</span>test_acc_bag <span class="op">-</span> test_acc_tree<span class="sc">:+.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-83"><a href="#cb33-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Accuracy improvement (Bagging → RF):         </span><span class="sc">{</span>test_acc_rf <span class="op">-</span> test_acc_bag<span class="sc">:+.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-84"><a href="#cb33-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Accuracy improvement (Árbol → RF):           </span><span class="sc">{</span>test_acc_rf <span class="op">-</span> test_acc_tree<span class="sc">:+.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>COMPARACIÓN: Árbol → Bagging → Random Forest
======================================================================

1. Árbol Individual
----------------------------------------------------------------------
   Train Accuracy: 1.0000
   Test Accuracy:  0.7667
   Overfitting:    0.2333

2. Bagging (100 árboles)
----------------------------------------------------------------------
   Train Accuracy: 1.0000
   OOB Accuracy:   0.8629
   Test Accuracy:  0.8400
   Overfitting:    0.1600

3. Random Forest (100 árboles, max_features='sqrt')
----------------------------------------------------------------------
   Train Accuracy: 1.0000
   OOB Accuracy:   0.8743
   Test Accuracy:  0.8800
   Overfitting:    0.1200

======================================================================
RESUMEN DE MEJORAS
======================================================================
Test Accuracy improvement (Árbol → Bagging):      +0.0733
Test Accuracy improvement (Bagging → RF):         +0.0400
Test Accuracy improvement (Árbol → RF):           +0.1133</code></pre>
</div>
</div>
</section>
<section id="análisis-del-efecto-de-max_features" class="level3">
<h3 class="anchored" data-anchor-id="análisis-del-efecto-de-max_features">Análisis del Efecto de max_features</h3>
<div id="cell-rf-max-features-analysis" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="18">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Probar diferentes valores de max_features</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>max_features_values <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>]</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>max_features_labels <span class="op">=</span> []</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>train_scores_mf <span class="op">=</span> []</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>test_scores_mf <span class="op">=</span> []</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>cv_scores_mf <span class="op">=</span> []</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mf <span class="kw">in</span> max_features_values:</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    rf <span class="op">=</span> RandomForestClassifier(</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        max_features<span class="op">=</span>mf,</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    rf.fit(X_train, y_train)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>    train_scores_mf.append(rf.score(X_train, y_train))</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    test_scores_mf.append(rf.score(X_test, y_test))</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cross-validation</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    cv_scores <span class="op">=</span> cross_val_score(rf, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    cv_scores_mf.append(cv_scores.mean())</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Etiqueta para el gráfico</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mf <span class="op">==</span> <span class="st">'sqrt'</span>:</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="ss">f'sqrt (</span><span class="sc">{</span><span class="bu">int</span>(np.sqrt(X.shape[<span class="dv">1</span>]))<span class="sc">}</span><span class="ss">)'</span></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> mf <span class="op">==</span> <span class="st">'log2'</span>:</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="ss">f'log2 (</span><span class="sc">{</span><span class="bu">int</span>(np.log2(X.shape[<span class="dv">1</span>]))<span class="sc">}</span><span class="ss">)'</span></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> mf <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="ss">f'All (</span><span class="sc">{</span>X<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">)'</span></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="bu">str</span>(mf)</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>    max_features_labels.append(label)</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización</span></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: Accuracy por max_features</span></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>x_pos <span class="op">=</span> np.arange(<span class="bu">len</span>(max_features_labels))</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(x_pos <span class="op">-</span> width, train_scores_mf, width, label<span class="op">=</span><span class="st">'Train'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(x_pos, cv_scores_mf, width, label<span class="op">=</span><span class="st">'CV (5-fold)'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(x_pos <span class="op">+</span> width, test_scores_mf, width, label<span class="op">=</span><span class="st">'Test'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'max_features'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Impacto de max_features en Random Forest'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticks(x_pos)</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticklabels(max_features_labels, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Overfitting (Train - Test gap)</span></span>
<span id="cb35-57"><a href="#cb35-57" aria-hidden="true" tabindex="-1"></a>overfitting_gap <span class="op">=</span> np.array(train_scores_mf) <span class="op">-</span> np.array(test_scores_mf)</span>
<span id="cb35-58"><a href="#cb35-58" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'red'</span> <span class="cf">if</span> gap <span class="op">&gt;</span> <span class="fl">0.1</span> <span class="cf">else</span> <span class="st">'orange'</span> <span class="cf">if</span> gap <span class="op">&gt;</span> <span class="fl">0.05</span> <span class="cf">else</span> <span class="st">'green'</span> <span class="cf">for</span> gap <span class="kw">in</span> overfitting_gap]</span>
<span id="cb35-59"><a href="#cb35-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-60"><a href="#cb35-60" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar(x_pos, overfitting_gap, color<span class="op">=</span>colors, alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb35-61"><a href="#cb35-61" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-62"><a href="#cb35-62" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'max_features'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb35-63"><a href="#cb35-63" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Train Accuracy - Test Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb35-64"><a href="#cb35-64" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Gap de Overfitting por max_features'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb35-65"><a href="#cb35-65" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticks(x_pos)</span>
<span id="cb35-66"><a href="#cb35-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticklabels(max_features_labels, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb35-67"><a href="#cb35-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb35-68"><a href="#cb35-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-69"><a href="#cb35-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadir leyenda de colores</span></span>
<span id="cb35-70"><a href="#cb35-70" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Patch</span>
<span id="cb35-71"><a href="#cb35-71" aria-hidden="true" tabindex="-1"></a>legend_elements <span class="op">=</span> [</span>
<span id="cb35-72"><a href="#cb35-72" aria-hidden="true" tabindex="-1"></a>    Patch(facecolor<span class="op">=</span><span class="st">'green'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Bajo (&lt;0.05)'</span>),</span>
<span id="cb35-73"><a href="#cb35-73" aria-hidden="true" tabindex="-1"></a>    Patch(facecolor<span class="op">=</span><span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Moderado (0.05-0.10)'</span>),</span>
<span id="cb35-74"><a href="#cb35-74" aria-hidden="true" tabindex="-1"></a>    Patch(facecolor<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Alto (&gt;0.10)'</span>)</span>
<span id="cb35-75"><a href="#cb35-75" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb35-76"><a href="#cb35-76" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend(handles<span class="op">=</span>legend_elements, title<span class="op">=</span><span class="st">'Overfitting'</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb35-77"><a href="#cb35-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-78"><a href="#cb35-78" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb35-79"><a href="#cb35-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb35-80"><a href="#cb35-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-81"><a href="#cb35-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Encontrar mejor max_features</span></span>
<span id="cb35-82"><a href="#cb35-82" aria-hidden="true" tabindex="-1"></a>best_idx <span class="op">=</span> np.argmax(test_scores_mf)</span>
<span id="cb35-83"><a href="#cb35-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Mejor max_features: </span><span class="sc">{</span>max_features_labels[best_idx]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-84"><a href="#cb35-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Accuracy: </span><span class="sc">{</span>test_scores_mf[best_idx]<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="rf-max-features-analysis" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/rf-max-features-analysis-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Impacto del número de características (max_features) en Random Forest</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Mejor max_features: 1
Test Accuracy: 0.9133</code></pre>
</div>
</div>
</section>
<section id="curva-de-aprendizaje-número-de-árboles" class="level3">
<h3 class="anchored" data-anchor-id="curva-de-aprendizaje-número-de-árboles">Curva de Aprendizaje: Número de Árboles</h3>
<div id="cell-rf-learning-curve-trees" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="19">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar convergencia con número de árboles</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>n_trees_range <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">5</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>train_scores_conv <span class="op">=</span> []</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>oob_scores_conv <span class="op">=</span> []</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>test_scores_conv <span class="op">=</span> []</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_trees <span class="kw">in</span> n_trees_range:</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    rf_conv <span class="op">=</span> RandomForestClassifier(</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_trees,</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>        max_features<span class="op">=</span><span class="st">'sqrt'</span>,</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>        oob_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    rf_conv.fit(X_train, y_train)</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    train_scores_conv.append(rf_conv.score(X_train, y_train))</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    oob_scores_conv.append(rf_conv.oob_score_)</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>    test_scores_conv.append(rf_conv.score(X_test, y_test))</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización</span></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: Accuracy vs Número de Árboles</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(n_trees_range, train_scores_conv, label<span class="op">=</span><span class="st">'Train'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(n_trees_range, oob_scores_conv, label<span class="op">=</span><span class="st">'OOB'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(n_trees_range, test_scores_conv, label<span class="op">=</span><span class="st">'Test'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Número de Árboles'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Convergencia de Random Forest'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar punto de convergencia</span></span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>diffs <span class="op">=</span> np.<span class="bu">abs</span>(np.diff(test_scores_conv))</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">any</span>(diffs <span class="op">&lt;</span> <span class="fl">0.001</span>):</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>    conv_idx <span class="op">=</span> np.where(diffs <span class="op">&lt;</span> <span class="fl">0.001</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>    conv_trees <span class="op">=</span> n_trees_range[conv_idx]</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].axvline(x<span class="op">=</span>conv_trees, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>                   label<span class="op">=</span><span class="ss">f'Convergencia ≈ </span><span class="sc">{</span>conv_trees<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].legend()</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Variabilidad de Test Accuracy</span></span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a>rolling_std <span class="op">=</span> pd.Series(test_scores_conv).rolling(window<span class="op">=</span>window_size).std()</span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(n_trees_range, test_scores_conv, <span class="st">'o-'</span>, markersize<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'Test Accuracy'</span>)</span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(n_trees_range, rolling_std, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'red'</span>,</span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Desv. Std. (ventana=</span><span class="sc">{</span>window_size<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Número de Árboles'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Valor'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Estabilización del Test Accuracy'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb37-54"><a href="#cb37-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb37-55"><a href="#cb37-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-56"><a href="#cb37-56" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb37-57"><a href="#cb37-57" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb37-58"><a href="#cb37-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-59"><a href="#cb37-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Recomendación:"</span>)</span>
<span id="cb37-60"><a href="#cb37-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Accuracy estabiliza alrededor de </span><span class="sc">{</span>conv_trees <span class="cf">if</span> <span class="bu">any</span>(diffs <span class="op">&lt;</span> <span class="fl">0.001</span>) <span class="cf">else</span> <span class="st">'N/A'</span><span class="sc">}</span><span class="ss"> árboles"</span>)</span>
<span id="cb37-61"><a href="#cb37-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Test accuracy final (200 árboles): </span><span class="sc">{</span>test_scores_conv[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb37-62"><a href="#cb37-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Variabilidad final: </span><span class="sc">{</span>rolling_std<span class="sc">.</span>iloc[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.5f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:611: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.
  warn(
/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:611: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.
  warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="rf-learning-curve-trees" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/rf-learning-curve-trees-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Convergencia de Random Forest con el número de árboles</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Recomendación:
  - Accuracy estabiliza alrededor de 21 árboles
  - Test accuracy final (200 árboles): 0.8867
  - Variabilidad final: 0.00984</code></pre>
</div>
</div>
</section>
<section id="importancia-de-variables-en-random-forest" class="level3">
<h3 class="anchored" data-anchor-id="importancia-de-variables-en-random-forest">Importancia de Variables en Random Forest</h3>
<p>Random Forest proporciona dos medidas de importancia:</p>
<p><strong>1. Mean Decrease in Impurity (MDI)</strong> - Default en scikit-learn</p>
<p><span class="math display">\[\text{Importancia}(X_j) = \frac{1}{B} \sum_{b=1}^{B} \sum_{t \in T_b : \text{usa } X_j} \frac{n_t}{n} \Delta I(t)\]</span></p>
<p><strong>2. Permutation Importance</strong> - Más robusta</p>
<p>Mide la degradación en accuracy al permutar aleatoriamente una característica.</p>
<div id="cell-rf-feature-importance" class="cell" data-fig-height="6" data-fig-width="14" data-execution_count="20">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> permutation_importance</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar Random Forest</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>rf_imp <span class="op">=</span> RandomForestClassifier(</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    max_features<span class="op">=</span><span class="st">'sqrt'</span>,</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>rf_imp.fit(X_train, y_train)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Mean Decrease in Impurity (MDI)</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>mdi_importance <span class="op">=</span> rf_imp.feature_importances_</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Permutation Importance</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>perm_importance <span class="op">=</span> permutation_importance(</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    rf_imp, X_test, y_test, n_repeats<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización comparativa</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">6</span>))</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: MDI Importance (Top 10)</span></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>mdi_indices <span class="op">=</span> np.argsort(mdi_importance)[::<span class="op">-</span><span class="dv">1</span>][:top_n]</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].barh(<span class="bu">range</span>(top_n), mdi_importance[mdi_indices], color<span class="op">=</span><span class="st">'steelblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticks(<span class="bu">range</span>(top_n))</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticklabels([<span class="ss">f'X</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> mdi_indices])</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].invert_yaxis()</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Mean Decrease in Impurity'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Importancia MDI (Top 10)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadir valores</span></span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, imp <span class="kw">in</span> <span class="bu">enumerate</span>(mdi_importance[mdi_indices]):</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(imp <span class="op">+</span> <span class="fl">0.002</span>, i, <span class="ss">f'</span><span class="sc">{</span>imp<span class="sc">:.3f}</span><span class="ss">'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Permutation Importance (Top 10)</span></span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>perm_means <span class="op">=</span> perm_importance.importances_mean</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>perm_std <span class="op">=</span> perm_importance.importances_std</span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>perm_indices <span class="op">=</span> np.argsort(perm_means)[::<span class="op">-</span><span class="dv">1</span>][:top_n]</span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].barh(<span class="bu">range</span>(top_n), perm_means[perm_indices],</span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>            xerr<span class="op">=</span>perm_std[perm_indices], color<span class="op">=</span><span class="st">'coral'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, capsize<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticks(<span class="bu">range</span>(top_n))</span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticklabels([<span class="ss">f'X</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> perm_indices])</span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].invert_yaxis()</span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Permutation Importance'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Importancia por Permutación (Top 10)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Añadir valores</span></span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (mean, std) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(perm_means[perm_indices], perm_std[perm_indices])):</span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].text(mean <span class="op">+</span> <span class="fl">0.002</span>, i, <span class="ss">f'</span><span class="sc">{</span>mean<span class="sc">:.3f}</span><span class="ss">±</span><span class="sc">{</span>std<span class="sc">:.3f}</span><span class="ss">'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb40-57"><a href="#cb40-57" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb40-58"><a href="#cb40-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-59"><a href="#cb40-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Tabla comparativa</span></span>
<span id="cb40-60"><a href="#cb40-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Comparación de Rankings (Top 10):"</span>)</span>
<span id="cb40-61"><a href="#cb40-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb40-62"><a href="#cb40-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Rank'</span><span class="sc">:&lt;6}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'MDI'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'MDI Value'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Permutation'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Perm Value'</span><span class="sc">:&lt;15}</span><span class="ss">"</span>)</span>
<span id="cb40-63"><a href="#cb40-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb40-64"><a href="#cb40-64" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> rank <span class="kw">in</span> <span class="bu">range</span>(top_n):</span>
<span id="cb40-65"><a href="#cb40-65" aria-hidden="true" tabindex="-1"></a>    mdi_feat <span class="op">=</span> <span class="ss">f"X</span><span class="sc">{</span>mdi_indices[rank]<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb40-66"><a href="#cb40-66" aria-hidden="true" tabindex="-1"></a>    mdi_val <span class="op">=</span> mdi_importance[mdi_indices[rank]]</span>
<span id="cb40-67"><a href="#cb40-67" aria-hidden="true" tabindex="-1"></a>    perm_feat <span class="op">=</span> <span class="ss">f"X</span><span class="sc">{</span>perm_indices[rank]<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb40-68"><a href="#cb40-68" aria-hidden="true" tabindex="-1"></a>    perm_val <span class="op">=</span> perm_means[perm_indices[rank]]</span>
<span id="cb40-69"><a href="#cb40-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>rank<span class="op">+</span><span class="dv">1</span><span class="sc">:&lt;6}</span><span class="ss"> </span><span class="sc">{</span>mdi_feat<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>mdi_val<span class="sc">:&lt;15.4f}</span><span class="ss"> </span><span class="sc">{</span>perm_feat<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>perm_val<span class="sc">:&lt;15.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="rf-feature-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/rf-feature-importance-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Importancia de variables en Random Forest (MDI vs Permutation)</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Comparación de Rankings (Top 10):
======================================================================
Rank   MDI             MDI Value       Permutation     Perm Value     
----------------------------------------------------------------------
1      X9              0.1788          X9              0.1560         
2      X8              0.0653          X8              0.0193         
3      X1              0.0634          X12             0.0153         
4      X2              0.0559          X11             0.0147         
5      X10             0.0536          X10             0.0107         
6      X11             0.0480          X3              0.0107         
7      X6              0.0478          X1              0.0087         
8      X4              0.0477          X14             0.0067         
9      X5              0.0467          X15             0.0067         
10     X17             0.0447          X19             0.0067         </code></pre>
</div>
</div>
</section>
<section id="fronteras-de-decisión-visualización-2d" class="level3">
<h3 class="anchored" data-anchor-id="fronteras-de-decisión-visualización-2d">Fronteras de Decisión: Visualización 2D</h3>
<div id="cell-rf-decision-boundaries-2d" class="cell" data-fig-height="5" data-fig-width="15" data-execution_count="21">
<div class="cell-output cell-output-display">
<div id="rf-decision-boundaries-2d" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/rf-decision-boundaries-2d-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación de fronteras de decisión con diferentes métodos</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Observaciones:</strong></p>
<ol type="1">
<li><strong>Árbol individual</strong>: Fronteras muy irregulares, sobreajuste evidente</li>
<li><strong>Bagging</strong>: Fronteras más suaves pero aún correlacionadas</li>
<li><strong>Random Forest</strong>: Fronteras más suaves y generalizables</li>
</ol>
</section>
<section id="análisis-de-sesgo-varianza" class="level3">
<h3 class="anchored" data-anchor-id="análisis-de-sesgo-varianza">Análisis de Sesgo-Varianza</h3>
<div id="cell-rf-bias-variance-analysis" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="22">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Experimento: entrenar múltiples modelos en diferentes muestras bootstrap</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>n_experiments <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>n_test_points <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>X_test_sample <span class="op">=</span> X_test[:n_test_points]</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>y_test_sample <span class="op">=</span> y_test[:n_test_points]</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Almacenar predicciones</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>predictions_bagging <span class="op">=</span> []</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>predictions_rf <span class="op">=</span> []</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> exp <span class="kw">in</span> <span class="bu">range</span>(n_experiments):</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generar muestra bootstrap del training set</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X_train), size<span class="op">=</span><span class="bu">len</span>(X_train), replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    X_boot <span class="op">=</span> X_train[indices]</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    y_boot <span class="op">=</span> y_train[indices]</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bagging</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    bag <span class="op">=</span> BaggingClassifier(</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>        estimator<span class="op">=</span>DecisionTreeClassifier(),</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span>exp,</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>    bag.fit(X_boot, y_boot)</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>    pred_bag <span class="op">=</span> bag.predict_proba(X_test_sample)[:, <span class="dv">1</span>]</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>    predictions_bagging.append(pred_bag)</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random Forest</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>    rf <span class="op">=</span> RandomForestClassifier(</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>        max_features<span class="op">=</span><span class="st">'sqrt'</span>,</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span>exp,</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>    rf.fit(X_boot, y_boot)</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>    pred_rf <span class="op">=</span> rf.predict_proba(X_test_sample)[:, <span class="dv">1</span>]</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>    predictions_rf.append(pred_rf)</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>predictions_bagging <span class="op">=</span> np.array(predictions_bagging)</span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>predictions_rf <span class="op">=</span> np.array(predictions_rf)</span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular varianza por punto de test</span></span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a>variance_bagging <span class="op">=</span> np.var(predictions_bagging, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>variance_rf <span class="op">=</span> np.var(predictions_rf, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización</span></span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: Comparación de varianzas</span></span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(<span class="bu">range</span>(n_test_points), variance_bagging,</span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>               label<span class="op">=</span><span class="st">'Bagging'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">60</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(<span class="bu">range</span>(n_test_points), variance_rf,</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a>               label<span class="op">=</span><span class="st">'Random Forest'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">60</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axhline(y<span class="op">=</span>np.mean(variance_bagging), color<span class="op">=</span><span class="st">'blue'</span>,</span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a>               linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Media Bagging'</span>)</span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axhline(y<span class="op">=</span>np.mean(variance_rf), color<span class="op">=</span><span class="st">'red'</span>,</span>
<span id="cb42-58"><a href="#cb42-58" aria-hidden="true" tabindex="-1"></a>               linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Media RF'</span>)</span>
<span id="cb42-59"><a href="#cb42-59" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Punto de Test'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb42-60"><a href="#cb42-60" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Varianza de Predicciones'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb42-61"><a href="#cb42-61" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Varianza por Punto de Test'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb42-62"><a href="#cb42-62" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb42-63"><a href="#cb42-63" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb42-64"><a href="#cb42-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-65"><a href="#cb42-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Distribución de varianzas</span></span>
<span id="cb42-66"><a href="#cb42-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].hist(variance_bagging, bins<span class="op">=</span><span class="dv">15</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Bagging'</span>, color<span class="op">=</span><span class="st">'blue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb42-67"><a href="#cb42-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].hist(variance_rf, bins<span class="op">=</span><span class="dv">15</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Random Forest'</span>, color<span class="op">=</span><span class="st">'red'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb42-68"><a href="#cb42-68" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axvline(x<span class="op">=</span>np.mean(variance_bagging), color<span class="op">=</span><span class="st">'blue'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb42-69"><a href="#cb42-69" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axvline(x<span class="op">=</span>np.mean(variance_rf), color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb42-70"><a href="#cb42-70" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Varianza'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb42-71"><a href="#cb42-71" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Frecuencia'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb42-72"><a href="#cb42-72" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Distribución de Varianzas'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb42-73"><a href="#cb42-73" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb42-74"><a href="#cb42-74" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb42-75"><a href="#cb42-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-76"><a href="#cb42-76" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-77"><a href="#cb42-77" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-78"><a href="#cb42-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-79"><a href="#cb42-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Estadísticas</span></span>
<span id="cb42-80"><a href="#cb42-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Estadísticas de Varianza:"</span>)</span>
<span id="cb42-81"><a href="#cb42-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb42-82"><a href="#cb42-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Método'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Media'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Std'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Reducción vs Bagging'</span><span class="sc">:&lt;20}</span><span class="ss">"</span>)</span>
<span id="cb42-83"><a href="#cb42-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb42-84"><a href="#cb42-84" aria-hidden="true" tabindex="-1"></a>mean_var_bag <span class="op">=</span> np.mean(variance_bagging)</span>
<span id="cb42-85"><a href="#cb42-85" aria-hidden="true" tabindex="-1"></a>mean_var_rf <span class="op">=</span> np.mean(variance_rf)</span>
<span id="cb42-86"><a href="#cb42-86" aria-hidden="true" tabindex="-1"></a>reduction <span class="op">=</span> (mean_var_bag <span class="op">-</span> mean_var_rf) <span class="op">/</span> mean_var_bag <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb42-87"><a href="#cb42-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-88"><a href="#cb42-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Bagging'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>mean_var_bag<span class="sc">:&lt;15.6f}</span><span class="ss"> </span><span class="sc">{</span>np<span class="sc">.</span>std(variance_bagging)<span class="sc">:&lt;15.6f}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'-'</span><span class="sc">:&lt;20}</span><span class="ss">"</span>)</span>
<span id="cb42-89"><a href="#cb42-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Random Forest'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>mean_var_rf<span class="sc">:&lt;15.6f}</span><span class="ss"> </span><span class="sc">{</span>np<span class="sc">.</span>std(variance_rf)<span class="sc">:&lt;15.6f}</span><span class="ss"> </span><span class="sc">{</span>reduction<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="rf-bias-variance-analysis" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/rf-bias-variance-analysis-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Análisis de Sesgo-Varianza: Bagging vs Random Forest</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Estadísticas de Varianza:
======================================================================
Método               Media           Std             Reducción vs Bagging
----------------------------------------------------------------------
Bagging              0.015281        0.013155        -                   
Random Forest        0.007110        0.003668        53.47%</code></pre>
</div>
</div>
</section>
<section id="ventajas-y-desventajas-de-random-forest" class="level3">
<h3 class="anchored" data-anchor-id="ventajas-y-desventajas-de-random-forest">Ventajas y Desventajas de Random Forest</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ventajas de Random Forest
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Excelente desempeño out-of-the-box</strong>: Pocos hiperparámetros que ajustar</li>
<li><strong>Reducción de varianza superior a bagging</strong>: Gracias a la decorrelación</li>
<li><strong>Robusto al sobreajuste</strong>: Aumentar árboles no degrada test performance</li>
<li><strong>OOB error</strong>: Estimación gratuita de error de test</li>
<li><strong>Importancia de variables</strong>: Dos métodos complementarios (MDI y permutación)</li>
<li><strong>Paralelizable</strong>: Árboles se entrenan independientemente</li>
<li><strong>Maneja datos mixtos</strong>: Numéricas y categóricas sin preprocesamiento</li>
<li><strong>Robusto a outliers y ruido</strong>: Hereda esta propiedad de los árboles</li>
<li><strong>Pocas suposiciones</strong>: No asume distribuciones específicas de los datos</li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Desventajas de Random Forest
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Pérdida de interpretabilidad</strong>: No es un modelo simple de visualizar</li>
<li><strong>Costo computacional</strong>: Mayor que árboles individuales</li>
<li><strong>Predicción lenta</strong>: Debe consultar todos los árboles (puede optimizarse)</li>
<li><strong>Uso de memoria</strong>: Debe almacenar todos los árboles</li>
<li><strong>No extrapola</strong>: Solo interpola dentro del rango de los datos de entrenamiento</li>
<li><strong>Menos efectivo en datos muy de alta dimensión</strong>: Cuando p &gt;&gt; n</li>
<li><strong>Sesgo hacia variables con muchos valores</strong>: En importancia MDI</li>
</ol>
</div>
</div>
</section>
<section id="aplicación-práctica-dataset-real-1" class="level3">
<h3 class="anchored" data-anchor-id="aplicación-práctica-dataset-real-1">Aplicación Práctica: Dataset Real</h3>
<div id="rf-real-dataset" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV, cross_val_score</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Cargar datos</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>cancer <span class="op">=</span> load_breast_cancer()</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>X_cancer <span class="op">=</span> cancer.data</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>y_cancer <span class="op">=</span> cancer.target</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>X_train_c, X_test_c, y_train_c, y_test_c <span class="op">=</span> train_test_split(</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    X_cancer, y_cancer, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y_cancer</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"APLICACIÓN: Wisconsin Breast Cancer Dataset"</span>)</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Muestras: </span><span class="sc">{</span>X_cancer<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> | Características: </span><span class="sc">{</span>X_cancer<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Clases: </span><span class="sc">{</span>cancer<span class="sc">.</span>target_names<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Distribución: </span><span class="sc">{</span><span class="bu">dict</span>(<span class="bu">zip</span>(<span class="op">*</span>np.unique(y_cancer, return_counts<span class="op">=</span><span class="va">True</span>)))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Random Forest con parámetros default</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">1. RANDOM FOREST (parámetros default)"</span>)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>rf_default <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>rf_default.fit(X_train_c, y_train_c)</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>train_acc_default <span class="op">=</span> rf_default.score(X_train_c, y_train_c)</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>test_acc_default <span class="op">=</span> rf_default.score(X_test_c, y_test_c)</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train Accuracy: </span><span class="sc">{</span>train_acc_default<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Accuracy:  </span><span class="sc">{</span>test_acc_default<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Optimización de hiperparámetros</span></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">2. OPTIMIZACIÓN DE HIPERPARÁMETROS (Grid Search)"</span>)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>],</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'sqrt'</span>, <span class="st">'log2'</span>],</span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="va">None</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>]</span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>    RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>    param_grid,</span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'accuracy'</span>,</span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Buscando mejores hiperparámetros (esto puede tomar un momento)..."</span>)</span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train_c, y_train_c)</span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Mejores hiperparámetros encontrados:"</span>)</span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param, value <span class="kw">in</span> grid_search.best_params_.items():</span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>param<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Evaluar mejor modelo</span></span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">3. EVALUACIÓN DEL MEJOR MODELO"</span>)</span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a>best_rf <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a>train_acc_best <span class="op">=</span> best_rf.score(X_train_c, y_train_c)</span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a>test_acc_best <span class="op">=</span> best_rf.score(X_test_c, y_test_c)</span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train Accuracy: </span><span class="sc">{</span>train_acc_best<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Accuracy:  </span><span class="sc">{</span>test_acc_best<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Validación cruzada</span></span>
<span id="cb44-75"><a href="#cb44-75" aria-hidden="true" tabindex="-1"></a>cv_scores <span class="op">=</span> cross_val_score(best_rf, X_train_c, y_train_c, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb44-76"><a href="#cb44-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CV Accuracy:    </span><span class="sc">{</span>cv_scores<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss"> (±</span><span class="sc">{</span>cv_scores<span class="sc">.</span>std()<span class="sc">:.4f}</span><span class="ss">)"</span>)</span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-78"><a href="#cb44-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Reporte de clasificación</span></span>
<span id="cb44-79"><a href="#cb44-79" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_rf.predict(X_test_c)</span>
<span id="cb44-80"><a href="#cb44-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Reporte de Clasificación:"</span>)</span>
<span id="cb44-81"><a href="#cb44-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test_c, y_pred, target_names<span class="op">=</span>cancer.target_names))</span>
<span id="cb44-82"><a href="#cb44-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-83"><a href="#cb44-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Matriz de confusión</span></span>
<span id="cb44-84"><a href="#cb44-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matriz de Confusión:"</span>)</span>
<span id="cb44-85"><a href="#cb44-85" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test_c, y_pred)</span>
<span id="cb44-86"><a href="#cb44-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>APLICACIÓN: Wisconsin Breast Cancer Dataset
======================================================================
Muestras: 569 | Características: 30
Clases: ['malignant' 'benign']
Distribución: {np.int64(0): np.int64(212), np.int64(1): np.int64(357)}

1. RANDOM FOREST (parámetros default)
----------------------------------------------------------------------
Train Accuracy: 1.0000
Test Accuracy:  0.9357

2. OPTIMIZACIÓN DE HIPERPARÁMETROS (Grid Search)
----------------------------------------------------------------------
Buscando mejores hiperparámetros (esto puede tomar un momento)...

Mejores hiperparámetros encontrados:
  max_depth: None
  max_features: sqrt
  min_samples_leaf: 1
  min_samples_split: 2
  n_estimators: 100

3. EVALUACIÓN DEL MEJOR MODELO
----------------------------------------------------------------------
Train Accuracy: 1.0000
Test Accuracy:  0.9357
CV Accuracy:    0.9725 (±0.0330)

Reporte de Clasificación:
              precision    recall  f1-score   support

   malignant       0.92      0.91      0.91        64
      benign       0.94      0.95      0.95       107

    accuracy                           0.94       171
   macro avg       0.93      0.93      0.93       171
weighted avg       0.94      0.94      0.94       171

Matriz de Confusión:
[[ 58   6]
 [  5 102]]</code></pre>
</div>
</div>
<div id="cell-rf-cancer-feature-importance" class="cell" data-fig-height="6" data-fig-width="12" data-execution_count="24">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importancia de variables</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> best_rf.feature_importances_</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>][:<span class="dv">10</span>]</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 1: Importancia MDI</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].barh(<span class="bu">range</span>(<span class="dv">10</span>), importances[indices], color<span class="op">=</span><span class="st">'steelblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticks(<span class="bu">range</span>(<span class="dv">10</span>))</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticklabels([cancer.feature_names[i] <span class="cf">for</span> i <span class="kw">in</span> indices])</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].invert_yaxis()</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Importancia (MDI)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Top 10 Características (MDI)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, imp <span class="kw">in</span> <span class="bu">enumerate</span>(importances[indices]):</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(imp <span class="op">+</span> <span class="fl">0.002</span>, i, <span class="ss">f'</span><span class="sc">{</span>imp<span class="sc">:.3f}</span><span class="ss">'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel 2: Permutation Importance</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>perm_imp <span class="op">=</span> permutation_importance(</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    best_rf, X_test_c, y_test_c, n_repeats<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>perm_indices <span class="op">=</span> np.argsort(perm_imp.importances_mean)[::<span class="op">-</span><span class="dv">1</span>][:<span class="dv">10</span>]</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].barh(<span class="bu">range</span>(<span class="dv">10</span>), perm_imp.importances_mean[perm_indices],</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>            xerr<span class="op">=</span>perm_imp.importances_std[perm_indices],</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span><span class="st">'coral'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, capsize<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticks(<span class="bu">range</span>(<span class="dv">10</span>))</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticklabels([cancer.feature_names[i] <span class="cf">for</span> i <span class="kw">in</span> perm_indices])</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].invert_yaxis()</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Importancia (Permutación)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Top 10 Características (Permutación)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="rf-cancer-feature-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="05-arboles_files/figure-html/rf-cancer-feature-importance-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Top 10 características más importantes para diagnóstico de cáncer (Random Forest)</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="conclusiones-y-mejores-prácticas" class="level2">
<h2 class="anchored" data-anchor-id="conclusiones-y-mejores-prácticas">Conclusiones y Mejores Prácticas</h2>
<section id="recomendaciones-para-usar-árboles-de-decisión" class="level3">
<h3 class="anchored" data-anchor-id="recomendaciones-para-usar-árboles-de-decisión">Recomendaciones para Usar Árboles de Decisión</h3>
<ol type="1">
<li><p><strong>Comienza simple</strong>: Empieza con árboles poco profundos (max_depth=3-5)</p></li>
<li><p><strong>Usa validación cruzada</strong>: Para seleccionar hiperparámetros óptimos</p></li>
<li><p><strong>Considera la interpretabilidad</strong>: Si necesitas explicar decisiones, mantén árboles pequeños</p></li>
<li><p><strong>Combina con ensemble</strong>: Para producción, considera Random Forest o Gradient Boosting</p></li>
<li><p><strong>Analiza importancia de variables</strong>: Para entender qué características son relevantes</p></li>
<li><p><strong>Visualiza el árbol</strong>: Ayuda a detectar problemas y entender el modelo</p></li>
<li><p><strong>Compara con baselines</strong>: Árbol vs regresión logística en datos lineales</p></li>
</ol>
</section>
<section id="cuándo-usar-árboles-de-decisión" class="level3">
<h3 class="anchored" data-anchor-id="cuándo-usar-árboles-de-decisión">Cuándo Usar Árboles de Decisión</h3>
<p><strong>Usar árboles cuando:</strong> - Necesitas interpretabilidad - Tienes interacciones complejas entre variables - Variables numéricas y categóricas mezcladas - Outliers en los datos - Recursos computacionales limitados (árboles son rápidos)</p>
<p><strong>Evitar árboles individuales cuando:</strong> - Datos con relaciones predominantemente lineales - Necesitas el mejor desempeño predictivo (usar ensemble) - Tienes muy pocos datos (alta varianza) - Variables con muchas categorías (sesgo en selección)</p>
</section>
<section id="próximos-pasos-métodos-ensemble" class="level3">
<h3 class="anchored" data-anchor-id="próximos-pasos-métodos-ensemble">Próximos Pasos: Métodos Ensemble</h3>
<p>Los árboles individuales tienen limitaciones, pero combinándolos podemos crear modelos extremadamente poderosos:</p>
<ol type="1">
<li><strong>Bagging</strong>: Reduce varianza promediando múltiples árboles</li>
<li><strong>Random Forest</strong>: Bagging + aleatorización de características</li>
<li><strong>Gradient Boosting</strong>: Construye árboles secuencialmente para corregir errores</li>
<li><strong>XGBoost, LightGBM, CatBoost</strong>: Implementaciones optimizadas de boosting</li>
</ol>
<p>Estos métodos ensemble están entre los algoritmos más efectivos en machine learning y serán tema de capítulos futuros.</p>
<hr>
<p><strong>Referencias clave:</strong></p>
<ul>
<li>Breiman, L., Friedman, J., Stone, C. J., &amp; Olshen, R. A. (1984). <em>Classification and regression trees</em>. CRC press.</li>
<li>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements of statistical learning</em> (2nd ed.). Springer.</li>
<li>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). <em>An introduction to statistical learning</em>. Springer.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiado");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiado");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04-clasificacion.html" class="pagination-link" aria-label="Clasificación">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Clasificación</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="Referencias">
        <span class="nav-page-text">Referencias</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>