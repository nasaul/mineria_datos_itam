{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Árboles de Decisión\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Los **árboles de decisión** son uno de los métodos más intuitivos y ampliamente utilizados en el aprendizaje supervisado. A diferencia de los métodos lineales como la regresión logística, los árboles pueden capturar relaciones no lineales complejas e interacciones entre variables de forma natural, produciendo modelos que son fáciles de interpretar y visualizar.\n",
        "\n",
        "### Motivación: Limitaciones de los Métodos Lineales\n",
        "\n",
        "Consideremos un problema donde queremos predecir si un cliente comprará un producto basándonos en su edad y su ingreso. Los métodos lineales (como regresión logística) asumirían que existe una **frontera de decisión lineal**:\n",
        "\n",
        "$$\\beta_0 + \\beta_1 \\cdot \\text{edad} + \\beta_2 \\cdot \\text{ingreso} = 0$$\n",
        "\n",
        "Sin embargo, la realidad puede ser más compleja: tal vez los clientes jóvenes con ingresos altos compran, los clientes mayores con cualquier ingreso compran, pero los clientes jóvenes con ingresos bajos no compran. Esta regla no es lineal y involucra **interacciones** entre variables.\n",
        "\n",
        "Los árboles de decisión resuelven este problema al **particionar el espacio de características** en regiones rectangulares, donde cada región tiene su propia predicción.\n",
        "\n",
        "### Estructura de un Árbol de Decisión\n",
        "\n",
        "Un árbol de decisión es una estructura jerárquica compuesta por:\n",
        "\n",
        "1. **Nodo raíz (root node)**: Contiene todos los datos de entrenamiento\n",
        "2. **Nodos internos (internal nodes)**: Representan decisiones basadas en características\n",
        "3. **Ramas (branches)**: Representan el resultado de una decisión\n",
        "4. **Nodos hoja o terminales (leaf nodes)**: Contienen las predicciones finales\n",
        "\n",
        "Cada nodo interno realiza una **pregunta binaria** sobre una característica:\n",
        "\n",
        "- \"¿Edad ≤ 30?\"\n",
        "- \"¿Ingreso > $50,000?\"\n",
        "- \"¿Categoría = A o B?\"\n",
        "\n",
        "### Ejemplo Visual Simple\n",
        "\n",
        "```\n",
        "                    [Edad ≤ 30?]\n",
        "                    /           \\\n",
        "                  Sí             No\n",
        "                 /                 \\\n",
        "        [Ingreso ≤ 40K?]        Compra = Sí\n",
        "           /         \\\n",
        "         Sí          No\n",
        "        /             \\\n",
        "   Compra = No    Compra = Sí\n",
        "```\n",
        "\n",
        "Este árbol representa las siguientes reglas:\n",
        "\n",
        "- Si edad > 30 → Compra = Sí\n",
        "- Si edad ≤ 30 y ingreso > 40K → Compra = Sí\n",
        "- Si edad ≤ 30 y ingreso ≤ 40K → Compra = No\n",
        "\n",
        "## Construcción de Árboles de Decisión\n",
        "\n",
        "### Particionamiento Recursivo del Espacio\n",
        "\n",
        "Los árboles de decisión construyen su estructura mediante **particionamiento recursivo binario** (recursive binary splitting). Este proceso:\n",
        "\n",
        "1. Comienza con todos los datos en el nodo raíz\n",
        "2. Encuentra la mejor división (variable y punto de corte)\n",
        "3. Divide los datos en dos nodos hijos\n",
        "4. Repite el proceso recursivamente para cada nodo hijo\n",
        "5. Se detiene cuando se cumple un criterio de parada\n",
        "\n",
        "Matemáticamente, el espacio de características $\\mathbb{R}^p$ se divide en $M$ regiones disjuntas $R_1, R_2, ..., R_M$ tales que:\n",
        "\n",
        "$$\\bigcup_{m=1}^{M} R_m = \\mathbb{R}^p, \\quad R_i \\cap R_j = \\emptyset \\text{ para } i \\neq j$$\n",
        "\n",
        "Cada región $R_m$ es un **hiperrectángulo** paralelo a los ejes de coordenadas.\n",
        "\n",
        "### Criterios de Impureza\n",
        "\n",
        "Para decidir cómo dividir un nodo, necesitamos medir la **impureza** o **heterogeneidad** de un nodo. Un nodo es \"puro\" si contiene mayormente ejemplos de una sola clase.\n",
        "\n",
        "#### 1. Índice de Gini\n",
        "\n",
        "El **índice de Gini** mide la probabilidad de clasificar incorrectamente un elemento elegido aleatoriamente si se etiqueta aleatoriamente según la distribución de clases del nodo:\n",
        "\n",
        "$$I_G(t) = \\sum_{k=1}^{K} p_k(t) \\cdot (1 - p_k(t)) = \\sum_{k=1}^{K} p_k(t) - \\sum_{k=1}^{K} p_k(t)^2 = 1 - \\sum_{k=1}^{K} p_k(t)^2$$\n",
        "\n",
        "Donde:\n",
        "\n",
        "- $K$ es el número de clases\n",
        "- $p_k(t)$ es la proporción de ejemplos de la clase $k$ en el nodo $t$\n",
        "\n",
        "**Propiedades del índice de Gini:**\n",
        "\n",
        "- **Mínimo** ($I_G = 0$): Nodo puro (una sola clase)\n",
        "  - Ejemplo: Si $p_1 = 1, p_2 = 0$ → $I_G = 1 - (1^2 + 0^2) = 0$\n",
        "\n",
        "- **Máximo** (cuando las clases están balanceadas):\n",
        "  - Para 2 clases con $p_1 = p_2 = 0.5$ → $I_G = 1 - (0.5^2 + 0.5^2) = 0.5$\n",
        "  - Para $K$ clases con $p_k = 1/K$ → $I_G = 1 - K(1/K)^2 = (K-1)/K$\n",
        "\n",
        "#### 2. Entropía\n",
        "\n",
        "La **entropía** mide el desorden o incertidumbre en un nodo, basada en la teoría de la información:\n",
        "\n",
        "$$H(t) = -\\sum_{k=1}^{K} p_k(t) \\log_2(p_k(t))$$\n",
        "\n",
        "Por convención, $0 \\log(0) = 0$.\n",
        "\n",
        "**Propiedades de la entropía:**\n",
        "\n",
        "- **Mínimo** ($H = 0$): Nodo puro (certidumbre completa)\n",
        "- **Máximo** ($H = \\log_2(K)$): Clases uniformemente distribuidas (máxima incertidumbre)\n",
        "  - Para 2 clases: $H_{\\max} = 1$ bit\n",
        "  - Para 4 clases: $H_{\\max} = 2$ bits\n",
        "\n",
        "**Ganancia de Información (Information Gain):**\n",
        "\n",
        "La ganancia de información mide la reducción en entropía al realizar una división:\n",
        "\n",
        "$$IG = H(t_{\\text{padre}}) - \\sum_{i \\in \\{\\text{izq, der}\\}} \\frac{n_i}{n} H(t_i)$$\n",
        "\n",
        "Donde $n_i$ es el número de ejemplos en el nodo hijo $i$ y $n$ es el total en el nodo padre.\n",
        "\n",
        "#### 3. Error de Clasificación\n",
        "\n",
        "El **error de clasificación** es la tasa de ejemplos que no pertenecen a la clase mayoritaria:\n",
        "\n",
        "$$E(t) = 1 - \\max_k p_k(t)$$\n",
        "\n",
        "Este criterio es menos sensible a cambios en la distribución de clases y se usa menos en la práctica.\n",
        "\n",
        "### Comparación Visual de Criterios de Impureza"
      ],
      "id": "6efaf259"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 10,
        "fig-height": 6
      },
      "source": [
        "#| label: impurity-comparison\n",
        "#| fig-cap: Comparación de criterios de impureza para clasificación binaria\n",
        "#| echo: false\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Proporción de la clase 1 (p)\n",
        "p = np.linspace(0.001, 0.999, 1000)\n",
        "\n",
        "# Calcular diferentes medidas de impureza\n",
        "gini = 2 * p * (1 - p)  # Para clasificación binaria: 2p(1-p)\n",
        "entropy = -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
        "classification_error = 1 - np.maximum(p, 1 - p)\n",
        "\n",
        "# Normalizar entropía para comparación visual\n",
        "entropy_norm = entropy / entropy.max()\n",
        "\n",
        "# Visualización\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Panel 1: Las tres medidas\n",
        "axes[0].plot(p, gini, label='Índice de Gini', linewidth=2.5, color='blue')\n",
        "axes[0].plot(p, entropy_norm, label='Entropía (normalizada)', linewidth=2.5, color='red')\n",
        "axes[0].plot(p, classification_error, label='Error de Clasificación',\n",
        "             linewidth=2.5, color='green', linestyle='--')\n",
        "axes[0].set_xlabel('Proporción de clase 1 ($p$)', fontsize=12)\n",
        "axes[0].set_ylabel('Impureza (normalizada)', fontsize=12)\n",
        "axes[0].set_title('Criterios de Impureza para Clasificación Binaria', fontsize=13)\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axvline(x=0.5, color='black', linestyle=':', linewidth=1, alpha=0.5)\n",
        "axes[0].text(0.5, 0.05, 'Máxima\\nImpureza', ha='center', fontsize=9,\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# Panel 2: Gini vs Entropía (sin normalizar)\n",
        "ax2 = axes[1]\n",
        "ax2.plot(p, gini, label='Índice de Gini', linewidth=2.5, color='blue')\n",
        "ax2.set_xlabel('Proporción de clase 1 ($p$)', fontsize=12)\n",
        "ax2.set_ylabel('Índice de Gini', fontsize=12, color='blue')\n",
        "ax2.tick_params(axis='y', labelcolor='blue')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Eje secundario para entropía\n",
        "ax2_twin = ax2.twinx()\n",
        "ax2_twin.plot(p, entropy, label='Entropía', linewidth=2.5, color='red')\n",
        "ax2_twin.set_ylabel('Entropía (bits)', fontsize=12, color='red')\n",
        "ax2_twin.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "axes[1].set_title('Gini vs Entropía (escalas reales)', fontsize=13)\n",
        "\n",
        "# Leyenda combinada\n",
        "lines1, labels1 = ax2.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
        "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Imprimir valores en puntos clave\n",
        "print(\"Valores de impureza en puntos clave:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Proporción p':<15} {'Gini':<12} {'Entropía':<12} {'Error':<12}\")\n",
        "print(\"-\" * 60)\n",
        "for p_val in [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]:\n",
        "    gini_val = 2 * p_val * (1 - p_val)\n",
        "    if p_val == 0.0 or p_val == 1.0:\n",
        "        entropy_val = 0.0\n",
        "    else:\n",
        "        entropy_val = -p_val * np.log2(p_val) - (1 - p_val) * np.log2(1 - p_val)\n",
        "    error_val = 1 - max(p_val, 1 - p_val)\n",
        "    print(f\"{p_val:<15.1f} {gini_val:<12.4f} {entropy_val:<12.4f} {error_val:<12.4f}\")"
      ],
      "id": "impurity-comparison",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observaciones:**\n",
        "\n",
        "1. **Gini y Entropía** son muy similares en comportamiento y suelen dar resultados comparables\n",
        "2. **Error de clasificación** es menos sensible a cambios en las probabilidades\n",
        "3. En la práctica, Gini es más común por ser más eficiente computacionalmente\n",
        "4. Todas alcanzan su máximo cuando las clases están balanceadas ($p = 0.5$)\n",
        "\n",
        "### Algoritmo de Construcción CART\n",
        "\n",
        "El algoritmo **CART** (Classification And Regression Trees) es el método más común para construir árboles de decisión:\n",
        "\n",
        "**Algoritmo: Construcción Greedy de Árbol de Decisión**\n",
        "\n",
        "```\n",
        "función CONSTRUIR_ARBOL(datos, profundidad_actual, max_profundidad):\n",
        "    // Criterios de parada\n",
        "    si profundidad_actual >= max_profundidad O\n",
        "       nodo es puro O\n",
        "       número de muestras < min_muestras:\n",
        "        crear nodo hoja con predicción mayoritaria\n",
        "        retornar\n",
        "\n",
        "    // Encontrar mejor división\n",
        "    mejor_ganancia = -infinito\n",
        "\n",
        "    para cada característica j en {1, ..., p}:\n",
        "        para cada posible punto de corte c:\n",
        "            dividir datos en: {x_j ≤ c} y {x_j > c}\n",
        "            calcular impureza ponderada de los nodos hijos\n",
        "            calcular ganancia = impureza_padre - impureza_hijos\n",
        "\n",
        "            si ganancia > mejor_ganancia:\n",
        "                mejor_ganancia = ganancia\n",
        "                mejor_característica = j\n",
        "                mejor_corte = c\n",
        "\n",
        "    // Crear división\n",
        "    crear nodo interno con pregunta: \"x[mejor_característica] ≤ mejor_corte?\"\n",
        "    datos_izq = datos donde x[mejor_característica] ≤ mejor_corte\n",
        "    datos_der = datos donde x[mejor_característica] > mejor_corte\n",
        "\n",
        "    // Recursión\n",
        "    hijo_izquierdo = CONSTRUIR_ARBOL(datos_izq, profundidad_actual + 1, max_profundidad)\n",
        "    hijo_derecho = CONSTRUIR_ARBOL(datos_der, profundidad_actual + 1, max_profundidad)\n",
        "\n",
        "    retornar nodo_actual\n",
        "```\n",
        "\n",
        "**Características clave del algoritmo:**\n",
        "\n",
        "1. **Greedy (Voraz)**: En cada paso, elige la mejor división local sin considerar divisiones futuras\n",
        "2. **Top-down**: Construye desde la raíz hacia las hojas\n",
        "3. **Recursivo**: Aplica el mismo proceso a cada subárbol\n",
        "4. **Binario**: Cada división genera exactamente dos nodos hijos\n",
        "\n",
        "### Ejemplo: Construcción Paso a Paso"
      ],
      "id": "3ab60d94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tree-construction-example\n",
        "#| echo: true\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generar datos sintéticos simples (2D para visualización)\n",
        "np.random.seed(42)\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=2,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_clusters_per_class=1,\n",
        "    flip_y=0.1,\n",
        "    class_sep=1.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Crear DataFrame para mejor visualización\n",
        "df = pd.DataFrame(X, columns=['X1', 'X2'])\n",
        "df['Clase'] = y\n",
        "\n",
        "print(\"Datos de ejemplo:\")\n",
        "print(\"=\" * 60)\n",
        "print(df.head(10))\n",
        "print(f\"\\nTotal de muestras: {len(df)}\")\n",
        "print(f\"Clases: {df['Clase'].value_counts().to_dict()}\")"
      ],
      "id": "tree-construction-example",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 10
      },
      "source": [
        "#| label: tree-depths-comparison\n",
        "#| fig-cap: Comparación de árboles con diferentes profundidades\n",
        "#| echo: true\n",
        "\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Entrenar árboles con diferentes profundidades\n",
        "profundidades = [1, 2, 3, 5]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, depth in enumerate(profundidades):\n",
        "    # Entrenar árbol\n",
        "    tree = DecisionTreeClassifier(\n",
        "        max_depth=depth,\n",
        "        criterion='gini',\n",
        "        random_state=42\n",
        "    )\n",
        "    tree.fit(X, y)\n",
        "\n",
        "    # Visualizar árbol\n",
        "    plot_tree(\n",
        "        tree,\n",
        "        ax=axes[idx],\n",
        "        feature_names=['X1', 'X2'],\n",
        "        class_names=['Clase 0', 'Clase 1'],\n",
        "        filled=True,\n",
        "        rounded=True,\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "    # Calcular accuracy en entrenamiento\n",
        "    train_accuracy = tree.score(X, y)\n",
        "    axes[idx].set_title(\n",
        "        f'Profundidad = {depth} | Accuracy = {train_accuracy:.3f}',\n",
        "        fontsize=12,\n",
        "        pad=10\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mostrar información detallada del árbol más complejo\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INFORMACIÓN DEL ÁRBOL (Profundidad = 5)\")\n",
        "print(\"=\" * 60)\n",
        "tree_detailed = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "tree_detailed.fit(X, y)\n",
        "print(f\"Número de nodos: {tree_detailed.tree_.node_count}\")\n",
        "print(f\"Número de hojas: {tree_detailed.get_n_leaves()}\")\n",
        "print(f\"Profundidad real: {tree_detailed.get_depth()}\")\n",
        "print(f\"Accuracy en entrenamiento: {tree_detailed.score(X, y):.3f}\")"
      ],
      "id": "tree-depths-comparison",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 10
      },
      "source": [
        "#| label: decision-boundaries\n",
        "#| fig-cap: Fronteras de decisión para diferentes profundidades de árbol\n",
        "#| echo: false\n",
        "\n",
        "# Función para visualizar fronteras de decisión\n",
        "def plot_decision_boundary(clf, X, y, ax, title):\n",
        "    \"\"\"Grafica la frontera de decisión de un clasificador\"\"\"\n",
        "    h = 0.02  # Tamaño del paso en la malla\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu_r')\n",
        "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Clase 0',\n",
        "               edgecolors='black', s=50, alpha=0.7)\n",
        "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Clase 1',\n",
        "               edgecolors='black', s=50, alpha=0.7)\n",
        "    ax.set_xlabel('X1', fontsize=11)\n",
        "    ax.set_ylabel('X2', fontsize=11)\n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Crear visualizaciones\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, depth in enumerate(profundidades):\n",
        "    tree = DecisionTreeClassifier(max_depth=depth, criterion='gini', random_state=42)\n",
        "    tree.fit(X, y)\n",
        "    train_acc = tree.score(X, y)\n",
        "\n",
        "    plot_decision_boundary(\n",
        "        tree, X, y, axes[idx],\n",
        "        f'Profundidad = {depth} | Accuracy = {train_acc:.3f}'\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "decision-boundaries",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observaciones importantes:**\n",
        "\n",
        "1. **Profundidad = 1** (stump): Una sola división, frontera muy simple\n",
        "2. **Profundidad = 2-3**: Capturas las principales regiones de decisión\n",
        "3. **Profundidad = 5**: Frontera muy compleja, posible sobreajuste\n",
        "4. Las fronteras son siempre **paralelas a los ejes** (particiones rectangulares)\n",
        "\n",
        "## Sobreajuste y Control de Complejidad\n",
        "\n",
        "### El Problema del Sobreajuste\n",
        "\n",
        "Los árboles de decisión tienen una tendencia natural al **sobreajuste** (overfitting). Sin restricciones, un árbol puede crecer hasta que cada nodo hoja contenga un solo ejemplo, logrando 100% de accuracy en entrenamiento pero generalizando muy mal.\n",
        "\n",
        "**Causas del sobreajuste:**\n",
        "\n",
        "1. **Alta varianza**: Pequeños cambios en los datos pueden producir árboles muy diferentes\n",
        "2. **Falta de regularización inherente**: Sin restricciones, el árbol memoriza los datos\n",
        "3. **Captura de ruido**: El árbol aprende patrones específicos del conjunto de entrenamiento\n",
        "\n",
        "### Estrategias de Control de Complejidad\n",
        "\n",
        "#### 1. Pre-Poda (Pre-Pruning)\n",
        "\n",
        "La **pre-poda** detiene el crecimiento del árbol durante su construcción mediante criterios:\n",
        "\n",
        "**Hiperparámetros comunes:**\n",
        "\n",
        "- `max_depth`: Profundidad máxima del árbol\n",
        "  - Valores típicos: 3-10\n",
        "  - Menor → Más sesgo, menos varianza\n",
        "\n",
        "- `min_samples_split`: Mínimo de muestras para dividir un nodo\n",
        "  - Valores típicos: 2-20\n",
        "  - Mayor → Árbol más pequeño\n",
        "\n",
        "- `min_samples_leaf`: Mínimo de muestras en una hoja\n",
        "  - Valores típicos: 1-10\n",
        "  - Mayor → Hojas más confiables\n",
        "\n",
        "- `max_features`: Número máximo de características a considerar por división\n",
        "  - `'sqrt'`: √p características (usado en Random Forest)\n",
        "  - `'log2'`: log₂(p) características\n",
        "  - `None`: Todas las características\n",
        "\n",
        "- `max_leaf_nodes`: Número máximo de nodos hoja\n",
        "  - Controla directamente el tamaño del árbol\n",
        "\n",
        "#### 2. Post-Poda (Post-Pruning)\n",
        "\n",
        "La **post-poda** construye un árbol completo y luego lo reduce eliminando nodos que no aportan suficiente mejora.\n",
        "\n",
        "**Cost-Complexity Pruning (Poda por Costo-Complejidad):**\n",
        "\n",
        "Define una función de costo que balancea error y complejidad:\n",
        "\n",
        "$$C_\\alpha(T) = \\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} L(y_i, \\hat{y}_m) + \\alpha |T|$$\n",
        "\n",
        "Donde:\n",
        "\n",
        "- $|T|$ es el número de nodos hoja\n",
        "- $\\alpha \\geq 0$ es el parámetro de complejidad\n",
        "- $L$ es la función de pérdida\n",
        "- $\\hat{y}_m$ es la predicción en el nodo hoja $m$\n",
        "\n",
        "**Efecto de $\\alpha$:**\n",
        "\n",
        "- $\\alpha = 0$: Árbol completo (sin poda)\n",
        "- $\\alpha$ grande: Árbol muy pequeño (mayor regularización)"
      ],
      "id": "3d7850ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 5
      },
      "source": [
        "#| label: pruning-demonstration\n",
        "#| fig-cap: Efecto de la poda en el desempeño del árbol\n",
        "#| echo: true\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Dividir datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar árbol completo\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "\n",
        "# Obtener camino de cost-complexity pruning\n",
        "path = tree_full.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "impurities = path.impurities\n",
        "\n",
        "print(\"Cost-Complexity Pruning Path:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Número de valores de alpha: {len(ccp_alphas)}\")\n",
        "print(f\"Rango de alpha: [{ccp_alphas[0]:.6f}, {ccp_alphas[-1]:.6f}]\")\n",
        "\n",
        "# Entrenar árboles para diferentes valores de alpha\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "n_leaves = []\n",
        "depths = []\n",
        "\n",
        "for alpha in ccp_alphas:\n",
        "    tree = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    tree.fit(X_train, y_train)\n",
        "    train_scores.append(tree.score(X_train, y_train))\n",
        "    test_scores.append(tree.score(X_test, y_test))\n",
        "    n_leaves.append(tree.get_n_leaves())\n",
        "    depths.append(tree.get_depth())\n",
        "\n",
        "# Visualización\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Panel 1: Accuracy vs Alpha\n",
        "axes[0].plot(ccp_alphas, train_scores, label='Entrenamiento',\n",
        "             marker='o', linewidth=2)\n",
        "axes[0].plot(ccp_alphas, test_scores, label='Prueba',\n",
        "             marker='s', linewidth=2)\n",
        "axes[0].set_xlabel('Alpha (ccp_alpha)', fontsize=11)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[0].set_title('Accuracy vs Alpha', fontsize=12)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Encontrar mejor alpha\n",
        "best_idx = np.argmax(test_scores)\n",
        "best_alpha = ccp_alphas[best_idx]\n",
        "axes[0].axvline(x=best_alpha, color='red', linestyle='--',\n",
        "                label=f'Mejor α = {best_alpha:.4f}')\n",
        "\n",
        "# Panel 2: Número de hojas vs Alpha\n",
        "axes[1].plot(ccp_alphas, n_leaves, marker='o', linewidth=2, color='green')\n",
        "axes[1].set_xlabel('Alpha (ccp_alpha)', fontsize=11)\n",
        "axes[1].set_ylabel('Número de Hojas', fontsize=11)\n",
        "axes[1].set_title('Complejidad del Árbol vs Alpha', fontsize=12)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axvline(x=best_alpha, color='red', linestyle='--')\n",
        "\n",
        "# Panel 3: Profundidad vs Alpha\n",
        "axes[2].plot(ccp_alphas, depths, marker='o', linewidth=2, color='purple')\n",
        "axes[2].set_xlabel('Alpha (ccp_alpha)', fontsize=11)\n",
        "axes[2].set_ylabel('Profundidad del Árbol', fontsize=11)\n",
        "axes[2].set_title('Profundidad vs Alpha', fontsize=12)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].axvline(x=best_alpha, color='red', linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"COMPARACIÓN: Árbol sin poda vs Árbol podado\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nÁrbol sin poda (α = 0):\")\n",
        "print(f\"  Hojas: {n_leaves[0]}\")\n",
        "print(f\"  Profundidad: {depths[0]}\")\n",
        "print(f\"  Accuracy entrenamiento: {train_scores[0]:.3f}\")\n",
        "print(f\"  Accuracy prueba: {test_scores[0]:.3f}\")\n",
        "\n",
        "print(f\"\\nÁrbol podado óptimo (α = {best_alpha:.4f}):\")\n",
        "print(f\"  Hojas: {n_leaves[best_idx]}\")\n",
        "print(f\"  Profundidad: {depths[best_idx]}\")\n",
        "print(f\"  Accuracy entrenamiento: {train_scores[best_idx]:.3f}\")\n",
        "print(f\"  Accuracy prueba: {test_scores[best_idx]:.3f}\")"
      ],
      "id": "pruning-demonstration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretabilidad y Análisis\n",
        "\n",
        "### Importancia de Variables\n",
        "\n",
        "Una de las grandes ventajas de los árboles es que podemos medir la **importancia** de cada variable basándonos en cuánto reduce la impureza:\n",
        "\n",
        "$$\\text{Importancia}(X_j) = \\sum_{t: \\text{usa } X_j} \\frac{n_t}{n} \\cdot \\Delta I(t)$$\n",
        "\n",
        "Donde:\n",
        "- $n_t$ es el número de muestras en el nodo $t$\n",
        "- $n$ es el número total de muestras\n",
        "- $\\Delta I(t)$ es la reducción en impureza por la división en el nodo $t$"
      ],
      "id": "8c76a4c9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 12,
        "fig-height": 5
      },
      "source": [
        "#| label: feature-importance\n",
        "#| fig-cap: Importancia de variables en árbol de decisión\n",
        "#| echo: true\n",
        "\n",
        "# Entrenar árbol en dataset con más características\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generar datos con 10 características\n",
        "X_multi, y_multi = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=10,\n",
        "    n_informative=6,\n",
        "    n_redundant=2,\n",
        "    n_repeated=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Nombres de características\n",
        "feature_names = [f'X{i+1}' for i in range(10)]\n",
        "\n",
        "# Entrenar árbol\n",
        "tree_multi = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "tree_multi.fit(X_multi, y_multi)\n",
        "\n",
        "# Obtener importancias\n",
        "importances = tree_multi.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Visualización\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Panel 1: Gráfico de barras\n",
        "axes[0].barh(range(10), importances[indices], color='steelblue', alpha=0.7)\n",
        "axes[0].set_yticks(range(10))\n",
        "axes[0].set_yticklabels([feature_names[i] for i in indices])\n",
        "axes[0].set_xlabel('Importancia', fontsize=11)\n",
        "axes[0].set_title('Importancia de Variables (Reducción de Impureza)', fontsize=12)\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Añadir valores\n",
        "for i, (idx, imp) in enumerate(zip(indices, importances[indices])):\n",
        "    axes[0].text(imp + 0.005, i, f'{imp:.3f}', va='center', fontsize=9)\n",
        "\n",
        "# Panel 2: Importancia acumulada\n",
        "cumsum_importance = np.cumsum(importances[indices])\n",
        "axes[1].plot(range(1, 11), cumsum_importance, marker='o', linewidth=2.5,\n",
        "             markersize=8, color='darkgreen')\n",
        "axes[1].fill_between(range(1, 11), cumsum_importance, alpha=0.3, color='green')\n",
        "axes[1].axhline(y=0.8, color='red', linestyle='--', linewidth=1.5,\n",
        "                label='80% de importancia')\n",
        "axes[1].axhline(y=0.95, color='orange', linestyle='--', linewidth=1.5,\n",
        "                label='95% de importancia')\n",
        "axes[1].set_xlabel('Número de Variables', fontsize=11)\n",
        "axes[1].set_ylabel('Importancia Acumulada', fontsize=11)\n",
        "axes[1].set_title('Importancia Acumulada de Variables', fontsize=12)\n",
        "axes[1].set_xticks(range(1, 11))\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Imprimir tabla de importancias\n",
        "print(\"\\nTabla de Importancias:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Variable':<12} {'Importancia':<15} {'Importancia Acum.':<20}\")\n",
        "print(\"-\" * 60)\n",
        "cumsum = 0\n",
        "for idx in indices:\n",
        "    cumsum += importances[idx]\n",
        "    print(f\"{feature_names[idx]:<12} {importances[idx]:<15.4f} {cumsum:<20.4f}\")"
      ],
      "id": "feature-importance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracción de Reglas\n",
        "\n",
        "Los árboles pueden convertirse en reglas IF-THEN interpretables:"
      ],
      "id": "11b106c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tree-rules\n",
        "#| echo: true\n",
        "\n",
        "from sklearn.tree import export_text\n",
        "\n",
        "# Entrenar árbol simple para mejor interpretabilidad\n",
        "tree_simple = DecisionTreeClassifier(max_depth=3, min_samples_leaf=10, random_state=42)\n",
        "tree_simple.fit(X[:, :2], y)\n",
        "\n",
        "# Exportar reglas como texto\n",
        "tree_rules = export_text(tree_simple, feature_names=['X1', 'X2'])\n",
        "\n",
        "print(\"REGLAS DE DECISIÓN DEL ÁRBOL:\")\n",
        "print(\"=\" * 60)\n",
        "print(tree_rules)\n",
        "\n",
        "# Función para extraer rutas de decisión\n",
        "def get_decision_path(tree, feature_names, sample):\n",
        "    \"\"\"Extrae la ruta de decisión para una muestra\"\"\"\n",
        "    node = 0\n",
        "    path = []\n",
        "\n",
        "    while tree.tree_.feature[node] != -2:  # -2 indica nodo hoja\n",
        "        feature_idx = tree.tree_.feature[node]\n",
        "        threshold = tree.tree_.threshold[node]\n",
        "\n",
        "        if sample[feature_idx] <= threshold:\n",
        "            direction = \"<=\"\n",
        "            node = tree.tree_.children_left[node]\n",
        "        else:\n",
        "            direction = \">\"\n",
        "            node = tree.tree_.children_right[node]\n",
        "\n",
        "        path.append(f\"{feature_names[feature_idx]} {direction} {threshold:.3f}\")\n",
        "\n",
        "    # Obtener predicción\n",
        "    class_probs = tree.tree_.value[node][0]\n",
        "    predicted_class = np.argmax(class_probs)\n",
        "\n",
        "    return path, predicted_class, class_probs\n",
        "\n",
        "# Ejemplo: explicar predicción para algunas muestras\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXPLICACIÓN DE PREDICCIONES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i in range(3):\n",
        "    sample = X[i, :2]\n",
        "    path, pred_class, probs = get_decision_path(tree_simple, ['X1', 'X2'], sample)\n",
        "\n",
        "    print(f\"\\nMuestra {i+1}: X1={sample[0]:.3f}, X2={sample[1]:.3f}\")\n",
        "    print(f\"Clase real: {y[i]}\")\n",
        "    print(f\"Predicción: {pred_class}\")\n",
        "    print(f\"Probabilidades: Clase 0 = {probs[0]:.3f}, Clase 1 = {probs[1]:.3f}\")\n",
        "    print(\"Ruta de decisión:\")\n",
        "    for step in path:\n",
        "        print(f\"  → {step}\")"
      ],
      "id": "tree-rules",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ventajas y Desventajas\n",
        "\n",
        "### Ventajas de los Árboles de Decisión\n",
        "\n",
        "1. **Interpretabilidad**: Fáciles de entender y explicar, incluso para no expertos\n",
        "   - Se pueden visualizar completamente\n",
        "   - Generan reglas IF-THEN interpretables\n",
        "\n",
        "2. **Manejo de variables mixtas**: Pueden manejar características numéricas y categóricas sin preprocesamiento\n",
        "\n",
        "3. **No requieren normalización**: Las decisiones son invariantes a transformaciones monótonas\n",
        "\n",
        "4. **Capturan interacciones automáticamente**: Detectan interacciones sin especificarlas explícitamente\n",
        "\n",
        "5. **Robustos a outliers**: Las divisiones son basadas en rankings, no en valores absolutos\n",
        "\n",
        "6. **Selección implícita de características**: Variables irrelevantes no se usan en las divisiones\n",
        "\n",
        "### Desventajas de los Árboles de Decisión\n",
        "\n",
        "1. **Alta varianza**: Pequeños cambios en datos → árboles muy diferentes\n",
        "   - Solución: Métodos ensemble (Random Forest, Gradient Boosting)\n",
        "\n",
        "2. **Dificultad con relaciones lineales**: Necesitan muchas divisiones para aproximar funciones lineales\n",
        "\n",
        "3. **Fronteras de decisión restrictivas**: Solo particiones rectangulares paralelas a los ejes\n",
        "\n",
        "4. **Sesgo hacia variables con muchos valores**: Tienden a seleccionar variables con más opciones de corte\n",
        "\n",
        "5. **Inestabilidad**: Pequeñas variaciones pueden cambiar completamente la estructura\n",
        "\n",
        "6. **Sobreajuste natural**: Sin restricciones, memorizan los datos de entrenamiento\n",
        "\n",
        "### Comparación Visual: Árbol vs Regresión Logística"
      ],
      "id": "121b73ac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 5
      },
      "source": [
        "#| label: tree-vs-logistic\n",
        "#| fig-cap: 'Comparación de fronteras de decisión: Árbol vs Regresión Logística'\n",
        "#| echo: false\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Generar datos con relación más lineal\n",
        "np.random.seed(123)\n",
        "X_linear = np.random.randn(200, 2)\n",
        "y_linear = (X_linear[:, 0] + X_linear[:, 1] > 0).astype(int)\n",
        "\n",
        "# Añadir algo de ruido\n",
        "noise_idx = np.random.choice(200, 20, replace=False)\n",
        "y_linear[noise_idx] = 1 - y_linear[noise_idx]\n",
        "\n",
        "# Entrenar modelos\n",
        "tree_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "logistic_model = LogisticRegression()\n",
        "\n",
        "tree_model.fit(X_linear, y_linear)\n",
        "logistic_model.fit(X_linear, y_linear)\n",
        "\n",
        "# Función auxiliar para graficar\n",
        "def plot_comparison(X, y, model1, model2, name1, name2):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    for ax, model, name in zip(axes, [model1, model2], [name1, name2]):\n",
        "        h = 0.02\n",
        "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                             np.arange(y_min, y_max, h))\n",
        "\n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu_r')\n",
        "        ax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Clase 0',\n",
        "                   edgecolors='black', s=50, alpha=0.7)\n",
        "        ax.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Clase 1',\n",
        "                   edgecolors='black', s=50, alpha=0.7)\n",
        "\n",
        "        acc = model.score(X, y)\n",
        "        ax.set_xlabel('X1', fontsize=11)\n",
        "        ax.set_ylabel('X2', fontsize=11)\n",
        "        ax.set_title(f'{name} | Accuracy = {acc:.3f}', fontsize=12)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "fig = plot_comparison(X_linear, y_linear, tree_model, logistic_model,\n",
        "                      'Árbol de Decisión', 'Regresión Logística')\n",
        "plt.show()\n",
        "\n",
        "print(\"Observaciones:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"- Regresión logística captura mejor la relación lineal subyacente\")\n",
        "print(\"- Árbol de decisión crea fronteras rectangulares que aproximan la línea\")\n",
        "print(\"- Para relaciones lineales, la regresión logística es más eficiente\")\n",
        "print(\"- Para relaciones no lineales, los árboles son más flexibles\")"
      ],
      "id": "tree-vs-logistic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aplicación Práctica: Dataset Real"
      ],
      "id": "211a3d7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: real-dataset-application\n",
        "#| echo: true\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"DATASET: Wisconsin Breast Cancer\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Número de muestras: {X_cancer.shape[0]}\")\n",
        "print(f\"Número de características: {X_cancer.shape[1]}\")\n",
        "print(f\"Clases: {cancer.target_names}\")\n",
        "print(f\"Distribución: {np.bincount(y_cancer)}\")\n",
        "\n",
        "# Dividir datos\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "# 1. Árbol sin regularización\n",
        "tree_unreg = DecisionTreeClassifier(random_state=42)\n",
        "tree_unreg.fit(X_train_c, y_train_c)\n",
        "\n",
        "print(\"\\n1. ÁRBOL SIN REGULARIZACIÓN\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Profundidad: {tree_unreg.get_depth()}\")\n",
        "print(f\"Número de hojas: {tree_unreg.get_n_leaves()}\")\n",
        "print(f\"Accuracy entrenamiento: {tree_unreg.score(X_train_c, y_train_c):.3f}\")\n",
        "print(f\"Accuracy prueba: {tree_unreg.score(X_test_c, y_test_c):.3f}\")\n",
        "\n",
        "# 2. Búsqueda de hiperparámetros óptimos\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "print(\"\\n2. BÚSQUEDA DE HIPERPARÁMETROS (Grid Search)\")\n",
        "print(\"-\" * 60)\n",
        "print(\"Evaluando combinaciones de hiperparámetros con CV...\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train_c, y_train_c)\n",
        "\n",
        "print(f\"Mejor combinación de parámetros:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# 3. Evaluar mejor modelo\n",
        "best_tree = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\n3. MEJOR ÁRBOL (después de optimización)\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Profundidad: {best_tree.get_depth()}\")\n",
        "print(f\"Número de hojas: {best_tree.get_n_leaves()}\")\n",
        "print(f\"Accuracy entrenamiento: {best_tree.score(X_train_c, y_train_c):.3f}\")\n",
        "print(f\"Accuracy prueba: {best_tree.score(X_test_c, y_test_c):.3f}\")\n",
        "\n",
        "# 4. Validación cruzada\n",
        "cv_scores = cross_val_score(best_tree, X_train_c, y_train_c, cv=5)\n",
        "print(f\"\\nValidación cruzada (5-fold):\")\n",
        "print(f\"  Scores: {cv_scores}\")\n",
        "print(f\"  Media: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")"
      ],
      "id": "real-dataset-application",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 10,
        "fig-height": 6
      },
      "source": [
        "#| label: cancer-feature-importance\n",
        "#| fig-cap: Top 10 características más importantes para clasificar cáncer de mama\n",
        "#| echo: true\n",
        "\n",
        "# Importancia de características\n",
        "importances_cancer = best_tree.feature_importances_\n",
        "indices_cancer = np.argsort(importances_cancer)[::-1][:10]  # Top 10\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(10), importances_cancer[indices_cancer], color='coral', alpha=0.7)\n",
        "plt.yticks(range(10), [cancer.feature_names[i] for i in indices_cancer])\n",
        "plt.xlabel('Importancia (Reducción de Impureza)', fontsize=12)\n",
        "plt.title('Top 10 Características Más Importantes', fontsize=13)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Añadir valores\n",
        "for i, imp in enumerate(importances_cancer[indices_cancer]):\n",
        "    plt.text(imp + 0.005, i, f'{imp:.3f}', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cancer-feature-importance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones y Mejores Prácticas\n",
        "\n",
        "### Recomendaciones para Usar Árboles de Decisión\n",
        "\n",
        "1. **Comienza simple**: Empieza con árboles poco profundos (max_depth=3-5)\n",
        "\n",
        "2. **Usa validación cruzada**: Para seleccionar hiperparámetros óptimos\n",
        "\n",
        "3. **Considera la interpretabilidad**: Si necesitas explicar decisiones, mantén árboles pequeños\n",
        "\n",
        "4. **Combina con ensemble**: Para producción, considera Random Forest o Gradient Boosting\n",
        "\n",
        "5. **Analiza importancia de variables**: Para entender qué características son relevantes\n",
        "\n",
        "6. **Visualiza el árbol**: Ayuda a detectar problemas y entender el modelo\n",
        "\n",
        "7. **Compara con baselines**: Árbol vs regresión logística en datos lineales\n",
        "\n",
        "### Cuándo Usar Árboles de Decisión\n",
        "\n",
        "**Usar árboles cuando:**\n",
        "- Necesitas interpretabilidad\n",
        "- Tienes interacciones complejas entre variables\n",
        "- Variables numéricas y categóricas mezcladas\n",
        "- Outliers en los datos\n",
        "- Recursos computacionales limitados (árboles son rápidos)\n",
        "\n",
        "**Evitar árboles individuales cuando:**\n",
        "- Datos con relaciones predominantemente lineales\n",
        "- Necesitas el mejor desempeño predictivo (usar ensemble)\n",
        "- Tienes muy pocos datos (alta varianza)\n",
        "- Variables con muchas categorías (sesgo en selección)\n",
        "\n",
        "### Próximos Pasos: Métodos Ensemble\n",
        "\n",
        "Los árboles individuales tienen limitaciones, pero combinándolos podemos crear modelos extremadamente poderosos:\n",
        "\n",
        "1. **Bagging**: Reduce varianza promediando múltiples árboles\n",
        "2. **Random Forest**: Bagging + aleatorización de características\n",
        "3. **Gradient Boosting**: Construye árboles secuencialmente para corregir errores\n",
        "4. **XGBoost, LightGBM, CatBoost**: Implementaciones optimizadas de boosting\n",
        "\n",
        "Estos métodos ensemble están entre los algoritmos más efectivos en machine learning y serán tema de capítulos futuros.\n",
        "\n",
        "---\n",
        "\n",
        "**Referencias clave:**\n",
        "\n",
        "- Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). *Classification and regression trees*. CRC press.\n",
        "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning* (2nd ed.). Springer.\n",
        "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). *An introduction to statistical learning*. Springer."
      ],
      "id": "a9f8eb21"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/xwing/miniforge3/envs/mineria_datos/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}