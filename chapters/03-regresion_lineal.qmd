# Regresi칩n lineal

## Regresi칩n Lineal Simple

Comenzaremos con el caso m치s sencillo: predecir una variable de resultado `Y` a partir de una 칰nica variable predictora `X`.

El modelo matem치tico que queremos ajustar es una l칤nea recta:

$$Y = \beta_0 + \beta_1 X + \epsilon$$

Donde:

- **$Y$**: La variable dependiente (lo que queremos predecir).
- **$X$**: La variable independiente (nuestro predictor).
- **$\beta_0$**: El intercepto (el valor de $Y$ cuando $X=0$).
- **$\beta_1$**: La pendiente (cu치nto cambia $Y$ por cada unidad que aumenta $X$).
- **$\epsilon$**: El t칠rmino de error (la parte de $Y$ que nuestro modelo no puede explicar).

Nuestro objetivo 游꿢 es encontrar los **mejores valores posibles** para los coeficientes $\beta_0$ y $\beta_1$ usando los datos que tenemos.



### 쮺칩mo estimamos los coeficientes $\beta_0$ y $\beta_1$?

"Mejor" para nosotros significa encontrar la l칤nea que minimice la distancia vertical entre cada punto de dato y la propia l칤nea. Espec칤ficamente, minimizamos la **Suma de los Errores al Cuadrado** (SEC o *Sum of Squared Errors*, SSE).

La funci칩n de costo (o p칠rdida) que queremos minimizar es:

$$J(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2$$

Tenemos dos m칠todos principales para encontrar los $\beta$ que minimizan esta funci칩n:

#### M칠todo 1: Las Ecuaciones Normales (La soluci칩n anal칤tica 游)

Este m칠todo utiliza c치lculo para encontrar el m칤nimo exacto de la funci칩n de costo. Para ello, tomamos las derivadas parciales de $J$ con respecto a $\beta_0$ y $\beta_1$, las igualamos a cero y resolvemos para los coeficientes.

::: {.callout-note collapse="true"}
## **Derivada parcial con respecto a $\beta_0$**:
$$\frac{\partial J}{\partial \beta_0} = \sum_{i=1}^{n} -2(y_i - \beta_0 - \beta_1 x_i) = 0$$
$$\sum y_i - n\beta_0 - \beta_1 \sum x_i = 0$$
$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$
::: 

::: {.callout-note collapse="true"}
## **Derivada parcial con respecto a $\beta_1$**
$$\frac{\partial J}{\partial \beta_1} = \sum_{i=1}^{n} -2x_i(y_i - \beta_0 - \beta_1 x_i) = 0$$
Sustituyendo $\beta_0$ de la primera ecuaci칩n y resolviendo, llegamos a:
$$\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$
::: 

Estas f칩rmulas nos dan los valores 칩ptimos y exactos de los coeficientes directamente a partir de los datos.

#### M칠todo 2: Descenso en Gradiente (La soluci칩n iterativa 丘뙖잺)

Este es un m칠todo computacional que nos "acerca" progresivamente a la soluci칩n. Es especialmente 칰til cuando tenemos una cantidad masiva de datos y calcular la soluci칩n anal칤tica es muy costoso.

**La intuici칩n:** Imagina que est치s en una monta침a (la funci칩n de costo) y quieres llegar al valle (el costo m칤nimo). El Descenso en Gradiente te dice que mires a tu alrededor y des un paso en la direcci칩n m치s inclinada hacia abajo. Repites esto hasta llegar al fondo.



El algoritmo funciona as칤:

1.  **Inicializa** los coeficientes $\beta_0$ y $\beta_1$ con valores aleatorios (o en ceros).
2.  **Calcula el gradiente** de la funci칩n de costo. El gradiente es un vector que apunta en la direcci칩n del m치ximo ascenso. Nosotros iremos en la direcci칩n opuesta.
    -   $\frac{\partial J}{\partial \beta_0} = -2 \sum (y_i - (\beta_0 + \beta_1 x_i))$
    -   $\frac{\partial J}{\partial \beta_1} = -2 \sum x_i(y_i - (\beta_0 + \beta_1 x_i))$
3.  **Actualiza** los coeficientes usando una **tasa de aprendizaje** ($\alpha$), que controla el tama침o del paso que damos.
    -   $\beta_0 := \beta_0 - \alpha \frac{\partial J}{\partial \beta_0}$
    -   $\beta_1 := \beta_1 - \alpha \frac{\partial J}{\partial \beta_1}$
4.  **Repite** los pasos 2 y 3 durante un n칰mero determinado de iteraciones o hasta que el cambio en el costo sea muy peque침o (convergencia).

::: {.callout-note collapse="true"}
#### Explicacion visual
![](imgs/gradient_descent.gif)
:::

## 쮺u치les son los supuestos de la regresi칩n? 游븷

Para que nuestro modelo sea confiable (es decir, para que los coeficientes y las predicciones tengan sentido), debemos cumplir con ciertos supuestos.

1.  **Linealidad:** La relaci칩n entre $\beta$ y $Y$ debe ser lineal.
    -   **쯇ara qu칠 sirve?** Si la relaci칩n no es lineal, nuestro modelo de l칤nea recta ser치 intr칤nsecamente incorrecto.

2.  **Independencia de los errores:** Los errores (residuos) no deben estar correlacionados entre s칤.
    -   **쯇ara qu칠 sirve?** Es crucial para datos de series temporales. Si los errores est치n correlacionados, la informaci칩n de un error nos da pistas sobre el siguiente, lo cual viola la idea de que cada observaci칩n es independiente.

3.  **Homocedasticidad (Varianza constante de los errores):** La varianza de los errores debe ser constante para todos los niveles de $X$.
    -   **쯇ara qu칠 sirve?** Si la varianza cambia (heterocedasticidad), nuestras predicciones ser치n mejores para algunas partes de los datos que para otras, y los intervalos de confianza para los coeficientes ser치n poco fiables. Visualmente, en un gr치fico de residuos vs. valores predichos, no queremos ver una forma de cono o embudo.

4.  **Normalidad de los errores:** Los errores deben seguir una distribuci칩n normal con media cero.
    -   **쯇ara qu칠 sirve?** Este supuesto es fundamental para poder realizar pruebas de hip칩tesis sobre los coeficientes (como los p-values) y construir intervalos de confianza. Podemos verificarlo con un histograma de los residuos o un gr치fico Q-Q.

---

## 쮺칩mo evaluar la precisi칩n del modelo? 游늳

Una vez que hemos ajustado el modelo, 쯖칩mo sabemos si es bueno?

### Coeficiente de Determinaci칩n ($R^2$)

El **$R^2$** mide la proporci칩n de la varianza total en la variable dependiente ($Y$) que es explicada por nuestro modelo.

$$R^2 = 1 - \frac{\text{Suma de Errores al Cuadrado (SEC)}}{\text{Suma Total de Cuadrados (STC)}} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$$

-   $R^2$ var칤a entre 0 y 1 (o 0% y 100%).
-   Un $R^2$ de 0.85 significa que el 85% de la variabilidad en $Y$ puede ser explicada por $X$.
-   Un $R^2$ m치s alto generalmente indica un mejor ajuste del modelo.

### p-values (Valores p)

El **p-value** nos ayuda a determinar si nuestra variable predictora $X$ es **estad칤sticamente significativa**. Responde a la pregunta: 쮼s probable que la relaci칩n que observamos entre $X$ y $Y$ haya ocurrido por puro azar?

-   **Hip칩tesis Nula ($H_0$):** No hay relaci칩n entre $X$ y $Y$ (es decir, $\beta_1 = 0$).
-   **Hip칩tesis Alternativa ($H_a$):** S칤 hay una relaci칩n entre $X$ y $Y$ (es decir, $\beta_1 \neq 0$).

Un **p-value peque침o** (t칤picamente < 0.05) nos da evidencia para rechazar la hip칩tesis nula. Esto sugiere que nuestra variable $X$ es un predictor 칰til para $Y$.

## M칠tricas de Error de Predicci칩n

Adem치s del $R^2$, existen m칰ltiples m칠tricas para evaluar qu칠 tan bien predice nuestro modelo. Cada una tiene sus ventajas y casos de uso espec칤ficos:

### Error Cuadr치tico Medio (MSE)

El **MSE** mide el promedio de los errores al cuadrado:

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

- **Ventajas:** Penaliza fuertemente errores grandes, diferenciable (칰til para optimizaci칩n)
- **Desventajas:** Sensible a valores at칤picos, dif칤cil de interpretar (unidades al cuadrado)
- **Cu치ndo usar:** Cuando errores grandes son especialmente costosos

### Ra칤z del Error Cuadr치tico Medio (RMSE)

El **RMSE** es la ra칤z cuadrada del MSE:

$$RMSE = \sqrt{MSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

- **Ventajas:** Mismas unidades que la variable objetivo, interpretable
- **Desventajas:** A칰n sensible a valores at칤picos
- **Interpretaci칩n:** "En promedio, nuestras predicciones se desv칤an X unidades del valor real"

### Error Absoluto Medio (MAE)

El **MAE** mide el promedio de los errores absolutos:

$$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

- **Ventajas:** Robusto a valores at칤picos, f치cil de interpretar
- **Desventajas:** No diferenciable en cero, trata todos los errores por igual
- **Cu치ndo usar:** Cuando hay valores at칤picos o todos los errores tienen igual importancia

### Error Porcentual Absoluto Medio (MAPE)

El **MAPE** expresa el error como porcentaje del valor real:

$$MAPE = \frac{100}{n} \sum_{i=1}^{n} \left|\frac{y_i - \hat{y}_i}{y_i}\right|$$

- **Ventajas:** Interpretable (% de error), adimensional, 칰til para comparar modelos en diferentes escalas
- **Desventajas:** Indefinido cuando $y_i = 0$, asim칠trico (penaliza m치s las sobreestimaciones)
- **Interpretaci칩n:** "Nuestras predicciones se desv칤an en promedio X% del valor real"
- **Cu치ndo usar:** Para comparar precisi칩n entre diferentes productos, regiones, o escalas

### Error Porcentual Absoluto Medio Sim칠trico (SMAPE)

El **SMAPE** es una versi칩n sim칠trica del MAPE:

$$SMAPE = \frac{100}{n} \sum_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{(|y_i| + |\hat{y}_i|)/2}$$

- **Ventajas:** Sim칠trico, acotado entre 0% y 200%
- **Desventajas:** Puede ser contraintuitivo, no tan est치ndar como MAPE
- **Cu치ndo usar:** Cuando queremos evitar el sesgo del MAPE hacia sobreestimaciones

### Error Logar칤tmico Cuadr치tico Medio (MSLE)

El **MSLE** usa transformaci칩n logar칤tmica:

$$MSLE = \frac{1}{n} \sum_{i=1}^{n} (\log(1 + y_i) - \log(1 + \hat{y}_i))^2$$

- **Ventajas:** Penaliza m치s las subestimaciones que las sobreestimaciones
- **Desventajas:** Solo para valores positivos, menos interpretable
- **Cu치ndo usar:** Cuando subestimar es m치s costoso que sobreestimar (ej: demanda de inventario)

### $R^2$ Ajustado

El **$R^2$ ajustado** penaliza por el n칰mero de variables en el modelo:

$$R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}$$

Donde $p$ es el n칰mero de predictores.

- **Ventajas:** No aumenta autom치ticamente al a침adir variables
- **Cu치ndo usar:** Para comparar modelos con diferente n칰mero de variables
- **Interpretaci칩n:** Similar a $R^2$ pero m치s conservador

#### 쮺u치l m칠trica elegir?

La elecci칩n de m칠trica depende del contexto del problema:

| **M칠trica** | **Mejor para** | **Evitar cuando** |
|-------------|----------------|-------------------|
| **RMSE** | Errores grandes son costosos | Hay muchos valores at칤picos |
| **MAE** | Errores tienen igual importancia | Necesitas diferenciabilidad |
| **MAPE** | Comparar diferentes escalas | Hay valores cercanos a cero |
| **SMAPE** | Comparar con simetr칤a | Interpretaci칩n debe ser simple |
| **R** | Explicar variabilidad | Solo importa precisi칩n de predicci칩n |

::: {.callout-tip}
## **Recomendaci칩n pr치ctica**
Usa **m칰ltiples m칠tricas** para evaluar tu modelo. Una combinaci칩n t칤pica ser칤a:
- **RMSE** para precisi칩n general
- **MAPE** para interpretabilidad de negocio  
- **R** para explicaci칩n de variabilidad
:::

---

## Regresi칩n Lineal M칰ltiple

Ahora, 쯤u칠 pasa si tenemos m칰ltiples predictores ($X_1, X_2, ..., X_p$)? El modelo se expande:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon$$

La intuici칩n es la misma, pero en lugar de ajustar una l칤nea, estamos ajustando un **hiperplano** en un espacio multidimensional.

Para manejar esto de forma elegante, usamos notaci칩n matricial:

$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$

Donde:
- $\mathbf{y}$ es el vector de observaciones.
- $\mathbf{X}$ es la matriz de dise침o (con una primera columna de unos para el intercepto).
- $\boldsymbol{\beta}$ es el vector de coeficientes.
- $\boldsymbol{\epsilon}$ es el vector de errores.

La funci칩n de costo en forma matricial es:
$$J(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$$

---

## Transformaciones Comunes en Modelos Lineales

A veces, la relaci칩n entre X e Y no es estrictamente lineal. Las transformaciones logar칤tmicas nos permiten modelar relaciones no lineales y, adem치s, ofrecen interpretaciones muy 칰tiles en t칠rminos de cambios porcentuales.

### Modelo Log-Nivel (Transformaci칩n en Y)

Este modelo se usa cuando el efecto de X sobre Y no es absoluto, sino porcentual. Por ejemplo, c칩mo un a침o m치s de educaci칩n afecta el *porcentaje* de aumento salarial.

-   **Ecuaci칩n:** $\ln(Y) = \beta_0 + \beta_1 X + \epsilon$
-   **Interpretaci칩n:** Un **incremento de una unidad** en $X$ est치 asociado con un cambio de $(100 \cdot \beta_1)\%$ en $Y$.



::: {.callout-note collapse="true"}
#### Explicaci칩n Matem치tica de la Aproximaci칩n
La clave est치 en la propiedad del logaritmo y el c치lculo. La derivada de $\ln(Y)$ con respecto a $X$ es $\beta_1$:
$$\frac{d(\ln(Y))}{dX} = \beta_1$$
Sabemos que $d(\ln(Y)) = \frac{dY}{Y}$. Por tanto:
$$\frac{dY/Y}{dX} = \beta_1$$
Para cambios peque침os (o discretos, $\Delta$), podemos aproximar los diferenciales:
$$\beta_1 \approx \frac{\Delta Y / Y}{\Delta X}$$
Si consideramos un cambio unitario en X, $\Delta X = 1$, entonces:
$$\beta_1 \approx \frac{\Delta Y}{Y}$$
Esto significa que $\beta_1$ es la aproximaci칩n del cambio porcentual en $Y$ ante un cambio de una unidad en $X$.
:::

### Modelo Nivel-Log (Transformaci칩n en X)

Este modelo es 칰til cuando el efecto de X sobre Y se reduce a medida que X aumenta (rendimientos decrecientes). Por ejemplo, el efecto de a침adir presupuesto de marketing sobre las ventas.

-   **Ecuaci칩n:** $Y = \beta_0 + \beta_1 \ln(X) + \epsilon$
-   **Interpretaci칩n:** Un **incremento del 1%** en $X$ est치 asociado con un cambio de $(\beta_1 / 100)$ **unidades** en $Y$.

::: {.callout-note collapse="true"}
#### Explicaci칩n Matem치tica de la Aproximaci칩n
Tomamos la derivada de $Y$ con respecto a $\ln(X)$:
$$\frac{dY}{d(\ln(X))} = \beta_1$$
Usando la regla de la cadena, sabemos que $d(\ln(X)) = \frac{dX}{X}$. Sustituyendo:
$$\frac{dY}{dX/X} = \beta_1 \implies dY = \beta_1 \frac{dX}{X}$$
Para cambios discretos, aproximamos:
$$\Delta Y \approx \beta_1 \frac{\Delta X}{X}$$
Si consideramos un cambio del 1% en X, entonces $\frac{\Delta X}{X} = 0.01$. La ecuaci칩n se convierte en:
$$\Delta Y \approx \beta_1 (0.01) = \frac{\beta_1}{100}$$
Esto significa que un cambio del 1% en $X$ provoca un cambio de $\beta_1/100$ unidades en $Y$.
:::

### Modelo Log-Log (Transformaci칩n en X e Y)

Este modelo es muy com칰n en econom칤a y modela la **elasticidad** constante entre dos variables.

-   **Ecuaci칩n:** $\ln(Y) = \beta_0 + \beta_1 \ln(X) + \epsilon$
-   **Interpretaci칩n:** Un **incremento del 1%** en $X$ est치 asociado con un cambio del $\beta_1\%$ en $Y$.



::: {.callout-note collapse="true"}
#### Explicaci칩n Matem치tica de la Aproximaci칩n
Este caso combina los dos anteriores. $\beta_1$ es la derivada de $\ln(Y)$ con respecto a $\ln(X)$, que es la definici칩n de elasticidad.
$$\beta_1 = \frac{d(\ln(Y))}{d(\ln(X))}$$
Usando las propiedades del c치lculo que vimos antes:
$$\beta_1 = \frac{dY/Y}{dX/X}$$
Aproximando para cambios discretos:
$$\beta_1 \approx \frac{\Delta Y / Y}{\Delta X / X}$$
Esta es la definici칩n de elasticidad: el cambio porcentual en $Y$ dividido por el cambio porcentual en $X$. Por lo tanto, si $X$ cambia en un 1% ($\Delta X / X = 0.01$), el cambio porcentual en $Y$ ($\Delta Y / Y$) ser치 aproximadamente $\beta_1 \times 0.01$, es decir, un $\beta_1\%$.
:::

---
