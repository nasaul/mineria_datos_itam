{
  "hash": "d65b712a0c1f8bec7e1d5a3b11b20613",
  "result": {
    "engine": "jupyter",
    "markdown": "# Regresi√≥n lineal\n\n## Regresi√≥n Lineal Simple\n\nComenzaremos con el caso m√°s sencillo: predecir una variable de resultado `Y` a partir de una √∫nica variable predictora `X`.\n\nEl modelo matem√°tico que queremos ajustar es una l√≠nea recta:\n\n$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$\n\nDonde:\n\n- **$Y$**: La variable dependiente (lo que queremos predecir).\n- **$X$**: La variable independiente (nuestro predictor).\n- **$\\beta_0$**: El intercepto (el valor de $Y$ cuando $X=0$).\n- **$\\beta_1$**: La pendiente (cu√°nto cambia $Y$ por cada unidad que aumenta $X$).\n- **$\\epsilon$**: El t√©rmino de error (la parte de $Y$ que nuestro modelo no puede explicar).\n\nNuestro objetivo üéØ es encontrar los **mejores valores posibles** para los coeficientes $\\beta_0$ y $\\beta_1$ usando los datos que tenemos.\n\n\n\n### ¬øC√≥mo estimamos los coeficientes $\\beta_0$ y $\\beta_1$?\n\n\"Mejor\" para nosotros significa encontrar la l√≠nea que minimice la distancia vertical entre cada punto de dato y la propia l√≠nea. Espec√≠ficamente, minimizamos la **Suma de los Errores al Cuadrado** (SEC o *Sum of Squared Errors*, SSE).\n\nLa funci√≥n de costo (o p√©rdida) que queremos minimizar es:\n\n$$J(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2$$\n\nTenemos dos m√©todos principales para encontrar los $\\beta$ que minimizan esta funci√≥n:\n\n#### M√©todo 1: Las Ecuaciones Normales (La soluci√≥n anal√≠tica üß†)\n\nEste m√©todo utiliza c√°lculo para encontrar el m√≠nimo exacto de la funci√≥n de costo. Para ello, tomamos las derivadas parciales de $J$ con respecto a $\\beta_0$ y $\\beta_1$, las igualamos a cero y resolvemos para los coeficientes.\n\n::: {.callout-note collapse=\"true\"}\n## **Derivada parcial con respecto a $\\beta_0$**:\n$$\\frac{\\partial J}{\\partial \\beta_0} = \\sum_{i=1}^{n} -2(y_i - \\beta_0 - \\beta_1 x_i) = 0$$\n$$\\sum y_i - n\\beta_0 - \\beta_1 \\sum x_i = 0$$\n$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n::: \n\n::: {.callout-note collapse=\"true\"}\n## **Derivada parcial con respecto a $\\beta_1$**\n$$\\frac{\\partial J}{\\partial \\beta_1} = \\sum_{i=1}^{n} -2x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0$$\nSustituyendo $\\beta_0$ de la primera ecuaci√≥n y resolviendo, llegamos a:\n$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n::: \n\nEstas f√≥rmulas nos dan los valores √≥ptimos y exactos de los coeficientes directamente a partir de los datos.\n\n#### M√©todo 2: Descenso en Gradiente (La soluci√≥n iterativa ‚öôÔ∏è)\n\nEste es un m√©todo computacional que nos \"acerca\" progresivamente a la soluci√≥n. Es especialmente √∫til cuando tenemos una cantidad masiva de datos y calcular la soluci√≥n anal√≠tica es muy costoso.\n\n**La intuici√≥n:** Imagina que est√°s en una monta√±a (la funci√≥n de costo) y quieres llegar al valle (el costo m√≠nimo). El Descenso en Gradiente te dice que mires a tu alrededor y des un paso en la direcci√≥n m√°s inclinada hacia abajo. Repites esto hasta llegar al fondo.\n\n\n\nEl algoritmo funciona as√≠:\n\n1.  **Inicializa** los coeficientes $\\beta_0$ y $\\beta_1$ con valores aleatorios (o en ceros).\n2.  **Calcula el gradiente** de la funci√≥n de costo. El gradiente es un vector que apunta en la direcci√≥n del m√°ximo ascenso. Nosotros iremos en la direcci√≥n opuesta.\n    -   $\\frac{\\partial J}{\\partial \\beta_0} = -2 \\sum (y_i - (\\beta_0 + \\beta_1 x_i))$\n    -   $\\frac{\\partial J}{\\partial \\beta_1} = -2 \\sum x_i(y_i - (\\beta_0 + \\beta_1 x_i))$\n3.  **Actualiza** los coeficientes usando una **tasa de aprendizaje** ($\\alpha$), que controla el tama√±o del paso que damos.\n    -   $\\beta_0 := \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0}$\n    -   $\\beta_1 := \\beta_1 - \\alpha \\frac{\\partial J}{\\partial \\beta_1}$\n4.  **Repite** los pasos 2 y 3 durante un n√∫mero determinado de iteraciones o hasta que el cambio en el costo sea muy peque√±o (convergencia).\n\n::: {.callout-note collapse=\"true\"}\n#### Explicacion visual\n![](imgs/gradient_descent.gif)\n:::\n\n## ¬øCu√°les son los supuestos de la regresi√≥n? üßê\n\nPara que nuestro modelo sea confiable (es decir, para que los coeficientes y las predicciones tengan sentido), debemos cumplir con ciertos supuestos.\n\n1.  **Linealidad:** La relaci√≥n entre $\\beta$ y $Y$ debe ser lineal.\n    -   **¬øPara qu√© sirve?** Si la relaci√≥n no es lineal, nuestro modelo de l√≠nea recta ser√° intr√≠nsecamente incorrecto.\n\n2.  **Independencia de los errores:** Los errores (residuos) no deben estar correlacionados entre s√≠.\n    -   **¬øPara qu√© sirve?** Es crucial para datos de series temporales. Si los errores est√°n correlacionados, la informaci√≥n de un error nos da pistas sobre el siguiente, lo cual viola la idea de que cada observaci√≥n es independiente.\n\n3.  **Homocedasticidad (Varianza constante de los errores):** La varianza de los errores debe ser constante para todos los niveles de $X$.\n    -   **¬øPara qu√© sirve?** Si la varianza cambia (heterocedasticidad), nuestras predicciones ser√°n mejores para algunas partes de los datos que para otras, y los intervalos de confianza para los coeficientes ser√°n poco fiables. Visualmente, en un gr√°fico de residuos vs. valores predichos, no queremos ver una forma de cono o embudo.\n\n4.  **Normalidad de los errores:** Los errores deben seguir una distribuci√≥n normal con media cero.\n    -   **¬øPara qu√© sirve?** Este supuesto es fundamental para poder realizar pruebas de hip√≥tesis sobre los coeficientes (como los p-values) y construir intervalos de confianza. Podemos verificarlo con un histograma de los residuos o un gr√°fico Q-Q.\n\n---\n\n## ¬øC√≥mo evaluar la precisi√≥n del modelo? üìà\n\nUna vez que hemos ajustado el modelo, ¬øc√≥mo sabemos si es bueno?\n\n### Coeficiente de Determinaci√≥n ($R^2$)\n\nEl **$R^2$** mide la proporci√≥n de la varianza total en la variable dependiente ($Y$) que es explicada por nuestro modelo.\n\n$$R^2 = 1 - \\frac{\\text{Suma de Errores al Cuadrado (SEC)}}{\\text{Suma Total de Cuadrados (STC)}} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$$\n\n-   $R^2$ var√≠a entre 0 y 1 (o 0% y 100%).\n-   Un $R^2$ de 0.85 significa que el 85% de la variabilidad en $Y$ puede ser explicada por $X$.\n-   Un $R^2$ m√°s alto generalmente indica un mejor ajuste del modelo.\n\n### p-values (Valores p)\n\nEl **p-value** nos ayuda a determinar si nuestra variable predictora $X$ es **estad√≠sticamente significativa**. Responde a la pregunta: ¬øEs probable que la relaci√≥n que observamos entre $X$ y $Y$ haya ocurrido por puro azar?\n\n-   **Hip√≥tesis Nula ($H_0$):** No hay relaci√≥n entre $X$ y $Y$ (es decir, $\\beta_1 = 0$).\n-   **Hip√≥tesis Alternativa ($H_a$):** S√≠ hay una relaci√≥n entre $X$ y $Y$ (es decir, $\\beta_1 \\neq 0$).\n\nUn **p-value peque√±o** (t√≠picamente < 0.05) nos da evidencia para rechazar la hip√≥tesis nula. Esto sugiere que nuestra variable $X$ es un predictor √∫til para $Y$.\n\n## M√©tricas de Error de Predicci√≥n\n\nAdem√°s del $R^2$, existen m√∫ltiples m√©tricas para evaluar qu√© tan bien predice nuestro modelo. Cada una tiene sus ventajas y casos de uso espec√≠ficos:\n\n### Error Cuadr√°tico Medio (MSE)\n\nEl **MSE** mide el promedio de los errores al cuadrado:\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\n- **Ventajas:** Penaliza fuertemente errores grandes, diferenciable (√∫til para optimizaci√≥n)\n- **Desventajas:** Sensible a valores at√≠picos, dif√≠cil de interpretar (unidades al cuadrado)\n- **Cu√°ndo usar:** Cuando errores grandes son especialmente costosos\n\n### Ra√≠z del Error Cuadr√°tico Medio (RMSE)\n\nEl **RMSE** es la ra√≠z cuadrada del MSE:\n\n$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n\n- **Ventajas:** Mismas unidades que la variable objetivo, interpretable\n- **Desventajas:** A√∫n sensible a valores at√≠picos\n- **Interpretaci√≥n:** \"En promedio, nuestras predicciones se desv√≠an X unidades del valor real\"\n\n### Error Absoluto Medio (MAE)\n\nEl **MAE** mide el promedio de los errores absolutos:\n\n$$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n\n- **Ventajas:** Robusto a valores at√≠picos, f√°cil de interpretar\n- **Desventajas:** No diferenciable en cero, trata todos los errores por igual\n- **Cu√°ndo usar:** Cuando hay valores at√≠picos o todos los errores tienen igual importancia\n\n### Error Porcentual Absoluto Medio (MAPE)\n\nEl **MAPE** expresa el error como porcentaje del valor real:\n\n$$MAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$$\n\n- **Ventajas:** Interpretable (% de error), adimensional, √∫til para comparar modelos en diferentes escalas\n- **Desventajas:** Indefinido cuando $y_i = 0$, asim√©trico (penaliza m√°s las sobreestimaciones)\n- **Interpretaci√≥n:** \"Nuestras predicciones se desv√≠an en promedio X% del valor real\"\n- **Cu√°ndo usar:** Para comparar precisi√≥n entre diferentes productos, regiones, o escalas\n\n### Error Porcentual Absoluto Medio Sim√©trico (SMAPE)\n\nEl **SMAPE** es una versi√≥n sim√©trica del MAPE:\n\n$$SMAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}$$\n\n- **Ventajas:** Sim√©trico, acotado entre 0% y 200%\n- **Desventajas:** Puede ser contraintuitivo, no tan est√°ndar como MAPE\n- **Cu√°ndo usar:** Cuando queremos evitar el sesgo del MAPE hacia sobreestimaciones\n\n### Error Logar√≠tmico Cuadr√°tico Medio (MSLE)\n\nEl **MSLE** usa transformaci√≥n logar√≠tmica:\n\n$$MSLE = \\frac{1}{n} \\sum_{i=1}^{n} (\\log(1 + y_i) - \\log(1 + \\hat{y}_i))^2$$\n\n- **Ventajas:** Penaliza m√°s las subestimaciones que las sobreestimaciones\n- **Desventajas:** Solo para valores positivos, menos interpretable\n- **Cu√°ndo usar:** Cuando subestimar es m√°s costoso que sobreestimar (ej: demanda de inventario)\n\n### $R^2$ Ajustado\n\nEl **$R^2$ ajustado** penaliza por el n√∫mero de variables en el modelo:\n\n$$R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}$$\n\nDonde $p$ es el n√∫mero de predictores.\n\n- **Ventajas:** No aumenta autom√°ticamente al a√±adir variables\n- **Cu√°ndo usar:** Para comparar modelos con diferente n√∫mero de variables\n- **Interpretaci√≥n:** Similar a $R^2$ pero m√°s conservador\n\n#### ¬øCu√°l m√©trica elegir?\n\nLa elecci√≥n de m√©trica depende del contexto del problema:\n\n| **M√©trica** | **Mejor para** | **Evitar cuando** |\n|-------------|----------------|-------------------|\n| **RMSE** | Errores grandes son costosos | Hay muchos valores at√≠picos |\n| **MAE** | Errores tienen igual importancia | Necesitas diferenciabilidad |\n| **MAPE** | Comparar diferentes escalas | Hay valores cercanos a cero |\n| **SMAPE** | Comparar con simetr√≠a | Interpretaci√≥n debe ser simple |\n| **R¬≤** | Explicar variabilidad | Solo importa precisi√≥n de predicci√≥n |\n\n::: {.callout-tip}\n## **Recomendaci√≥n pr√°ctica**\nUsa **m√∫ltiples m√©tricas** para evaluar tu modelo. Una combinaci√≥n t√≠pica ser√≠a:\n- **RMSE** para precisi√≥n general\n- **MAPE** para interpretabilidad de negocio  \n- **R¬≤** para explicaci√≥n de variabilidad\n:::\n\n---\n\n## Regresi√≥n Lineal M√∫ltiple\n\nAhora, ¬øqu√© pasa si tenemos m√∫ltiples predictores ($X_1, X_2, ..., X_p$)? El modelo se expande:\n\n$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon$$\n\nLa intuici√≥n es la misma, pero en lugar de ajustar una l√≠nea, estamos ajustando un **hiperplano** en un espacio multidimensional.\n\nPara manejar esto de forma elegante, usamos notaci√≥n matricial:\n\n$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$$\n\nDonde:\n- $\\mathbf{y}$ es el vector de observaciones.\n- $\\mathbf{X}$ es la matriz de dise√±o (con una primera columna de unos para el intercepto).\n- $\\boldsymbol{\\beta}$ es el vector de coeficientes.\n- $\\boldsymbol{\\epsilon}$ es el vector de errores.\n\nLa funci√≥n de costo en forma matricial es:\n$$J(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$$\n\n---\n\n## Transformaciones Comunes en Modelos Lineales\n\nA veces, la relaci√≥n entre X e Y no es estrictamente lineal. Las transformaciones logar√≠tmicas nos permiten modelar relaciones no lineales y, adem√°s, ofrecen interpretaciones muy √∫tiles en t√©rminos de cambios porcentuales.\n\n### Modelo Log-Nivel (Transformaci√≥n en Y)\n\nEste modelo se usa cuando el efecto de X sobre Y no es absoluto, sino porcentual. Por ejemplo, c√≥mo un a√±o m√°s de educaci√≥n afecta el *porcentaje* de aumento salarial.\n\n-   **Ecuaci√≥n:** $\\ln(Y) = \\beta_0 + \\beta_1 X + \\epsilon$\n-   **Interpretaci√≥n:** Un **incremento de una unidad** en $X$ est√° asociado con un cambio de $(100 \\cdot \\beta_1)\\%$ en $Y$.\n\n\n\n::: {.callout-note collapse=\"true\"}\n#### Explicaci√≥n Matem√°tica de la Aproximaci√≥n\nLa clave est√° en la propiedad del logaritmo y el c√°lculo. La derivada de $\\ln(Y)$ con respecto a $X$ es $\\beta_1$:\n$$\\frac{d(\\ln(Y))}{dX} = \\beta_1$$\nSabemos que $d(\\ln(Y)) = \\frac{dY}{Y}$. Por tanto:\n$$\\frac{dY/Y}{dX} = \\beta_1$$\nPara cambios peque√±os (o discretos, $\\Delta$), podemos aproximar los diferenciales:\n$$\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X}$$\nSi consideramos un cambio unitario en X, $\\Delta X = 1$, entonces:\n$$\\beta_1 \\approx \\frac{\\Delta Y}{Y}$$\nEsto significa que $\\beta_1$ es la aproximaci√≥n del cambio porcentual en $Y$ ante un cambio de una unidad en $X$.\n:::\n\n### Modelo Nivel-Log (Transformaci√≥n en X)\n\nEste modelo es √∫til cuando el efecto de X sobre Y se reduce a medida que X aumenta (rendimientos decrecientes). Por ejemplo, el efecto de a√±adir presupuesto de marketing sobre las ventas.\n\n-   **Ecuaci√≥n:** $Y = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon$\n-   **Interpretaci√≥n:** Un **incremento del 1%** en $X$ est√° asociado con un cambio de $(\\beta_1 / 100)$ **unidades** en $Y$.\n\n::: {.callout-note collapse=\"true\"}\n#### Explicaci√≥n Matem√°tica de la Aproximaci√≥n\nTomamos la derivada de $Y$ con respecto a $\\ln(X)$:\n$$\\frac{dY}{d(\\ln(X))} = \\beta_1$$\nUsando la regla de la cadena, sabemos que $d(\\ln(X)) = \\frac{dX}{X}$. Sustituyendo:\n$$\\frac{dY}{dX/X} = \\beta_1 \\implies dY = \\beta_1 \\frac{dX}{X}$$\nPara cambios discretos, aproximamos:\n$$\\Delta Y \\approx \\beta_1 \\frac{\\Delta X}{X}$$\nSi consideramos un cambio del 1% en X, entonces $\\frac{\\Delta X}{X} = 0.01$. La ecuaci√≥n se convierte en:\n$$\\Delta Y \\approx \\beta_1 (0.01) = \\frac{\\beta_1}{100}$$\nEsto significa que un cambio del 1% en $X$ provoca un cambio de $\\beta_1/100$ unidades en $Y$.\n:::\n\n### Modelo Log-Log (Transformaci√≥n en X e Y)\n\nEste modelo es muy com√∫n en econom√≠a y modela la **elasticidad** constante entre dos variables.\n\n-   **Ecuaci√≥n:** $\\ln(Y) = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon$\n-   **Interpretaci√≥n:** Un **incremento del 1%** en $X$ est√° asociado con un cambio del $\\beta_1\\%$ en $Y$.\n\n\n\n::: {.callout-note collapse=\"true\"}\n#### Explicaci√≥n Matem√°tica de la Aproximaci√≥n\nEste caso combina los dos anteriores. $\\beta_1$ es la derivada de $\\ln(Y)$ con respecto a $\\ln(X)$, que es la definici√≥n de elasticidad.\n$$\\beta_1 = \\frac{d(\\ln(Y))}{d(\\ln(X))}$$\nUsando las propiedades del c√°lculo que vimos antes:\n$$\\beta_1 = \\frac{dY/Y}{dX/X}$$\nAproximando para cambios discretos:\n$$\\beta_1 \\approx \\frac{\\Delta Y / Y}{\\Delta X / X}$$\nEsta es la definici√≥n de elasticidad: el cambio porcentual en $Y$ dividido por el cambio porcentual en $X$. Por lo tanto, si $X$ cambia en un 1% ($\\Delta X / X = 0.01$), el cambio porcentual en $Y$ ($\\Delta Y / Y$) ser√° aproximadamente $\\beta_1 \\times 0.01$, es decir, un $\\beta_1\\%$.\n:::\n\n---\n\n## Regresi√≥n Regularizada (Penalizada) üéØ\n\nHasta ahora hemos visto la regresi√≥n lineal cl√°sica, pero ¬øqu√© pasa cuando tenemos **muchas variables** o cuando nuestro modelo sufre de **sobreajuste**? Aqu√≠ es donde entran las t√©cnicas de **regularizaci√≥n**.\n\n### ¬øPor qu√© necesitamos regularizaci√≥n?\n\nLa regresi√≥n lineal ordinaria (OLS) puede presentar varios problemas:\n\n1. **Sobreajuste**: Cuando tenemos muchas variables relativas al n√∫mero de observaciones\n2. **Multicolinealidad**: Variables predictoras altamente correlacionadas\n3. **Inestabilidad**: Peque√±os cambios en los datos causan grandes cambios en los coeficientes\n4. **Interpretabilidad**: Demasiadas variables hacen dif√≠cil entender el modelo\n\nLa **regularizaci√≥n** a√±ade una **penalizaci√≥n** a la funci√≥n de costo para controlar la complejidad del modelo.\n\n---\n\n### Ridge Regression (Regresi√≥n Ridge) üèîÔ∏è\n\nLa **regresi√≥n Ridge** a√±ade una penalizaci√≥n **L2** (suma de cuadrados) a los coeficientes:\n\n$$J_{Ridge}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n\nDonde:\n- $\\lambda > 0$ es el **par√°metro de regularizaci√≥n**\n- $\\sum_{j=1}^{p} \\beta_j^2$ es la **penalizaci√≥n L2**\n\n#### Caracter√≠sticas de Ridge:\n\n‚úÖ **Ventajas:**\n- Reduce el sobreajuste\n- Maneja bien la multicolinealidad\n- Siempre tiene soluci√≥n √∫nica\n- Estabiliza los coeficientes\n\n‚ùå **Desventajas:**\n- **NO** elimina variables (coeficientes nunca son exactamente cero)\n- Dificulta la interpretabilidad\n- Requiere estandarizar las variables\n\n#### Soluci√≥n Anal√≠tica:\n\n$$\\hat{\\boldsymbol{\\beta}}_{Ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n\nEl t√©rmino $\\lambda\\mathbf{I}$ hace que la matriz sea invertible incluso con multicolinealidad.\n\n#### ¬øC√≥mo elegir Œª?\n\n- **Œª = 0**: Regresi√≥n ordinaria (sin penalizaci√≥n)\n- **Œª ‚Üí ‚àû**: Todos los coeficientes ‚Üí 0\n- **Œª √≥ptimo**: Se encuentra usando **validaci√≥n cruzada**\n\n---\n\n### Lasso Regression (Least Absolute Shrinkage and Selection Operator) ‚úÇÔ∏è\n\nLa **regresi√≥n Lasso** usa penalizaci√≥n **L1** (suma de valores absolutos):\n\n$$J_{Lasso}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n\n#### Caracter√≠sticas de Lasso:\n\n‚úÖ **Ventajas:**\n- **Selecci√≥n autom√°tica de variables** (coeficientes = 0)\n- Modelos m√°s interpretables y simples\n- √ötil cuando muchas variables son irrelevantes\n\n‚ùå **Desventajas:**\n- Puede ser inestable con grupos de variables correlacionadas\n- Selecciona arbitrariamente entre variables correlacionadas\n- No tiene soluci√≥n anal√≠tica cerrada\n\n#### La \"Magia\" de L1: ¬øPor qu√© produce ceros exactos?\n\nLa penalizaci√≥n L1 crea una regi√≥n factible con **esquinas puntiagudas**. La soluci√≥n √≥ptima tiende a ocurrir en estas esquinas, donde algunos coeficientes son exactamente cero.\n\n::: {.callout-note collapse=\"true\"}\n## Intuici√≥n Geom√©trica\nImagina que est√°s minimizando una funci√≥n bajo la restricci√≥n de que $|\\beta_1| + |\\beta_2| \\leq t$. Esta restricci√≥n forma un **diamante** en 2D. La funci√≥n objetivo forma **elipses**. La soluci√≥n est√° donde la elipse m√°s peque√±a toca el diamante, y esto frecuentemente ocurre en los v√©rtices (donde $\\beta_1 = 0$ o $\\beta_2 = 0$).\n:::\n\n---\n\n### Elastic Net: Lo Mejor de Ambos Mundos üï∏Ô∏è\n\n**Elastic Net** combina las penalizaciones L1 y L2:\n\n$$J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2$$\n\nO equivalentemente, con un par√°metro de mezcla $\\alpha$:\n\n$$J_{ElasticNet}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda \\left[ \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right]$$\n\nDonde:\n- $\\alpha \\in [0,1]$ controla la mezcla entre L1 y L2\n- $\\alpha = 0$: Pure Ridge\n- $\\alpha = 1$: Pure Lasso\n- $\\alpha = 0.5$: Igual peso a ambas penalizaciones\n\n#### Caracter√≠sticas de Elastic Net:\n\n‚úÖ **Ventajas:**\n- **Selecci√≥n de variables** como Lasso\n- **Estabilidad** como Ridge\n- Maneja bien **grupos de variables correlacionadas**\n- M√°s flexible que Ridge o Lasso por separado\n\n‚ùå **Desventajas:**\n- Dos hiperpar√°metros para ajustar ($\\lambda$ y $\\alpha$)\n- M√°s complejo computacionalmente\n\n---\n\n### Comparaci√≥n Visual: Ridge vs Lasso vs Elastic Net\n\n| **Aspecto** | **Ridge** | **Lasso** | **Elastic Net** |\n|-------------|-----------|-----------|-----------------|\n| **Penalizaci√≥n** | L2: $\\sum \\beta_j^2$ | L1: $\\sum |\\beta_j|$ | L1 + L2 combinadas |\n| **Selecci√≥n de variables** | ‚ùå No | ‚úÖ S√≠ | ‚úÖ S√≠ |\n| **Coeficientes exactamente cero** | ‚ùå No | ‚úÖ S√≠ | ‚úÖ S√≠ |\n| **Manejo de multicolinealidad** | ‚úÖ Excelente | ‚ö†Ô∏è Problem√°tico | ‚úÖ Muy bueno |\n| **Estabilidad** | ‚úÖ Alta | ‚ö†Ô∏è Media | ‚úÖ Alta |\n| **Interpretabilidad** | ‚ö†Ô∏è Media | ‚úÖ Alta | ‚úÖ Alta |\n| **Cuando usar** | Todas las variables importan | Pocas variables importantes | Situaciones mixtas |\n\n### ¬øCu√°ndo usar cada m√©todo?\n\n#### Usa **Ridge** cuando:\n- Crees que **todas las variables** contribuyen al modelo\n- Tienes **multicolinealidad** severa\n- Quieres **estabilizar** coeficientes sin eliminar variables\n- El n√∫mero de observaciones es **peque√±o** relativo a variables\n\n#### Usa **Lasso** cuando:\n- Crees que **pocas variables** son realmente importantes\n- Quieres un modelo **simple e interpretable**\n- Necesitas **selecci√≥n autom√°tica** de variables\n- Tienes muchas variables **irrelevantes**\n\n#### Usa **Elastic Net** cuando:\n- No est√°s seguro de cu√°ntas variables son importantes\n- Tienes **grupos de variables correlacionadas**\n- Quieres balancear **selecci√≥n** y **estabilidad**\n- Es tu **primera opci√≥n** cuando no conoces la estructura de los datos\n\n---\n\n### Validaci√≥n de Modelos y Selecci√≥n de Hiperpar√°metros\n\n#### ¬øPor qu√© necesitamos dividir nuestros datos?\n\nCuando construimos modelos de machine learning, enfrentamos un dilema fundamental: **¬øc√≥mo sabemos si nuestro modelo funcionar√° bien con datos nuevos?**\n\n##### El Problema del Sobreajuste\n\nImagina que est√°s prepar√°ndote para un examen. Si solo estudias las preguntas exactas que aparecer√°n en el examen, podr√≠as obtener una calificaci√≥n perfecta. Pero si las preguntas cambian ligeramente, tu rendimiento se desplomar√≠a. Esto es **sobreajuste**: el modelo memoriza los datos de entrenamiento pero no generaliza.\n\n#### Divisi√≥n T√≠pica de Datos: Entrenamiento/Validaci√≥n/Prueba\n\nLa estrategia est√°ndar es dividir nuestros datos en **tres conjuntos**:\n\n```\nüìä Dataset Completo (100%)\n‚îú‚îÄ‚îÄ üèãÔ∏è Entrenamiento (60%) - Para ajustar coeficientes\n‚îú‚îÄ‚îÄ üéØ Validaci√≥n (20%)     - Para seleccionar hiperpar√°metros  \n‚îî‚îÄ‚îÄ üß™ Prueba (20%)         - Para evaluaci√≥n final\n```\n\n##### Conjunto de Entrenamiento (60%)\n- **Prop√≥sito**: Ajustar los coeficientes $\\beta$ del modelo\n- **Analog√≠a**: Los ejercicios que haces para aprender\n\n##### Conjunto de Validaci√≥n (20%)\n- **Prop√≥sito**: Comparar diferentes hiperpar√°metros (como $\\lambda$ en Ridge/Lasso)\n- **Analog√≠a**: Ex√°menes de pr√°ctica para decidir qu√© estrategia de estudio funciona mejor\n\n##### Conjunto de Prueba (20%)\n- **Prop√≥sito**: Evaluaci√≥n final y honesta del modelo\n- **Analog√≠a**: El examen final real\n- **‚ö†Ô∏è Regla de Oro**: ¬°Solo se usa UNA vez al final!\n\n#### ¬øQu√© pasa si tenemos pocos datos?\n\nCuando nuestro dataset es peque√±o (< 1000 observaciones), dividir en tres partes puede ser problem√°tico:\n\n‚ùå **Problemas con datasets peque√±os:**\n- Conjunto de entrenamiento muy peque√±o ‚Üí modelo pobre\n- Conjunto de validaci√≥n peque√±o ‚Üí selecci√≥n inestable de hiperpar√°metros\n- Conjunto de prueba peque√±o ‚Üí evaluaci√≥n poco confiable\n\n**Soluci√≥n**: ¬°Validaci√≥n Cruzada!\n\n---\n\n#### Validaci√≥n Cruzada (Cross-Validation)\n\nLa **validaci√≥n cruzada** es una t√©cnica que maximiza el uso de nuestros datos limitados. En lugar de usar una sola divisi√≥n, usamos **m√∫ltiples divisiones**.\n\n##### Validaci√≥n Cruzada k-fold\n\nEl m√©todo m√°s com√∫n es **k-fold cross-validation**:\n\n1. **Dividir** el dataset en $k$ \"pliegues\" (folds) de igual tama√±o\n2. **Repetir** $k$ veces:\n   - Usar $k-1$ pliegues para entrenamiento\n   - Usar 1 pliegue para validaci√≥n\n3. **Promediar** los resultados de las $k$ evaluaciones\n\n::: {#cell-cv-visualization .cell fig-height='8' fig-width='12' execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![Visualizaci√≥n de 5-Fold Cross Validation mostrando c√≥mo se dividen los datos en cada iteraci√≥n](03-regresion_lineal_files/figure-html/cv-visualization-output-1.png){#cv-visualization}\n:::\n:::\n\n\n##### Ventajas de la Validaci√≥n Cruzada\n\n‚úÖ **Maximiza el uso de datos**: Cada observaci√≥n se usa tanto para entrenamiento como validaci√≥n\n\n‚úÖ **Estimaci√≥n m√°s robusta**: Promedia m√∫ltiples evaluaciones independientes\n\n‚úÖ **Reduce la varianza**: Menos dependiente de una divisi√≥n particular\n\n‚úÖ **Detecta inestabilidad**: Si los resultados var√≠an mucho entre folds, el modelo es inestable\n\n#### Validaci√≥n Cruzada para Selecci√≥n de Hiperpar√°metros\n\nEn regresi√≥n regularizada, usamos CV para encontrar el mejor $\\lambda$:\n\n::: {#cell-validation-curve .cell fig-height='6' fig-width='12' execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\nüéØ SELECCI√ìN DE HIPERPAR√ÅMETROS CON VALIDACI√ìN CRUZADA\n============================================================\nPara cada valor de Œª:\n  1. Aplicar 5-fold CV\n  2. Calcular error promedio\n  3. Seleccionar Œª con menor error\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Curva de validaci√≥n mostrando c√≥mo seleccionar el hiperpar√°metro √≥ptimo Œª usando validaci√≥n cruzada](03-regresion_lineal_files/figure-html/validation-curve-output-2.png){#validation-curve}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nüìà Resultado: Œª √≥ptimo = 0.1274\nüìâ Error de CV m√≠nimo = 0.4776\n```\n:::\n:::\n\n\n#### Proceso Completo de Validaci√≥n\n\nEl flujo completo para modelos regularizados es:\n\n```\n1. üìä Dividir datos originales\n   ‚îî‚îÄ‚îÄ 80% para desarrollo (entrenamiento + validaci√≥n)\n   ‚îî‚îÄ‚îÄ 20% para prueba final (¬°NO TOCAR hasta el final!)\n\n2. üîÑ En el conjunto de desarrollo:\n   ‚îî‚îÄ‚îÄ Para cada Œª candidato:\n       ‚îú‚îÄ‚îÄ Aplicar k-fold CV\n       ‚îú‚îÄ‚îÄ Calcular error promedio\n       ‚îî‚îÄ‚îÄ Guardar resultado\n\n3. üéØ Seleccionar Œª con menor error de CV\n\n4. üèóÔ∏è Entrenar modelo final con Œª √≥ptimo en TODO el conjunto de desarrollo\n\n5. üß™ Evaluaci√≥n final en conjunto de prueba\n```\n\n#### Variantes de Validaci√≥n Cruzada\n\n##### Leave-One-Out CV (LOOCV)\n- **k = n** (n√∫mero de observaciones)\n- **Ventaja**: M√°ximo uso de datos para entrenamiento\n- **Desventaja**: Computacionalmente costoso, alta varianza\n\n##### Stratified CV\n- **Para problemas de clasificaci√≥n**\n- Mantiene la proporci√≥n de clases en cada fold\n\n##### Time Series CV\n- **Para datos temporales**\n- Respeta el orden temporal (no mezcla futuro con pasado)\n\n",
    "supporting": [
      "03-regresion_lineal_files"
    ],
    "filters": [],
    "includes": {}
  }
}