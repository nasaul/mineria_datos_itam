{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: Análisis de Regresión con el Dataset Wine Quality\n",
    "\n",
    "## Descripción del Dataset\n",
    "\n",
    "El dataset **Wine Quality** contiene resultados de análisis fisicoquímicos de vinos portugueses \"Vinho Verde\" y su calidad evaluada por expertos. El objetivo es predecir la calidad del vino basándose en sus propiedades químicas.\n",
    "\n",
    "### Variables del dataset:\n",
    "- **fixed acidity**: Acidez fija (g/L de ácido tartárico)\n",
    "- **volatile acidity**: Acidez volátil (g/L de ácido acético)\n",
    "- **citric acid**: Ácido cítrico (g/L)\n",
    "- **residual sugar**: Azúcar residual (g/L)\n",
    "- **chlorides**: Cloruros (g/L de cloruro de sodio)\n",
    "- **free sulfur dioxide**: Dióxido de azufre libre (mg/L)\n",
    "- **total sulfur dioxide**: Dióxido de azufre total (mg/L)\n",
    "- **density**: Densidad (g/cm³)\n",
    "- **pH**: pH del vino\n",
    "- **sulphates**: Sulfatos (g/L de sulfato de potasio)\n",
    "- **alcohol**: Contenido de alcohol (% vol)\n",
    "- **quality**: Calidad del vino (puntuación de 0-10) - **Variable objetivo**\n",
    "\n",
    "En este ejercicio, trabajarás con el dataset de vinos tintos y aplicarás diferentes técnicas de regresión para predecir la calidad del vino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar librerías y cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de gráficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configurar seed para reproducibilidad\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de vinos tintos\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine_data = pd.read_csv(url, sep=';')\n",
    "\n",
    "print(\"Dataset cargado exitosamente!\")\n",
    "print(f\"Dimensiones del dataset: {wine_data.shape}\")\n",
    "print(f\"\\nColumnas del dataset:\")\n",
    "print(wine_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "### Ejercicio 2.1: Exploración inicial\n",
    "Completa el análisis exploratorio inicial del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las primeras filas del dataset\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Muestra la información general del dataset (tipos de datos, valores no nulos)\n",
    "# Tu código aquí\n",
    "wine_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula y muestra las estadísticas descriptivas del dataset\n",
    "# Tu código aquí\n",
    "wine_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verifica si hay valores nulos en el dataset\n",
    "# Tu código aquí\n",
    "wine_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2: Análisis de la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar la distribución de la calidad del vino\n",
    "plt.figure(figsize=(10, 6))\n",
    "wine_data['quality'].value_counts().sort_index().plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Calidad del Vino', fontsize=12)\n",
    "plt.ylabel('Frecuencia', fontsize=12)\n",
    "plt.title('Distribución de la Calidad del Vino', fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar estadísticas\n",
    "mean_quality = wine_data['quality'].mean()\n",
    "median_quality = wine_data['quality'].median()\n",
    "plt.axhline(y=wine_data['quality'].value_counts().mean(), color='red', \n",
    "            linestyle='--', label=f'Media de frecuencia')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estadísticas de la calidad del vino:\")\n",
    "print(f\"Media: {mean_quality:.2f}\")\n",
    "print(f\"Mediana: {median_quality:.2f}\")\n",
    "print(f\"Desviación estándar: {wine_data['quality'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.3: Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula la matriz de correlación y visualízala con un heatmap\n",
    "# Pista: Usa sns.heatmap() con annot=True para mostrar los valores\n",
    "# Tu código aquí\n",
    "corr_matrix = wine_data.corr()\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identifica y muestra las 5 variables más correlacionadas con 'quality'\n",
    "# Tu código aquí\n",
    "correlations = wine_data.corr()['quality'].drop('quality').sort_values(ascending=False)\n",
    "print('Top 5 variables más correlacionadas con quality:')\n",
    "print(correlations.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.4: Visualización de relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar las 4 variables más correlacionadas con quality\n",
    "top_features = ['alcohol', 'volatile acidity', 'citric acid', 'sulphates']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Relación entre Variables Principales y Calidad del Vino', fontsize=16)\n",
    "\n",
    "for idx, (ax, feature) in enumerate(zip(axes.flat, top_features)):\n",
    "    # TODO: Crea un scatter plot para cada variable vs quality\n",
    "    # Agrega una línea de tendencia\n",
    "    sns.regplot(x=wine_data[feature], y=wine_data['quality'], ax=ax, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('quality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características (X) y variable objetivo (y)\n",
    "X = wine_data.drop('quality', axis=1)\n",
    "y = wine_data['quality']\n",
    "\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(f\"\\nCaracterísticas: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide los datos en conjuntos de entrenamiento y prueba\n",
    "# Usa test_size=0.2 y random_state=42\n",
    "# Tu código aquí\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Estandariza las características\n",
    "# Recuerda: ajusta el scaler solo con los datos de entrenamiento\n",
    "# Tu código aquí\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validación Cruzada para Selección de Hiperparámetros\n",
    "\n",
    "La validación cruzada es fundamental para seleccionar los mejores hiperparámetros sin usar el conjunto de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4.1: Implementación manual de validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Validación cruzada manual para Ridge\n",
    "def manual_cross_validation(X, y, alpha, n_folds=5):\n",
    "    \"\"\"\n",
    "    Implementa validación cruzada manualmente para Ridge regression\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Dividir datos\n",
    "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Estandarizar\n",
    "        scaler_cv = StandardScaler()\n",
    "        X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n",
    "        X_val_cv_scaled = scaler_cv.transform(X_val_cv)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(X_train_cv_scaled, y_train_cv)\n",
    "        \n",
    "        # Evaluar\n",
    "        y_pred = model.predict(X_val_cv_scaled)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n",
    "        scores.append(rmse)\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# Probar diferentes valores de alpha\n",
    "alphas_to_test = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "cv_results_manual = []\n",
    "\n",
    "print(\"Validación Cruzada Manual para Ridge Regression:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for alpha in alphas_to_test:\n",
    "    mean_rmse, std_rmse = manual_cross_validation(X_train.values, y_train, alpha)\n",
    "    cv_results_manual.append({'alpha': alpha, 'mean_rmse': mean_rmse, 'std_rmse': std_rmse})\n",
    "    print(f\"Alpha: {alpha:7.3f} | RMSE: {mean_rmse:.4f} (+/- {std_rmse:.4f})\")\n",
    "\n",
    "# TODO: Identifica el mejor alpha basado en el RMSE medio más bajo\n",
    "# Tu código aquí\n",
    "best_alpha_manual = min(cv_results_manual, key=lambda x: x['mean_rmse'])['alpha']\n",
    "print(f\"\\nMejor alpha (manual): {best_alpha_manual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4.2: Usar RidgeCV para validación cruzada automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Usa RidgeCV para encontrar automáticamente el mejor alpha\n",
    "# Pista: RidgeCV tiene un parámetro 'alphas' y 'cv'\n",
    "# Tu código aquí\n",
    "alphas = np.logspace(-3, 3, 100)  # 100 valores entre 0.001 y 1000\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Mejor alpha encontrado por RidgeCV: {ridge_cv.alpha_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4.3: GridSearchCV para búsqueda exhaustiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo completo con GridSearchCV para Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir parámetros a buscar\n",
    "param_grid_ridge = {\n",
    "    'alpha': np.logspace(-3, 3, 20)  # 20 valores entre 0.001 y 1000\n",
    "}\n",
    "\n",
    "# Crear modelo base\n",
    "ridge_base = Ridge(random_state=42)\n",
    "\n",
    "# TODO: Implementa GridSearchCV\n",
    "# Usa cv=5, scoring='neg_mean_squared_error'\n",
    "# Tu código aquí\n",
    "grid_search_ridge = GridSearchCV(ridge_base, param_grid_ridge, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Mejor alpha: {grid_search_ridge.best_params_['alpha']}\")\n",
    "print(f\"Mejor score (RMSE): {np.sqrt(-grid_search_ridge.best_score_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualiza los resultados de la validación cruzada\n",
    "# Crea un gráfico que muestre cómo cambia el RMSE con diferentes valores de alpha\n",
    "# Tu código aquí\n",
    "results = grid_search_ridge.cv_results_\n",
    "alphas_plot = results['param_alpha'].data.astype(float)\n",
    "mean_rmse = np.sqrt(-results['mean_test_score'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas_plot, mean_rmse, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('RMSE (validación cruzada)')\n",
    "plt.title('RMSE vs Alpha (GridSearchCV Ridge)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelos de Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Regresión Lineal Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa y entrena un modelo de regresión lineal\n",
    "# Tu código aquí\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_lr_train = lr_model.predict(X_train_scaled)\n",
    "y_pred_lr_test = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calcular métricas\n",
    "rmse_lr_train = np.sqrt(mean_squared_error(y_train, y_pred_lr_train))\n",
    "rmse_lr_test = np.sqrt(mean_squared_error(y_test, y_pred_lr_test))\n",
    "r2_lr_train = r2_score(y_train, y_pred_lr_train)\n",
    "r2_lr_test = r2_score(y_test, y_pred_lr_test)\n",
    "\n",
    "print(\"Regresión Lineal Normal:\")\n",
    "print(f\"RMSE Train: {rmse_lr_train:.4f}\")\n",
    "print(f\"RMSE Test: {rmse_lr_test:.4f}\")\n",
    "print(f\"R² Train: {r2_lr_train:.4f}\")\n",
    "print(f\"R² Test: {r2_lr_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ridge Regression con mejor alpha de CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena Ridge con el mejor alpha encontrado por validación cruzada\n",
    "# Tu código aquí\n",
    "best_alpha_ridge = ridge_cv.alpha_  # Usa el mejor alpha de la sección anterior\n",
    "ridge_model = Ridge(alpha=best_alpha_ridge)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones y métricas\n",
    "y_pred_ridge_train = ridge_model.predict(X_train_scaled)\n",
    "y_pred_ridge_test = ridge_model.predict(X_test_scaled)\n",
    "rmse_ridge_train = np.sqrt(mean_squared_error(y_train, y_pred_ridge_train))\n",
    "rmse_ridge_test = np.sqrt(mean_squared_error(y_test, y_pred_ridge_test))\n",
    "r2_ridge_train = r2_score(y_train, y_pred_ridge_train)\n",
    "r2_ridge_test = r2_score(y_test, y_pred_ridge_test)\n",
    "\n",
    "print(\"Ridge Regression:\")\n",
    "print(f\"RMSE Train: {rmse_ridge_train:.4f}\")\n",
    "print(f\"RMSE Test: {rmse_ridge_test:.4f}\")\n",
    "print(f\"R² Train: {r2_ridge_train:.4f}\")\n",
    "print(f\"R² Test: {r2_ridge_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Lasso Regression con validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa LassoCV para encontrar el mejor alpha automáticamente\n",
    "# Tu código aquí\n",
    "alphas_lasso = np.logspace(-3, 1, 100)\n",
    "lasso_cv = LassoCV(alphas=alphas_lasso, cv=5, random_state=42)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Mejor alpha para Lasso: {lasso_cv.alpha_}\")\n",
    "print(f\"Número de características seleccionadas: {np.sum(lasso_cv.coef_ != 0)}\")\n",
    "\n",
    "# Predicciones y métricas\n",
    "y_pred_lasso_train = lasso_cv.predict(X_train_scaled)\n",
    "y_pred_lasso_test = lasso_cv.predict(X_test_scaled)\n",
    "rmse_lasso_train = np.sqrt(mean_squared_error(y_train, y_pred_lasso_train))\n",
    "rmse_lasso_test = np.sqrt(mean_squared_error(y_test, y_pred_lasso_test))\n",
    "r2_lasso_train = r2_score(y_train, y_pred_lasso_train)\n",
    "r2_lasso_test = r2_score(y_test, y_pred_lasso_test)\n",
    "\n",
    "print(\"Lasso Regression:\")\n",
    "print(f\"RMSE Train: {rmse_lasso_train:.4f}\")\n",
    "print(f\"RMSE Test: {rmse_lasso_test:.4f}\")\n",
    "print(f\"R² Train: {r2_lasso_train:.4f}\")\n",
    "print(f\"R² Test: {r2_lasso_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identifica qué características fueron eliminadas por Lasso\n",
    "# Tu código aquí\n",
    "eliminadas = X.columns[lasso_cv.coef_ == 0]\n",
    "print('Características eliminadas por Lasso:')\n",
    "print(list(eliminadas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Crea una tabla comparativa con todos los modelos\n",
    "# Incluye: RMSE Train, RMSE Test, R² Train, R² Test, MAE Test\n",
    "# Tu código aquí\n",
    "comparison_data = {\n",
    "    'Modelo': ['Linear Regression', 'Ridge', 'Lasso'],\n",
    "    'RMSE Train': [rmse_lr_train, rmse_ridge_train, rmse_lasso_train],\n",
    "    'RMSE Test': [rmse_lr_test, rmse_ridge_test, rmse_lasso_test],\n",
    "    'R² Train': [r2_lr_train, r2_ridge_train, r2_lasso_train],\n",
    "    'R² Test': [r2_lr_test, r2_ridge_test, r2_lasso_test],\n",
    "    'MAE Test': [mean_absolute_error(y_test, y_pred_lr_test), mean_absolute_error(y_test, y_pred_ridge_test), mean_absolute_error(y_test, y_pred_lasso_test)]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Crea visualizaciones para comparar los modelos\n",
    "# 1. Gráfico de barras comparando RMSE\n",
    "# 2. Gráfico de barras comparando R²\n",
    "# Tu código aquí\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(x='Modelo', y='RMSE Test', data=comparison_df)\n",
    "plt.title('Comparación RMSE Test')\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(x='Modelo', y='R² Test', data=comparison_df)\n",
    "plt.title('Comparación R² Test')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis de Residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Para el mejor modelo, crea:\n",
    "# 1. Gráfico de residuos vs predicciones\n",
    "# 2. Histograma de residuos\n",
    "# 3. Q-Q plot de residuos\n",
    "# Tu código aquí\n",
    "# Usaremos Ridge como ejemplo, puedes cambiar por el mejor modelo según la tabla\n",
    "import scipy.stats as stats\n",
    "residuos = y_test - y_pred_ridge_test\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(y_pred_ridge_test, residuos, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Residuos')\n",
    "plt.title('Residuos vs Predicciones')\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(residuos, kde=True)\n",
    "plt.title('Histograma de Residuos')\n",
    "plt.subplot(1,3,3)\n",
    "stats.probplot(residuos, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q plot de Residuos')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Importancia de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualiza los coeficientes de los tres modelos en un mismo gráfico\n",
    "# Esto te ayudará a entender qué características son más importantes\n",
    "# Tu código aquí\n",
    "coefs = pd.DataFrame({\n",
    "    'Linear': lr_model.coef_,\n",
    "    'Ridge': ridge_model.coef_,\n",
    "    'Lasso': lasso_cv.coef_\n",
    "}, index=X.columns)\n",
    "coefs.plot(kind='bar', figsize=(14,6))\n",
    "plt.title('Comparación de Coeficientes de Modelos')\n",
    "plt.ylabel('Valor del Coeficiente')\n",
    "plt.xlabel('Característica')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validación Cruzada Final del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Realiza validación cruzada con 10 folds del mejor modelo\n",
    "# Reporta la media y desviación estándar del RMSE\n",
    "# Tu código aquí\n",
    "from sklearn.model_selection import cross_val_score\n",
    "ridge_cv_scores = cross_val_score(ridge_model, X_train_scaled, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "ridge_cv_rmse = np.sqrt(-ridge_cv_scores)\n",
    "print(f\"RMSE promedio (10-fold CV): {ridge_cv_rmse.mean():.4f}\")\n",
    "print(f\"Desviación estándar RMSE: {ridge_cv_rmse.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusiones y Preguntas de Reflexión\n",
    "\n",
    "### Preguntas para responder:\n",
    "\n",
    "1. **¿Cuál modelo tuvo el mejor desempeño? ¿Por qué crees que fue así?**\n",
    "   - Tu respuesta:\n",
    "\n",
    "2. **¿Qué características son las más importantes para predecir la calidad del vino?**\n",
    "   - Tu respuesta:\n",
    "\n",
    "3. **¿Observas señales de sobreajuste en algún modelo? ¿Cómo lo identificaste?**\n",
    "   - Tu respuesta:\n",
    "\n",
    "4. **¿Cómo cambió el rendimiento de Ridge y Lasso con diferentes valores de alpha?**\n",
    "   - Tu respuesta:\n",
    "\n",
    "5. **¿Qué ventajas observaste al usar validación cruzada para seleccionar hiperparámetros?**\n",
    "   - Tu respuesta:\n",
    "\n",
    "6. **Si Lasso eliminó algunas características, ¿crees que esto mejoró o empeoró el modelo? ¿Por qué?**\n",
    "   - Tu respuesta:\n",
    "\n",
    "7. **¿Qué otros pasos podrías tomar para mejorar el rendimiento del modelo?**\n",
    "   - Tu respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio Extra: Ingeniería de Características\n",
    "\n",
    "### Desafío:\n",
    "Intenta mejorar el rendimiento del modelo creando nuevas características:\n",
    "\n",
    "1. Crea interacciones entre variables (ej: alcohol × pH)\n",
    "2. Crea características polinomiales\n",
    "3. Agrupa la calidad en categorías (baja: 3-4, media: 5-6, alta: 7-8) y úsala como característica\n",
    "4. Crea ratios entre características relacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa ingeniería de características y evalúa si mejora el modelo\n",
    "# Tu código aquí\n",
    "# Ejemplo: crear una interacción y una característica polinomial\n",
    "X_feat = X.copy()\n",
    "X_feat['alcohol_x_pH'] = X_feat['alcohol'] * X_feat['pH']\n",
    "X_feat['alcohol_squared'] = X_feat['alcohol'] ** 2\n",
    "\n",
    "# Repetir split y escalado\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_feat, y, test_size=0.2, random_state=42)\n",
    "scaler_f = StandardScaler()\n",
    "X_train_f_scaled = scaler_f.fit_transform(X_train_f)\n",
    "X_test_f_scaled = scaler_f.transform(X_test_f)\n",
    "\n",
    "# Entrenar Ridge con el mejor alpha anterior\n",
    "ridge_model_f = Ridge(alpha=best_alpha_ridge)\n",
    "ridge_model_f.fit(X_train_f_scaled, y_train_f)\n",
    "y_pred_ridge_f_test = ridge_model_f.predict(X_test_f_scaled)\n",
    "rmse_ridge_f_test = np.sqrt(mean_squared_error(y_test_f, y_pred_ridge_f_test))\n",
    "print(f\"RMSE Test con ingeniería de características: {rmse_ridge_f_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
