{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfe77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Importar las librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de gráficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configurar seed para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar el dataset de vinos tintos\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine_data = pd.read_csv(url, sep=';')\n",
    "\n",
    "print(\"Dataset cargado exitosamente!\")\n",
    "print(f\"Dimensiones del dataset: {wine_data.shape}\")\n",
    "print(f\"\\nColumnas del dataset:\")\n",
    "print(wine_data.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 2. Análisis Exploratorio de Datos (EDA)\n",
    "#\n",
    "# ### Ejercicio 2.1: Exploración inicial\n",
    "# Completa el análisis exploratorio inicial del dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mostrar las primeras filas del dataset\n",
    "wine_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Muestra la información general del dataset (tipos de datos, valores no nulos)\n",
    "# Tu código aquí\n",
    "wine_data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Calcula y muestra las estadísticas descriptivas del dataset\n",
    "# Tu código aquí\n",
    "wine_data.describe().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761297db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Verifica si hay valores nulos en el dataset\n",
    "# Tu código aquí\n",
    "wine_data.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b43cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Ejercicio 2.2: Análisis de la variable objetivo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analizar la distribución de la calidad del vino\n",
    "plt.figure(figsize=(10, 6))\n",
    "wine_data['quality'].value_counts().sort_index().plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Calidad del Vino', fontsize=12)\n",
    "plt.ylabel('Frecuencia', fontsize=12)\n",
    "plt.title('Distribución de la Calidad del Vino', fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar estadísticas\n",
    "mean_quality = wine_data['quality'].mean()\n",
    "median_quality = wine_data['quality'].median()\n",
    "plt.axhline(y=wine_data['quality'].value_counts().mean(), color='red',\n",
    "            linestyle='--', label=f'Media de frecuencia')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estadísticas de la calidad del vino:\")\n",
    "print(f\"Media: {mean_quality:.2f}\")\n",
    "print(f\"Mediana: {median_quality:.2f}\")\n",
    "print(f\"Desviación estándar: {wine_data['quality'].std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ac479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Ejercicio 2.3: Matriz de correlación\n",
    "\n",
    "\n",
    "# TODO: Calcula la matriz de correlación y visualízala con un heatmap\n",
    "# Pista: Usa sns.heatmap() con annot=True para mostrar los valores\n",
    "# Tu código aquí\n",
    "plt.figure(figsize=(14, 10))\n",
    "corr = wine_data.corr(numeric_only=True)\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=False, cbar=True)\n",
    "plt.title(\"Matriz de correlación\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Identifica y muestra las 5 variables más correlacionadas con 'quality'\n",
    "# Tu código aquí\n",
    "target_corr = corr['quality'].drop('quality').sort_values(ascending=False)\n",
    "top5_corr = target_corr.head(5)\n",
    "print(\"Top 5 variables más correlacionadas positivamente con 'quality':\")\n",
    "print(top5_corr)\n",
    "print(\"\\nTop 5 variables más correlacionadas (en valor absoluto):\")\n",
    "print(target_corr.abs().sort_values(ascending=False).head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Ejercicio 2.4: Visualización de relaciones\n",
    "\n",
    "# Visualizar las 4 variables más correlacionadas con quality\n",
    "top_features = ['alcohol', 'volatile acidity', 'citric acid', 'sulphates']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Relación entre Variables Principales y Calidad del Vino', fontsize=16)\n",
    "\n",
    "for ax, feature in zip(axes.flat, top_features):\n",
    "    # TODO: Crea un scatter plot para cada variable vs quality\n",
    "    # Agrega una línea de tendencia\n",
    "    # Tu código aquí\n",
    "    x = wine_data[feature]\n",
    "    y = wine_data['quality']\n",
    "    ax.scatter(x, y, alpha=0.4, edgecolor='k')\n",
    "    # línea de tendencia\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    x_line = np.linspace(x.min(), x.max(), 100)\n",
    "    ax.plot(x_line, m*x_line + b, linestyle='--')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('quality')\n",
    "    ax.set_title(f'{feature} vs quality (pendiente {m:.2f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fc894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 3. Preparación de Datos\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = wine_data.drop('quality', axis=1)\n",
    "y = wine_data['quality']\n",
    "\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(f\"\\nCaracterísticas: {X.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57699f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: Divide los datos en conjuntos de entrenamiento y prueba\n",
    "# Usa test_size=0.2 y random_state=42\n",
    "# Tu código aquí\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape[0]}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {X_test.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Estandariza las características\n",
    "# Recuerda: ajusta el scaler solo con los datos de entrenamiento\n",
    "# Tu código aquí\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 4. Validación Cruzada para Selección de Hiperparámetros\n",
    "#\n",
    "# La validación cruzada es fundamental para seleccionar los mejores hiperparámetros sin usar el conjunto de prueba.\n",
    "\n",
    "\n",
    "# ### Ejercicio 4.1: Implementación manual de validación cruzada\n",
    "\n",
    "\n",
    "# Ejemplo: Validación cruzada manual para Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def manual_cross_validation(X, y, alpha, n_folds=5):\n",
    "    \"\"\"\n",
    "    Implementa validación cruzada manualmente para Ridge regression\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Dividir datos\n",
    "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Estandarizar\n",
    "        scaler_cv = StandardScaler()\n",
    "        X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n",
    "        X_val_cv_scaled = scaler_cv.transform(X_val_cv)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model = Ridge(alpha=alpha, random_state=42)\n",
    "        model.fit(X_train_cv_scaled, y_train_cv)\n",
    "        \n",
    "        # Evaluar\n",
    "        y_pred = model.predict(X_val_cv_scaled)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n",
    "        scores.append(rmse)\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# Probar diferentes valores de alpha\n",
    "alphas_to_test = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "cv_results_manual = []\n",
    "\n",
    "print(\"Validación Cruzada Manual para Ridge Regression:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for alpha in alphas_to_test:\n",
    "    mean_rmse, std_rmse = manual_cross_validation(X.values, y, alpha)\n",
    "    cv_results_manual.append({'alpha': alpha, 'mean_rmse': mean_rmse, 'std_rmse': std_rmse})\n",
    "    print(f\"Alpha: {alpha:7.3f} | RMSE: {mean_rmse:.4f} (+/- {std_rmse:.4f})\")\n",
    "\n",
    "# TODO: Identifica el mejor alpha basado en el RMSE medio más bajo\n",
    "# Tu código aquí\n",
    "cv_df = pd.DataFrame(cv_results_manual)\n",
    "best_alpha_manual = cv_df.loc[cv_df['mean_rmse'].idxmin(), 'alpha']\n",
    "print(f\"\\nMejor alpha (CV manual): {best_alpha_manual}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03acf88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Ejercicio 4.2: Usar RidgeCV para validación cruzada automática\n",
    "\n",
    "\n",
    "# TODO: Usa RidgeCV para encontrar automáticamente el mejor alpha\n",
    "# Pista: RidgeCV tiene un parámetro 'alphas' y 'cv'\n",
    "# Tu código aquí\n",
    "\n",
    "alphas = np.logspace(-3, 3, 100)  # 100 valores entre 0.001 y 1000\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Mejor alpha encontrado por RidgeCV: {ridge_cv.alpha_:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Ejercicio 4.3: GridSearchCV para búsqueda exhaustiva\n",
    "\n",
    "# Ejemplo completo con GridSearchCV para Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir parámetros a buscar\n",
    "param_grid_ridge = {\n",
    "    'alpha': np.logspace(-3, 3, 20)  # 20 valores entre 0.001 y 1000\n",
    "}\n",
    "\n",
    "# Crear modelo base\n",
    "ridge_base = Ridge(random_state=42)\n",
    "\n",
    "# TODO: Implementa GridSearchCV\n",
    "# Usa cv=5, scoring='neg_mean_squared_error'\n",
    "# Tu código aquí\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=ridge_base,\n",
    "    param_grid=param_grid_ridge,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "grid_search_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Mejor alpha (GridSearchCV): {grid_search_ridge.best_params_['alpha']}\")\n",
    "print(f\"Mejor score (RMSE CV): {np.sqrt(-grid_search_ridge.best_score_):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34481135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Visualiza los resultados de la validación cruzada\n",
    "# Crea un gráfico que muestre cómo cambia el RMSE con diferentes valores de alpha\n",
    "# Tu código aquí\n",
    "\n",
    "results = grid_search_ridge.cv_results_\n",
    "alphas_gs = results['param_alpha'].data.astype(float)\n",
    "rmse_means = np.sqrt(-results['mean_test_score'])\n",
    "rmse_stds = np.sqrt(-results['std_test_score']**2)  # std ya viene en score, sólo para claridad\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas_gs, rmse_means, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha (log)')\n",
    "plt.ylabel('RMSE (CV)')\n",
    "plt.title('Ridge: RMSE en CV vs alpha')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 5. Modelos de Regresión\n",
    "#\n",
    "# ### 5.1 Regresión Lineal Normal\n",
    "\n",
    "\n",
    "# TODO: Implementa y entrena un modelo de regresión lineal\n",
    "# Tu código aquí\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_lr_train = lr_model.predict(X_train_scaled)\n",
    "y_pred_lr_test  = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calcular métricas\n",
    "rmse_lr_train = np.sqrt(mean_squared_error(y_train, y_pred_lr_train))\n",
    "rmse_lr_test  = np.sqrt(mean_squared_error(y_test, y_pred_lr_test))\n",
    "r2_lr_train   = r2_score(y_train, y_pred_lr_train)\n",
    "r2_lr_test    = r2_score(y_test, y_pred_lr_test)\n",
    "\n",
    "print(\"Regresión Lineal Normal:\")\n",
    "print(f\"RMSE Train: {rmse_lr_train:.4f}\")\n",
    "print(f\"RMSE Test : {rmse_lr_test:.4f}\")\n",
    "print(f\"R² Train : {r2_lr_train:.4f}\")\n",
    "print(f\"R² Test  : {r2_lr_test:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### 5.2 Ridge Regression con mejor alpha de CV\n",
    "\n",
    "\n",
    "# TODO: Entrena Ridge con el mejor alpha encontrado por validación cruzada\n",
    "# Tu código aquí\n",
    "\n",
    "best_alpha_ridge = ridge_cv.alpha_  # de la sección anterior\n",
    "ridge_model = Ridge(alpha=best_alpha_ridge, random_state=42)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones y métricas\n",
    "y_pred_ridge_train = ridge_model.predict(X_train_scaled)\n",
    "y_pred_ridge_test  = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "rmse_ridge_train = np.sqrt(mean_squared_error(y_train, y_pred_ridge_train))\n",
    "rmse_ridge_test  = np.sqrt(mean_squared_error(y_test, y_pred_ridge_test))\n",
    "r2_ridge_train   = r2_score(y_train, y_pred_ridge_train)\n",
    "r2_ridge_test    = r2_score(y_test, y_pred_ridge_test)\n",
    "\n",
    "print(\"\\nRidge Regression:\")\n",
    "print(f\"alpha óptimo: {best_alpha_ridge:.6f}\")\n",
    "print(f\"RMSE Train: {rmse_ridge_train:.4f}\")\n",
    "print(f\"RMSE Test : {rmse_ridge_test:.4f}\")\n",
    "print(f\"R² Train : {r2_ridge_train:.4f}\")\n",
    "print(f\"R² Test  : {r2_ridge_test:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95750943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### 5.3 Lasso Regression con validación cruzada\n",
    "\n",
    "# TODO: Implementa LassoCV para encontrar el mejor alpha automáticamente\n",
    "# Tu código aquí\n",
    "\n",
    "alphas_lasso = np.logspace(-3, 1, 100)\n",
    "lasso_cv = LassoCV(alphas=alphas_lasso, cv=5, random_state=42)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nMejor alpha para Lasso: {lasso_cv.alpha_:.6f}\")\n",
    "print(f\"Número de características seleccionadas: {(lasso_cv.coef_ != 0).sum()} / {X.shape[1]}\")\n",
    "\n",
    "# Predicciones y métricas\n",
    "y_pred_lasso_train = lasso_cv.predict(X_train_scaled)\n",
    "y_pred_lasso_test  = lasso_cv.predict(X_test_scaled)\n",
    "\n",
    "rmse_lasso_train = np.sqrt(mean_squared_error(y_train, y_pred_lasso_train))\n",
    "rmse_lasso_test  = np.sqrt(mean_squared_error(y_test, y_pred_lasso_test))\n",
    "r2_lasso_train   = r2_score(y_train, y_pred_lasso_train)\n",
    "r2_lasso_test    = r2_score(y_test, y_pred_lasso_test)\n",
    "\n",
    "print(\"\\nLasso Regression (CV):\")\n",
    "print(f\"RMSE Train: {rmse_lasso_train:.4f}\")\n",
    "print(f\"RMSE Test : {rmse_lasso_test:.4f}\")\n",
    "print(f\"R² Train : {r2_lasso_train:.4f}\")\n",
    "print(f\"R² Test  : {r2_lasso_test:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dce9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Identifica qué características fueron eliminadas por Lasso\n",
    "# Tu código aquí\n",
    "lasso_coefs = pd.Series(lasso_cv.coef_, index=X.columns)\n",
    "dropped_features = lasso_coefs[lasso_coefs == 0].index.tolist()\n",
    "print(\"\\nCaracterísticas eliminadas por Lasso:\")\n",
    "print(dropped_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 6. Comparación de Modelos\n",
    "\n",
    "# TODO: Crea una tabla comparativa con todos los modelos\n",
    "# Incluye: RMSE Train, RMSE Test, R² Train, R² Test, MAE Test\n",
    "# Tu código aquí\n",
    "\n",
    "comparison_data = {\n",
    "    'Modelo':      ['Linear', 'Ridge', 'Lasso'],\n",
    "    'RMSE Train':  [rmse_lr_train, rmse_ridge_train, rmse_lasso_train],\n",
    "    'RMSE Test':   [rmse_lr_test,  rmse_ridge_test,  rmse_lasso_test],\n",
    "    'R² Train':    [r2_lr_train,   r2_ridge_train,   r2_lasso_train],\n",
    "    'R² Test':     [r2_lr_test,    r2_ridge_test,    r2_lasso_test],\n",
    "    'MAE Test':    [\n",
    "        mean_absolute_error(y_test, y_pred_lr_test),\n",
    "        mean_absolute_error(y_test, y_pred_ridge_test),\n",
    "        mean_absolute_error(y_test, y_pred_lasso_test)\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nComparación de modelos:\")\n",
    "print(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Crea visualizaciones para comparar los modelos\n",
    "# 1. Gráfico de barras comparando RMSE\n",
    "# 2. Gráfico de barras comparando R²\n",
    "# Tu código aquí\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.barplot(x='Modelo', y='RMSE Test', data=comparison_df, ax=ax[0])\n",
    "ax[0].set_title('Comparación RMSE (Test)')\n",
    "ax[0].set_ylabel('RMSE')\n",
    "\n",
    "sns.barplot(x='Modelo', y='R² Test', data=comparison_df, ax=ax[1])\n",
    "ax[1].set_title('Comparación R² (Test)')\n",
    "ax[1].set_ylabel('R²')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb777586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 7. Análisis de Residuos\n",
    "\n",
    "\n",
    "# TODO: Para el mejor modelo, crea:\n",
    "# 1. Gráfico de residuos vs predicciones\n",
    "# 2. Histograma de residuos\n",
    "# 3. Q-Q plot de residuos\n",
    "# Tu código aquí\n",
    "\n",
    "# Elegimos el mejor por menor RMSE Test\n",
    "best_idx = comparison_df['RMSE Test'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_idx, 'Modelo']\n",
    "\n",
    "if best_model_name == 'Linear':\n",
    "    best_model = lr_model\n",
    "    y_pred_best = y_pred_lr_test\n",
    "elif best_model_name == 'Ridge':\n",
    "    best_model = ridge_model\n",
    "    y_pred_best = y_pred_ridge_test\n",
    "else:\n",
    "    best_model = lasso_cv\n",
    "    y_pred_best = y_pred_lasso_test\n",
    "\n",
    "residuals = y_test - y_pred_best\n",
    "\n",
    "# 1) Residuos vs predicciones\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_pred_best, residuals, alpha=0.5, edgecolor='k')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Residuos')\n",
    "plt.title(f'Residuos vs Predicciones ({best_model_name})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Histograma de residuos\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(residuals, bins=30, edgecolor='k')\n",
    "plt.title('Histograma de Residuos')\n",
    "plt.xlabel('Residuo')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Q-Q plot\n",
    "import scipy.stats as stats\n",
    "plt.figure(figsize=(7,5))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot de Residuos')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a69d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 8. Importancia de Características\n",
    "\n",
    "\n",
    "# TODO: Visualiza los coeficientes de los tres modelos en un mismo gráfico\n",
    "# Esto te ayudará a entender qué características son más importantes\n",
    "# Tu código aquí\n",
    "\n",
    "coef_linear = pd.Series(lr_model.coef_, index=X.columns)\n",
    "coef_ridge  = pd.Series(ridge_model.coef_, index=X.columns)\n",
    "coef_lasso  = pd.Series(lasso_cv.coef_, index=X.columns)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Linear': coef_linear,\n",
    "    'Ridge':  coef_ridge,\n",
    "    'Lasso':  coef_lasso\n",
    "})\n",
    "\n",
    "coef_df.plot(kind='bar', figsize=(14,6))\n",
    "plt.title('Coeficientes por modelo')\n",
    "plt.ylabel('Valor del coeficiente (escala estandarizada)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## 9. Validación Cruzada Final del Mejor Modelo\n",
    "\n",
    "# TODO: Realiza validación cruzada con 10 folds del mejor modelo\n",
    "# Reporta la media y desviación estándar del RMSE\n",
    "# Tu código aquí\n",
    "\n",
    "def rmse_cv_estimator(estimator, X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # pipeline manual de escalado\n",
    "    rmses = []\n",
    "    for tr_idx, te_idx in kf.split(X):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "        sc = StandardScaler()\n",
    "        X_tr_s = sc.fit_transform(X_tr)\n",
    "        X_te_s = sc.transform(X_te)\n",
    "        est = estimator\n",
    "        est.fit(X_tr_s, y_tr)\n",
    "        y_hat = est.predict(X_te_s)\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_te, y_hat)))\n",
    "    return np.mean(rmses), np.std(rmses)\n",
    "\n",
    "if best_model_name == 'Linear':\n",
    "    est = LinearRegression()\n",
    "elif best_model_name == 'Ridge':\n",
    "    est = Ridge(alpha=best_alpha_ridge, random_state=42)\n",
    "else:\n",
    "    est = Lasso(alpha=lasso_cv.alpha_, random_state=42)\n",
    "\n",
    "rmse_mean_10cv, rmse_std_10cv = rmse_cv_estimator(est, X, y, n_splits=10)\n",
    "print(f\"Validación cruzada 10-fold ({best_model_name}) -> RMSE medio: {rmse_mean_10cv:.4f} +/- {rmse_std_10cv:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd573ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 10. Conclusiones y Preguntas de Reflexión\n",
    "#\n",
    "# 1. **¿Cuál modelo tuvo el mejor desempeño? ¿Por qué crees que fue así?**  \n",
    "#    - Respuesta sugerida: El mejor por RMSE en test fue `{best_model_name}`. Regularizaciones (Ridge/Lasso) suelen mejorar generalización al reducir varianza/overfitting respecto a la regresión lineal pura.\n",
    "#\n",
    "# 2. **¿Qué características son las más importantes para predecir la calidad del vino?**  \n",
    "#    - Respuesta sugerida: En general, *alcohol* (positiva) y *volatile acidity* (negativa) tienden a dominar; *sulphates* y *citric acid* también aportan.\n",
    "#\n",
    "# 3. **¿Observas señales de sobreajuste en algún modelo? ¿Cómo lo identificaste?**  \n",
    "#    - Respuesta sugerida: Comparando R² (train vs test) y RMSE (train vs test). Una brecha grande indicaría sobreajuste.\n",
    "#\n",
    "# 4. **¿Cómo cambió el rendimiento de Ridge y Lasso con diferentes valores de alpha?**  \n",
    "#    - Respuesta sugerida: Con alpha pequeño ≈ modelo lineal; al aumentar alpha, los coeficientes se encogen, sube el sesgo y baja la varianza; hay un punto óptimo.\n",
    "#\n",
    "# 5. **¿Qué ventajas observaste al usar validación cruzada para seleccionar hiperparámetros?**  \n",
    "#    - Respuesta sugerida: Usa mejor los datos, reduce varianza de estimación y evita usar el set de prueba para selección.\n",
    "#\n",
    "# 6. **Si Lasso eliminó algunas características, ¿crees que esto mejoró o empeoró el modelo? ¿Por qué?**  \n",
    "#    - Respuesta sugerida: Puede mejorar al reducir ruido y simplificar el modelo; depende del patrón de correlaciones y de alpha óptimo.\n",
    "#\n",
    "# 7. **¿Qué otros pasos podrías tomar para mejorar el rendimiento del modelo?**  \n",
    "#    - Respuesta sugerida: Ingeniería de características (interacciones), modelos no lineales (árboles/GBM), detección de outliers, validación anidada, calibración de hiperparámetros más fina.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
